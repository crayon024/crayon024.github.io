<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2021 -&gt; 2022 总结和计划</title>
    <url>/2022/01/13/2021-2022/</url>
    <content><![CDATA[<h2 id="2021-总结"><a href="#2021-总结" class="headerlink" title="2021 总结"></a>2021 总结</h2><ol>
<li><p>我的 21 年关键字是 ‘纠结’。<span id="more"></span></p>
<p>2021 年中工作经验满 1 年。从毕业前后拖拉的在不多的应届生岗位中寻找工作。入职后，虽然工作可以胜任，但是对于自己能力的猜疑让自己一直在纠结中度过，回头看来其实纠结的点都是没有必要。</p>
<p>2021 从英文世界获取信息的能力大幅提升。自己从头翻译了 Redis 5.0.8 的配置文件 redis.conf，心理上不抗拒加上感受到获取到的信息质量确实有差异后，正反馈明显。</p>
</li>
<li><p>参加 Java 训练营。</p>
<p>技术上的收获是从更高的维度完善了技术体系的认知；其次是见识到了社群中更多优秀的同学也在不断进步。</p>
</li>
<li><p>拖延。<br>关于这篇总结文章，可以看做是 2022 摆脱拖延的开始。此前内心总有许多想法，但都没有真正去执行。比如为自己的职业生涯，技能提升做一个明确的规划并为之努力。</p>
<p>比如一直想搞定算法，但是都是有一阵没一阵，并没有系统的提高算法能力。能够应付大部分面试的算法部分。内心的想法很多 - 摄影 （极客时间买了摄影课程）、视频（安装过达芬奇 等等剪辑软件）都不了了之。</p>
<p>LeetCode:</p>
<p><img src="/2022/01/13/2021-2022/image-20220113173401456.png" alt="leetcode提交记录"></p>
<p>GitHub:</p>
<p><img src="/2022/01/13/2021-2022/image-20220113173717713.png" alt="GitHub提交记录"></p>
</li>
<li><p>收获。<br>GitHub + Hexo 搭建了博客，整理了平时的学习笔记发布。</p>
<p>英文的听读能力有了客观进步。</p>
</li>
<li><p>书籍。</p>
<p>技术：《Java 8 实战》《Netty 实战》《Java 并发编程的艺术》《Java 并发编程实战》《深入理解 Java 虚拟机 JVM》《第一本 Docker 书》<br>其他：《俗世奇人》《软技能：代码之外的生存指南》《你是你吃出来的》《一往无前》《我们唱》《掌控：开启不疲惫、不焦虑的人生》《睡眠革命》《科比：黄金年代》</p>
</li>
<li><p>娱乐。</p>
<ol>
<li>推荐剧集：《绝命毒师》1-5 季。</li>
<li>推荐游戏：《巫师 3 狂猎》。</li>
</ol>
</li>
</ol>
<h2 id="2022-计划"><a href="#2022-计划" class="headerlink" title="2022 计划"></a>2022 计划</h2><p>希望 2022 可以比 2021 更加从容，生活工作上都能如此。</p>
<ol>
<li>Object 1：提高技术能力<ol>
<li>KR1：丰富博客站内容，增加 30 篇内容。</li>
<li>KR2：完成 5 篇高质量的技术博客。</li>
<li>KR3：参与开源项目，为开源项目提交代码。</li>
</ol>
</li>
<li>Object 2：搞定算法，能够应付大部分面试的算法部分。<ol>
<li>KR1：分模块进行刷题训练。DP，搜索，贪心 …</li>
<li>KR2：LeetCode 剑指 Offer 系列题目刷完。</li>
</ol>
</li>
</ol>
<p>有了大致的目标后，如何分配自己的时间去做也是重点的部分。我目前的做法基于番茄工作法，从《软技能：代码之外的生存指南》中为自己指定了一天 7 个番茄时钟的做法去实践。</p>
<p><a href="https://draveness.me/few-words-time-management/">如何管理自己的时间资产</a></p>
<p><img src="/2022/01/13/2021-2022/image-20220113201324042.png" alt="image-20220113201324042"></p>
<p><a href="https://book.douban.com/subject/26835090/">软技能：代码之外的生存指南</a></p>
<p><img src="/2022/01/13/2021-2022/image-20220113201656902.png" alt="image-20220113201656902"></p>
]]></content>
      <categories>
        <category>记录</category>
      </categories>
      <tags>
        <tag>日常</tag>
        <tag>记录</tag>
      </tags>
  </entry>
  <entry>
    <title>类加载器机制 学习笔记</title>
    <url>/2021/09/15/ClassLoader-And-Tomact-isolation/</url>
    <content><![CDATA[<h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><p>此篇文章主要从 3 个部分解析 JVM 的类加载机制：</p>
<ol>
<li><p>基础的类加载机制（包括类加载的 7 个步骤、类加载（初始化）的时机以及经典的双亲委派模型）概念开始，辅助以 ClassLoader 的源码进行分析验证双亲委派机制。</p>
</li>
<li><p>Tomcat 内部是如何通过<strong>打破双亲委派机制</strong>来实现自己的功能需要。</p>
</li>
<li><p><strong>上下文类加载器</strong>的出现，顶层的类加载器可以通过 ThreadContextClassLoader 来调用下层的加载器来加载类。<span id="more"></span></p>
</li>
</ol>
<h2 id="类加载机制"><a href="#类加载机制" class="headerlink" title="类加载机制"></a>类加载机制</h2><blockquote>
<p>Java虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这个过程被称作虚拟机的类加载机制。</p>
<p>在 Java 语言里面，类型的加载、连接和初始化过程<strong>都是在程序运行期间完成的</strong>，这种策略让 Java 语言进行提前编译会面临额外的困难，也会让类加载时稍微增加一些性能开销， 但是却为 Java 应用提供了极高的扩展性和灵活性，Java 天生可以动态扩展的语言特性就是依赖运行期动态加载和动态连接这个特点实现的。</p>
</blockquote>
<p><img src="/2021/09/15/ClassLoader-And-Tomact-isolation/Snipaste_1012144059.png" alt="Snipaste_1012144059"></p>
<h3 id="类加载的时机"><a href="#类加载的时机" class="headerlink" title="类加载的时机"></a>类加载的时机</h3><blockquote>
<p>关于在什么情况下需要开始类加载过程的第一个阶段“加载”，《Java虚拟机规范》中并没有进行 强制约束，这点可以交给虚拟机的具体实现来自由把握。但是对于初始化阶段，《Java虚拟机规范》 则是严格规定了几种必须立即对类进行“初始化”的情况（而加载、验证、准备自然需要在此之前开始）：</p>
</blockquote>
<p><img src="/2021/09/15/ClassLoader-And-Tomact-isolation/Snipaste_1012144958.png" alt="Snipaste_1012144958"></p>
<h3 id="双亲委派"><a href="#双亲委派" class="headerlink" title="双亲委派"></a>双亲委派</h3><p><img src="/2021/09/15/ClassLoader-And-Tomact-isolation/Snipaste_12105956.png" alt="Snipaste_12105956"></p>
<ul>
<li><p>BootstrapClassLoader 是启动类加载器，由 C 语言实现，用来加载 JVM 启动时所需要的核心类.这个类加载器负责加载存放在 JAVA_HOME\lib 目录，或者被-Xbootclasspath参数所指定的路径中存放的，而且是Java虚拟机能够 识别的（按照文件名识别，如rt.jar、tools.jar，名字不符合的类库即使放在lib目录中也不会被加载）类 库加载到虚拟机的内存中。</p>
</li>
<li><p>ExtClassLoader 是扩展类加载器，用来加载 JAVA_HOME\lib\ext 目录下 JAR 包。</p>
</li>
<li><p>AppClassLoader 是系统类加载器，用来加载 classpath 下的类，应用程序默认用它来加载类。</p>
</li>
<li><p>自定义类加载器，用来加载自定义路径下的类。</p>
</li>
</ul>
<h3 id="ClassLoader-源码分析"><a href="#ClassLoader-源码分析" class="headerlink" title="ClassLoader  源码分析"></a>ClassLoader  源码分析</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ClassLoader</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//每个类加载器都有个父加载器</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ClassLoader parent;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> Class&lt;?&gt; loadClass(String name) &#123;</span><br><span class="line">  </span><br><span class="line">        <span class="comment">//查找一下这个类是不是已经加载过了</span></span><br><span class="line">        Class&lt;?&gt; c = findLoadedClass(name);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//如果没有加载过</span></span><br><span class="line">        <span class="keyword">if</span>( c == <span class="keyword">null</span> )&#123;</span><br><span class="line">          <span class="comment">//先委托给父加载器去加载，注意这是个递归调用</span></span><br><span class="line">          <span class="keyword">if</span> (parent != <span class="keyword">null</span>) &#123;</span><br><span class="line">              c = parent.loadClass(name);</span><br><span class="line">          &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="comment">// 如果父加载器为空，查找Bootstrap加载器是不是加载过了</span></span><br><span class="line">              c = findBootstrapClassOrNull(name);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 如果父加载器没加载成功，调用自己的findClass去加载</span></span><br><span class="line">        <span class="keyword">if</span> (c == <span class="keyword">null</span>) &#123;</span><br><span class="line">            c = findClass(name);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> c；</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">protected</span> Class&lt;?&gt; findClass(String name)&#123;</span><br><span class="line">       <span class="comment">//1. 根据传入的类名name，到在特定目录下去寻找类文件，把.class文件读入内存</span></span><br><span class="line">          ...</span><br><span class="line">       <span class="comment">//2. 调用defineClass将字节数组转成Class对象</span></span><br><span class="line">       <span class="keyword">return</span> defineClass(buf, off, len)；</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 将字节码数组解析成一个Class对象，用native方法实现</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> Class&lt;?&gt; defineClass(<span class="keyword">byte</span>[] b, <span class="keyword">int</span> off, <span class="keyword">int</span> len)&#123;</span><br><span class="line">       ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从上面的代码我们可以发现，</p>
<ol>
<li><strong>public loadClass 方法</strong>明显先委托给父加载器先去加载。</li>
<li>findClass 方法负责查找 .class 文件</li>
<li>defineClass 负责把字节数组解析成一个堆上的 Class 对象</li>
</ol>
<h3 id="Tomcat-的类加载器"><a href="#Tomcat-的类加载器" class="headerlink" title="Tomcat 的类加载器"></a>Tomcat 的类加载器</h3><p>Tomcat 中的自定义类加载器 WebAppClassLoader 中的实现打破了双亲委派机制。优先加载 Web 应用目录下的类，然后再加载其他目录下的类。</p>
<h4 id="loadClass-方法"><a href="#loadClass-方法" class="headerlink" title="loadClass 方法"></a>loadClass 方法</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> Class&lt;?&gt; loadClass(String name, <span class="keyword">boolean</span> resolve) <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (getClassLoadingLock(name)) &#123;</span><br><span class="line">        Class&lt;?&gt; clazz = <span class="keyword">null</span>;</span><br><span class="line">        <span class="comment">//1. 先在本地cache查找该类是否已经加载过</span></span><br><span class="line">        clazz = findLoadedClass0(name);</span><br><span class="line">            <span class="keyword">return</span> clazz;</span><br><span class="line">        <span class="comment">//2. 从系统类加载器的cache中查找是否加载过</span></span><br><span class="line">        clazz = findLoadedClass(name);</span><br><span class="line">            <span class="keyword">return</span> clazz;</span><br><span class="line">        <span class="comment">// 3. 尝试用ExtClassLoader类加载器类加载，为什么？</span></span><br><span class="line">        ClassLoader javaseLoader = getJavaseClassLoader();</span><br><span class="line">            clazz = javaseLoader.loadClass(name);</span><br><span class="line">                <span class="keyword">return</span> clazz;</span><br><span class="line">        <span class="comment">// 4. 尝试在本地目录搜索class并加载</span></span><br><span class="line">            clazz = findClass(name);</span><br><span class="line">                <span class="keyword">return</span> clazz;</span><br><span class="line">        <span class="comment">// 5. 尝试用系统类加载器(也就是AppClassLoader)来加载</span></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                clazz = Class.forName(name, <span class="keyword">false</span>, parent);</span><br><span class="line">				<span class="keyword">return</span> clazz;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">                <span class="comment">// Ignore</span></span><br><span class="line">            &#125;</span><br><span class="line">       &#125;</span><br><span class="line">    <span class="comment">//6. 上述过程都加载失败，抛出异常</span></span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> ClassNotFoundException(name);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>以上代码的第 3 步，让 ExtClassLoader 去加载，一方面可以避免 Web 应用的自己的类覆盖 JRE 核心类；另一方面 AppClassLoader 上一层，因为 AppClassLoader 会加载 ClassPath 下的类（包括了 web path），这样就无法优先加载 web 应用目录下的类。</p>
<h3 id="Tomcat-类加载机制实现的目的"><a href="#Tomcat-类加载机制实现的目的" class="headerlink" title="Tomcat  类加载机制实现的目的"></a>Tomcat  类加载机制实现的目的</h3><p>我们知道，Tomcat 作为 Servlet 容器，它负责加载我们的 Servlet 类，此外它还负责加载 Servlet 所依赖的 JAR 包。并且 Tomcat 本身也是一个 Java 程序，因此它需要加载自己的类和依赖的 JAR 包。</p>
<ol>
<li><p>假如我们在 Tomcat 中运行了两个 Web 应用程序，两个 Web 应用中有同名的 Servlet，但是功能不同，Tomcat 需要同时加载和管理这两个同名的 Servlet 类，保证它们不会冲突，因此 Web 应用之间的类需要隔离。</p>
</li>
<li><p>假如两个 Web 应用都依赖同一个第三方的 JAR 包，比如 Spring，那 Spring 的 JAR 包被加载到内存后，Tomcat 要保证这两个 Web 应用能够共享，也就是说 Spring 的 JAR 包只被加载一次，否则随着依赖的第三方 JAR 包增多，JVM 的内存会膨胀。</p>
</li>
<li><p>跟 JVM 一样，我们需要隔离 Tomcat 本身的类和 Web 应用的类。</p>
</li>
</ol>
<p><img src="/2021/09/15/ClassLoader-And-Tomact-isolation/Snipaste_12103923.png" alt="Snipaste_12103923"></p>
<p><a href="https://time.geekbang.org/column/article/105711?utm_source=u_nav_web&utm_medium=u_nav_web&utm_term=u_nav_web">25 | Context容器（中）：Tomcat如何隔离Web应用？ (geekbang.org)</a></p>
<h3 id="上下文类加载器"><a href="#上下文类加载器" class="headerlink" title="上下文类加载器"></a>上下文类加载器</h3><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol>
<li><a href="https://time.geekbang.org/column/article/105711?utm_source=u_nav_web&utm_medium=u_nav_web&utm_term=u_nav_web">Tomcat如何隔离Web应用？ (geekbang.org)</a></li>
<li>《深入理解 Java 虚拟机》</li>
</ol>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>jvm</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>Java 并发编程（二）</title>
    <url>/2021/08/19/Concurrent-Programming-2/</url>
    <content><![CDATA[<h1 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h1><h2 id="Lock-接口"><a href="#Lock-接口" class="headerlink" title="Lock 接口"></a>Lock 接口</h2><p>Java 1.5 之后，并发包新增了 Lock 接口以及相关实现类来提供更丰富的锁功能。synchronized 同步块虽然使用上很便携，但是可操作性比较欠缺<span id="more"></span>。通过引入 Lock ，程序员可以手动获取锁，释放锁，<strong>超时获取锁，非阻塞获取锁，响应中断</strong>等等。</p>
<p>基于 Lock 接口，提供了许多好用的实现类。</p>
<p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0210113909.png" alt="Sni_0210113909"></p>
<p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0210113932-3146023.png" alt="Sni_0210113932"></p>
<h2 id="AQS"><a href="#AQS" class="headerlink" title="AQS"></a>AQS</h2><p>AbstractQueuedSynchronizer - 队列同步器，许多 Lock 的实现类都是基于这个基础组件来实现资源的访问控制的。</p>
<p>AQS 是基于模板方法模式设计的，想要实现同步组件，我们可以继承 AQS，重写对应的方法，然后调用 AQS 提供的模板方法来使用。</p>
<h3 id="使用同步组件"><a href="#使用同步组件" class="headerlink" title="使用同步组件"></a>使用同步组件</h3><p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0210120908.png" alt="Sni_0210120908"></p>
<p>可以看到 5 个可由我们重写的方法（这些资源就是 AQS 里面的 state 基础变量）：</p>
<ul>
<li>尝试获取内部的资源</li>
<li>释放资源</li>
<li>尝试获取共享资源</li>
<li>释放共享资源</li>
<li>是否是独占模式</li>
</ul>
<p>一般我们将该实现类放到内部，然后暴露给外部使用的方法中调用这些方法来进行控制。</p>
<p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0210124106.png" alt="Sni_0210124106"></p>
<p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0210124230.png" alt="Sni_0210124230"></p>
<p>暴露出来的方法，比如 tryLock() 内部调用我们重写的 tryAcquire 方法。内部的 tryAcquire() 方法实现，核心是围绕着 <strong>state</strong> 进行资源控制的。主要通过这 3 个方法:</p>
<ol>
<li>getState()</li>
<li>setState()</li>
<li>compareAndSetState()</li>
</ol>
<p>上面的这个简单实现表示，如果state == 0，表示没有线程占用，我们用 cas 成功后，当前线程则获取到锁。state此时 == 1。</p>
<p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0210162819.png" alt="Sni_0210162819"></p>
<p>MyLock 实现 Lock 接口，通过内部类实现 AQS，进行资源控制。lock() 方法则是直接调用 AQS 提供的**模板方法 acquire()**，先是 tryAcquire() 不行的话就去排队。</p>
<h2 id="ReentrantLock"><a href="#ReentrantLock" class="headerlink" title="ReentrantLock"></a>ReentrantLock</h2><p>可重入锁。它支持同一个线程对同步资源重复加锁（释放锁也需要调用对应次数 unlock())。</p>
<p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0210165349.png" alt="Sni_0210165349"></p>
<p>ReentrantLock 提供了公平锁和非公平锁，公平的指的是排队等待锁的线程获取锁是否公平。公平的话，当锁可获取时，等待最久的线程优先获取锁。不公平就是无法保证。非公平的锁性能一般高些（线程切换，同一个线程重复占用）。</p>
<h3 id="ReentrantLock-的内部-AQS-实现类（基于默认的-NonFairSync）"><a href="#ReentrantLock-的内部-AQS-实现类（基于默认的-NonFairSync）" class="headerlink" title="ReentrantLock 的内部 AQS 实现类（基于默认的 NonFairSync）"></a>ReentrantLock 的内部 AQS 实现类（基于默认的 NonFairSync）</h3><p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0210170533.png" alt="Sni_0210170533"></p>
<ol>
<li>NonfairSync#lock()。cas 获取 state，<ol>
<li>设置成功拿到锁，设置独占线程。</li>
<li>失败，调用 acquire（AQS 的实现），将会先调用 tryAcquire() 尝试获取，不行的话就去排队。</li>
</ol>
</li>
<li>tryAcquire() 直接调用父类的 nonfiarTryAcquire() 实现，实现如下。</li>
</ol>
<p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0210171143.png" alt="Sni_0210171143"></p>
<ol start="3">
<li><p>公平锁的 tryAcruire() 实现，在判断 c == 0 后，设置同步位的时候调用 hasQueuedPredecessors() 一起判断。该方法的注释如下。</p>
<p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0210173400.png" alt="Sni_0210173400"></p>
</li>
<li><p>Sync#nonfairTryAquire</p>
<ul>
<li>getState() 判断资源占用状态</li>
<li>== 0，这一时刻咩有线程占用，cas 尝试设置状态 -&gt;<ul>
<li>成功，return ture，获取锁成功。</li>
<li>失败，return false，tryacquire 失败。</li>
</ul>
</li>
<li>!= 0，表示资源被线程持有，判断是不是当前线程 -&gt;<ul>
<li>是的话，<strong>说明是重复加锁（可重入）</strong>，锁状态 state 叠加后 setState() 设置</li>
<li>不是的话，说明已有线程独占，获取失败。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="ReentrantReadWriteLock"><a href="#ReentrantReadWriteLock" class="headerlink" title="ReentrantReadWriteLock"></a>ReentrantReadWriteLock</h2><p>Java 中 ReadWriteLock 接口，也就是读写锁。与直接的互斥锁不同，读写锁中的读锁允许多个线程一起读临界资源。这种锁在读多写少的场景下性能比互斥锁好很多。读写锁有这 3 个特点：</p>
<ol>
<li>读共享</li>
<li>写互斥</li>
<li>写的时候，无法获取到读锁（也就是不可读）</li>
</ol>
<p>Java 中 ReadWriteLock 有两个实现： ReadWriteView in StampLock 和 ReentrantReadWriteLock。这部分我们主要分析常用的第二个实现 可重入读写锁。</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Example</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 实例化 ReentrantReadWriteLock() rwl</span></span><br><span class="line">    <span class="keyword">final</span> ReadWriteLock rwl = <span class="keyword">new</span> ReentrantReadWriteLock();</span><br><span class="line">    <span class="comment">// 通过 rwl 获取对应的锁</span></span><br><span class="line">    <span class="keyword">final</span> Lock r = rwl.readLock();</span><br><span class="line">    <span class="keyword">final</span> Lock w = rwl.writeLock();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="利用读写锁实现简易缓存方案的-demo"><a href="#利用读写锁实现简易缓存方案的-demo" class="headerlink" title="利用读写锁实现简易缓存方案的 demo"></a>利用读写锁实现简易缓存方案的 demo</h3><p><img src="/2021/08/19/Concurrent-Programming-2/carbon.png" alt="carbon (1)"></p>
<h3 id="非公平锁-读写锁排队问题"><a href="#非公平锁-读写锁排队问题" class="headerlink" title="非公平锁 读写锁排队问题"></a>非公平锁 读写锁排队问题</h3><p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0410190800.png" alt="Sni_0410190800"></p>
<p>写锁始终不排队。非公平实现下，被阻塞的线程等待锁，如果此时阻塞在读锁，那么新来的读锁可能可以一起去共享锁，这样的话申请写锁的线程一直被插队，无法执行。</p>
<p>实现中的 readerShouldBlock() 调用的方法 apparentlyFirstQueuedIsExclusive()，等待队列头线程是写，读请求阻塞，排在写的后面来避免写请求一直被阻塞的问题。</p>
<h3 id="注意的问题"><a href="#注意的问题" class="headerlink" title="注意的问题"></a>注意的问题</h3><ol>
<li>不支持锁的升级。<strong>获取读锁后，获取写锁</strong>，会导致写锁永远阻塞，进而读锁无法释放，相关的线程也被阻塞。</li>
<li>支持锁的降级。锁的降级用在的场景在于虽然拥有写锁包含了读锁的功能，但及时释放写锁可能让其他等待读的线程拿到锁。<ol>
<li>获取写锁</li>
<li>业务处理</li>
<li><strong>获取读锁</strong></li>
<li>释放写锁</li>
<li>业务处理</li>
<li>最后释放读锁。</li>
</ol>
</li>
<li>写锁支持 Condition 条件变量，读锁不支持。</li>
</ol>
<h1 id="并发工具"><a href="#并发工具" class="headerlink" title="并发工具"></a>并发工具</h1><h2 id="Condition-接口"><a href="#Condition-接口" class="headerlink" title="Condition 接口"></a>Condition 接口</h2><p>通过 Lock.newCondition() ，相当于进一步的通信协作。可以 new 多个 Conditions 出来。</p>
<p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0210200355.png" alt="Sni_0210200355"></p>
<h2 id="LockSupport"><a href="#LockSupport" class="headerlink" title="LockSupport"></a>LockSupport</h2><p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0210175044.png" alt="Sni_0210175044"></p>
<p>Java 6 加入 part/parkNanos/parkUntil(Ojbeck blocker,long nanos,long dendlin)。</p>
<p>unpark() 方法传入一个 thread 参数，唤醒线程要让其他线程来。</p>
<p>对象 blocker 的作用，大致是方便诊断工具，为我们提供更多信息。</p>
<p><a href="https://stackoverflow.com/questions/36939218/what-is-the-usage-of-the-parameter-of-locksupport-parkobject-blocker">java - What is the usage of the parameter of LockSupport.park(Object blocker)? - Stack Overflow</a></p>
<h2 id="Semaphore"><a href="#Semaphore" class="headerlink" title="Semaphore"></a>Semaphore</h2><p>线程通过调用 acquire() 获取信号量 permit，permit - 1 &gt; 0，则表示成功，线程可以继续往下执行；否则等待。</p>
<p>线程通过调用 releasse() 释放 permit，permit + 1。</p>
<p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0210193029.png" alt="Sni_0210193029"></p>
<h2 id="CountDawnLatch"><a href="#CountDawnLatch" class="headerlink" title="CountDawnLatch"></a>CountDawnLatch</h2><p>线程等待（调用 await() 方法）,其他线程通过 countDown() 方法减少计数，变成 0 后，等待的那个线程继续往下执行。</p>
<p>不能重复使用。</p>
<p><img src="/2021/08/19/Concurrent-Programming-2/image-20211002180904481.png" alt="image-20211002180904481"></p>
<h2 id="CyclicBarrier"><a href="#CyclicBarrier" class="headerlink" title="CyclicBarrier"></a>CyclicBarrier</h2><p>循环屏障，线程到达屏障（await() 方法），停下来等待，达到定义的数量后，一起放行所有被拦住的线程，继续往下执行。</p>
<p>CyclicBarrier 可以重复使用。且可提供一个回调方法，由最后一个到达且触发放行的线程执行。</p>
<p><img src="/2021/08/19/Concurrent-Programming-2/image-20211002180833955.png" alt="image-20211002180833955"></p>
<h2 id="ThreadLocal"><a href="#ThreadLocal" class="headerlink" title="ThreadLocal"></a>ThreadLocal</h2><h2 id="parallel-stream"><a href="#parallel-stream" class="headerlink" title="parallel stream"></a>parallel stream</h2><h1 id="并发原子类"><a href="#并发原子类" class="headerlink" title="并发原子类"></a>并发原子类</h1><p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0210194058.png" alt="Sni_0210194058"></p>
<p>并发原子类的核心原理：</p>
<ol>
<li>volatile </li>
<li>cas、自旋</li>
</ol>
<p>无锁和有锁在不同并发程度下的性能差异：</p>
<ol>
<li>压力小，有锁无锁都没关系，本身压力小。</li>
<li>压力一般，无锁一次写入成功的概率高，性能高些。</li>
<li>压力大，自旋导致 cpu 消耗大，性能差些。</li>
</ol>
<h1 id="并发容器"><a href="#并发容器" class="headerlink" title="并发容器"></a>并发容器</h1><p><img src="/2021/08/19/Concurrent-Programming-2/Sni_0410105523.png" alt="Sni_0410105523"></p>
<h2 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h2><ul>
<li>阻塞队列的 ArrayBlockingQueue 和 LinkedBlockingQueue 支持有界队列。</li>
</ul>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><ol>
<li><p><a href="https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html">https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html</a>，JSR-133 FAQ</p>
</li>
<li><p><a href="http://ifeve.com/jmm-faq/">Java内存模型FAQ | 并发编程网 – ifeve.com</a></p>
</li>
<li><p><a href="https://time.geekbang.org/column/intro/100023901?utm_source=u_nav_web&utm_medium=u_nav_web&utm_term=u_nav_web&tab=catalog">Java并发编程实战 (geekbang.org)</a></p>
</li>
<li><p><a href="https://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.4">https://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.4</a></p>
</li>
<li><p><a href="http://gee.cs.oswego.edu/dl/cpj/jmm.html">http://gee.cs.oswego.edu/dl/cpj/jmm.html</a>，Conucrrent Programming in Java，Doug Lea</p>
</li>
<li><p>《Java 并发编程的艺术》</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java 并发系列</tag>
      </tags>
  </entry>
  <entry>
    <title>ConcurrentHashMap 源码分析</title>
    <url>/2021/05/05/ConcurrentHashMap-Analyse/</url>
    <content><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>ConcurrentHashMap 是高性能的线程安全的哈希容器。</p>
<p>值得一提的是 ConcurrentHashMap 的 key 和 value 不允许 null 值。</p>
<p>Doug Lea 和 Josh Bloch 对 HashMap，ConcurrentHashMap key 允不允许 null 值的讨论。</p>
<ul>
<li><p><a href="http://cs.oswego.edu/pipermail/concurrency-interest/2006-May/002485.html?fileGuid=Hxch6t3HxHCRkVqG">http://cs.oswego.edu/pipermail/concurrency-interest/2006-May/002485.html</a></p>
</li>
<li><p><a href="http://cs.oswego.edu/pipermail/concurrency-interest/2006-May/002486.html?fileGuid=Hxch6t3HxHCRkVqG">http://cs.oswego.edu/pipermail/concurrency-interest/2006-May/002486.html</a></p>
<span id="more"></span></li>
</ul>
<p>邮件里大致的内容说明是因为 null 值有二义性。在并发环境下，如果你通过 get(key) 得到了一个 null 值，无法判断是因为 map 中不存在这个 key 还是因为 key 对应的 value 是 null。在并发环境下，你无法通过调用 containsKey() 来确定。</p>
<p>因为不允许 null 值，所以在 ConcurrentHashMap 中，可以通过 get() 是否为 null 来直接判断是否 contains 一个 key。</p>
<h1 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h1><h2 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a>存储结构</h2><p>JDK 1.8 之后，采用的存储结构与 HashMap 一样，Node&lt;K,V&gt;[] table。不同的是 Node 中的 val 和 next 用<strong>volatile</strong>来修饰，用于保证可见性。</p>
<p><img src="/2021/05/05/ConcurrentHashMap-Analyse/image-20210925124000740.png" alt="image-20210925124000740"></p>
<h2 id="put-方法"><a href="#put-方法" class="headerlink" title="put() 方法"></a>put() 方法</h2><ol>
<li><p><img src="/2021/05/05/ConcurrentHashMap-Analyse/image-20210925124058747.png" alt="image-20210925124058747">rehash，保证均匀分布</p>
</li>
<li><p>索引定位后，如果该槽上没有值，通过 CAS 更新。这里的 tabAt，casTabAt 需要利用到 volatile 的可见性来保证正确。</p>
</li>
<li><p>表正在扩容</p>
</li>
<li><p>都不满足，对索引定位到的槽加锁操作。</p>
</li>
<li><p>槽节点的 hash 值大等 0，说明该槽上不是红黑树。按照拉链法的方式来插入值。</p>
</li>
<li><p>槽根节点的 hash 值小于 0，说明是树节点，走红黑树的插入逻辑。</p>
</li>
<li><p>插入后，判断阈值来进行树化。</p>
<p><img src="/2021/05/05/ConcurrentHashMap-Analyse/image-20210925124118357.png" alt="image-20210925124118357"></p>
</li>
</ol>
<p>JDK 1.8 之后，利用了 CAS+synchronized，相比于 1.7 的锁定 Segment 的方式，取消了ReentrantLock 使用synchronized（看来 1.8 对于 synchronized 的优化很可观。网上的资源提到 synchronized 的锁升级，其实在 1.6 就已经引入），优化了锁的粒度和使用。</p>
<h2 id="get-方法"><a href="#get-方法" class="headerlink" title="get() 方法"></a>get() 方法</h2><p>get 方法相对比较简单，因为使用了<strong>volatile</strong>修饰 val 和 next 变量，get 的时候可以无锁操作。</p>
<p><img src="/2021/05/05/ConcurrentHashMap-Analyse/image-20210925124156045.png" alt="image-20210925124156045"></p>
<ol>
<li><p>索引定位哈希槽的位置上是否有值，没有的话直接返回 null。有的话，比较哈希槽上根节点的 key ，一致的话直接返回 val 值。</p>
</li>
<li><p>哈希槽根节点的 hash 值小于 0 ，说明是红黑树，走红黑树的查找方法。</p>
</li>
<li><p>都不满足的话，在链表中往下遍历查找。</p>
</li>
</ol>
<p><img src="/2021/05/05/ConcurrentHashMap-Analyse/image-20210925124249047.png" alt="image-20210925124249047"></p>
<h1 id="硬件同步原语-CAS"><a href="#硬件同步原语-CAS" class="headerlink" title="硬件同步原语 CAS"></a>硬件同步原语 CAS</h1><p>硬件同步原语是由计算机硬件提供的一组原子操作，具体来说就是 CPU 提供的实现，可以保证指令操作的原子性。</p>
<p>CAS（Compare and Swap）的意思是，先获取某个想要修改的旧值，然后在修改的时候，比较当前的值和旧值，如果一致，就更新为新的，返回 true。否则就不改变，返回 false。</p>
<p>还有一个常用的 FAA（Fetch and Add），的作用是获取某个变量的值，然后将变量的值增加，然后返回旧值。</p>
<p>在各种高级编程语言中，这些原语一般都有相应的实现。</p>
<p>比如 JDK 中提供的 CAS 。</p>
<p><img src="/2021/05/05/ConcurrentHashMap-Analyse/image-20210925124311450.png"></p>
<p>Java 中的 FAA 好像是通过 CAS 实现的。</p>
<h3 id><a href="#" class="headerlink" title></a><img src="/2021/05/05/ConcurrentHashMap-Analyse/image-20210925124327406.png" alt="image-20210925124327406"></h3><h3 id="CAS-的-ABA-问题"><a href="#CAS-的-ABA-问题" class="headerlink" title="CAS 的 ABA 问题"></a>CAS 的 ABA 问题</h3><p>CAS 在写入时仅仅判断当前值和旧值，期间值可能改变过，但是无法判断。Java 提供了<a href="http://tutorials.jenkov.com/java-util-concurrent/atomicstampedreference.html?fileGuid=Hxch6t3HxHCRkVqG">AtomicStampedReference</a>工具类，通过版本号的方式来完善这个问题。</p>
<h3 id="CAS-的开销和优点"><a href="#CAS-的开销和优点" class="headerlink" title="CAS 的开销和优点"></a>CAS 的开销和优点</h3><p>使用 CAS 可以避免使用锁，减小开销。但是 CAS 面对频繁的资源竞争的话，一直反复采用 CAS 尝试更新失败概率高，CPU 的开销也随之变大。</p>
<p>CAS 像是乐观锁的思路，总认为修改成功的概率很高。悲观锁的思路就是共享资源的竞争可能很频繁，就采用独占的方式操作，比如  synchronized 关键字。</p>
<h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>ConcurrentHashMap 是高性能的线程安全的容器，但并不意味着使用它就没有安全问题。比如其 size() ，putALL() 等方法在并发情况下只能反映中间情况。</p>
<p>使用 ConcurrentHashMap 的时候，对其的多个操作之间仍然不是原子性的，如果需要的话可以对 map 加锁操作。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>ConcurrentHashMap 在 1.7 之前采用分段锁的形式，默认并发 16，取决于 Segment。1.8 之后的存储结构和 HashMap 类似，且采用 CAS + synchronized 来保证原子性的读写操作。</p>
<p>使用 ConcurrentHashMap 、CopyOnWriteArrayList 等线程安全的工具类，并不意味着就没有线程安全问题，有关并发安全的知识则需要自己学习运用。</p>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><ol>
<li><a href="https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentHashMap.html?fileGuid=Hxch6t3HxHCRkVqG">https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentHashMap.html</a>，ConcurrentHashMap 官方文档</li>
<li><a href="http://tutorials.jenkov.com/java-util-concurrent/atomicstampedreference.html?fileGuid=Hxch6t3HxHCRkVqG">http://tutorials.jenkov.com/java-util-concurrent/atomicstampedreference.html</a>，AtomicStampedReference 使用教程</li>
<li><a href="https://en.wikipedia.org/wiki/Compare-and-swap?fileGuid=Hxch6t3HxHCRkVqG">https://en.wikipedia.org/wiki/Compare-and-swap</a>，CAS 原语</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java 集合</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Java 并发编程（一）</title>
    <url>/2021/08/11/Concurrent-Programming/</url>
    <content><![CDATA[<h1 id="可见性、有序性和原子性"><a href="#可见性、有序性和原子性" class="headerlink" title="可见性、有序性和原子性"></a>可见性、有序性和原子性</h1><h2 id="可见性"><a href="#可见性" class="headerlink" title="可见性"></a>可见性</h2><p>多核 CPU ，每个 CPU 都有属于自己的多层缓存可以提高访问数据的速度以及减少共享内存的竞争。多核 CPU，每个线程对共享变量都可能通过缓存来访问，<strong>不同线程之间对共享变量的修改是否立即可见（刷新到主存并且更新缓存），一般称为可见性。</strong><span id="more"></span></p>
<h2 id="有序性"><a href="#有序性" class="headerlink" title="有序性"></a>有序性</h2><p>编译器和处理器为了优化性能对<strong>指令进行重排序导致的有序性问题</strong>。比如我们常见的 new 一个对象的操作。我们以为的顺序应该是：</p>
<ol>
<li>分配一块内存</li>
<li>在该内存上初始化对象</li>
<li>将该内存的地址赋值在对象变量</li>
</ol>
<p>但是有优化后的顺序可能是这样的：</p>
<ol>
<li>分配一块内存</li>
<li>将该内存的地址赋值给对象变量</li>
<li>再在该内存上初始化对象</li>
</ol>
<p>以上的重排序问题可以导致 Double-Check 创建单例模式的时候出现问题。为了避免这个问题，需要使用 volatile 定义变量来避免重排序。</p>
<p>编译器和处理器会对指令进行重排序，<strong>不会影响到单核处理和单线程处理情况的语义正确性</strong>，<strong>但不保证在多线程环境下的正确性。</strong></p>
<h2 id="原子性"><a href="#原子性" class="headerlink" title="原子性"></a>原子性</h2><p>原子性是指一个操作是不可中断的。在多线程环境中，一个线程在执行某个原子性的逻辑时，不会被干扰。</p>
<p>Java 并发程序在执行时，是可能发生线程切换的，线程切换可能发生在<strong>任一 CPU 指令之间，而不是我们在高级语言层面看到所认为的。</strong>最常见的自增操作 i++ 在 CPU 层面可以分成 3 步操作：</p>
<ol>
<li>将 i 的值从内存中加载到 CPU 寄存器</li>
<li>执行 i + 1 的操作</li>
<li>写回内存（可能是 CPU 缓存）。</li>
</ol>
<p>线程切换可能发生在我们直觉上认为应该是一个整体的操作，这也是导致并发问题的原因之一。</p>
<h1 id="如何解决这些问题？"><a href="#如何解决这些问题？" class="headerlink" title="如何解决这些问题？"></a>如何解决这些问题？</h1><p>我们知道可见性问题是因为缓存，有序性是因为重排序。那想要解决这两个问题最粗暴的思路就是禁用缓存和指令重排序优化（cpu 和编译器），这样应用程序相当于放弃了这些优化技术发展带来的性能优势。</p>
<p>我们的程序并不是所有的地方都有上面说的问题，所以可以按需禁用缓存以及编译优化。</p>
<h2 id="Java-内存模型"><a href="#Java-内存模型" class="headerlink" title="Java 内存模型"></a>Java 内存模型</h2><p>The Java Memory Model describes what behaviors are legal in multithreaded code, and how threads may interact through memory. It describes the relationship between variables in a program and the low-level details of <strong>storing and retrieving them to and from memory</strong> or registers in a real computer system. <strong>It does this in a way that can be implemented correctly using a wide variety of hardware</strong> and a wide variety of <strong>compiler optimizations.</strong></p>
<p>JMM 围绕带来线程安全问题的有序性和可见性问题，<strong>规范了 JVM 为我们提供避免这些问题的基本实现规则。</strong></p>
<p>站在程序员的角度来说，JMM 为我们提供了 volatile，synchronized 和 final 关键字，以及 6 个和我们有关的 Happens-Before 规则。</p>
<ul>
<li><a href="https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html">https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html</a>，JSR-133 FAQ</li>
<li><a href="http://gee.cs.oswego.edu/dl/cpj/jmm.html">http://gee.cs.oswego.edu/dl/cpj/jmm.html</a>，Conucrrent Programming in Java，Doug Lea</li>
</ul>
<h2 id="volatile"><a href="#volatile" class="headerlink" title="volatile"></a>volatile</h2><p>使用 volatile 关键字声明变量，表示对这个变量的读写，禁用 CPU 缓存，避免可见性问题。volatile 在 1.5 前后有语义增强，通过 Happens-Before 规则确保。</p>
<h2 id="final-关键字"><a href="#final-关键字" class="headerlink" title="final 关键字"></a>final 关键字</h2><p>final 关键字的意思是表示被修饰的内容的整个生命周期内容都不改变，相当于天然的并发安全，且可以被尽力的优化。</p>
<p> final 关键字修饰的内容在 1.5 以前可能导致线程看到的 final 变量值发现变化。<br><a href="http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#finalWrong">www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#finalWrong</a></p>
<p>现在的 JMM 程序只需要保证构造函数没有 this 逸出就不会出现问题。</p>
<h2 id="Happens-before-Order"><a href="#Happens-before-Order" class="headerlink" title="Happens-before Order"></a>Happens-before Order</h2><p>Two actions can be ordered by a happens-before relationship. If one action happens-before another, <strong>then the first is visible to and ordered before the second</strong>.<br><strong>（前面的操作结果对后续的操作是可见的）</strong></p>
<p>JDK 1.5 开始，Java 采用了新的内存模型。新的内存模型引入了 Happens-before 规则来说明操作之间的可见性。比如下面两个写操作，操作 A 的结果在操作 B 的时候是可见的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x = 55;</span><br><span class="line">volatileVar = 66;</span><br></pre></td></tr></table></figure>

<p>同样的，站在程序员的角度，happens-before 规则对我们理解并发有关的规则有以下几条：</p>
<ul>
<li><strong>程序的顺序性规则；一个线程中的每个操作，都 happens-before 于后续的操作。</strong></li>
<li><strong>volatile 变量的规则：对于一个 volatile 变量的写，happens-before 于后续对这个变量的读。</strong></li>
<li><strong>传递性：A happens-before B，B happens-before C，那么 A happens-before C。</strong></li>
<li><strong>对一个锁的解锁 happens-before 与后续对这个锁的加锁。</strong></li>
<li><strong>对线程调用 start(）的操作 happens-before 于在被调用了 start(）线程中的任何操作。</strong></li>
<li><strong>线程调用其他线程的 join() 方法，其他线程成功返回后，其他线程中的操作 happens-before 线程后续的操作。</strong></li>
</ul>
<p><a href="https://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.4">https://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.4</a></p>
<p><a href="http://ifeve.com/jmm-faq/">Java内存模型FAQ | 并发编程网 – ifeve.com</a></p>
<h1 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h1><p>前面提到的原子性问题，在于线程切换，在多核 CPU 时代，我们必须保证<strong>同一个时刻只有一个线程</strong>在操作（32 位机器上写 long 变量，单纯的禁止线程切换对多核不好使）。</p>
<h2 id="互斥锁模型"><a href="#互斥锁模型" class="headerlink" title="互斥锁模型"></a>互斥锁模型</h2><p><img src="/2021/08/11/Concurrent-Programming/Sni_0110120400.png" alt="Sni_0110120400"><br><strong>资源与锁 1 对 1，每个锁都有自己的锁资源的对象。</strong></p>
<h2 id="synchronized-关键字"><a href="#synchronized-关键字" class="headerlink" title="synchronized 关键字"></a>synchronized 关键字</h2><p>synchronized 可以修饰普通方法、静态方法、和代码块，要注意这三种方式锁的资源不同。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Sync</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span></span>;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">bar</span><span class="params">()</span></span>;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">block</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (obj)&#123;</span><br><span class="line">    </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="Java-线程"><a href="#Java-线程" class="headerlink" title="Java 线程"></a>Java 线程</h1><h2 id="Runnable-接口"><a href="#Runnable-接口" class="headerlink" title="Runnable 接口"></a>Runnable 接口</h2><p>顶层基础接口，实现 run 方法，定义线程启动后执行的任务。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="Thread-类"><a href="#Thread-类" class="headerlink" title="Thread 类"></a>Thread 类</h2><p>Thread 类实现了 Runnable 接口。<br>通过调用 Thread#start() 来创建新线程并执行任务。<br>调用 Thread#run() 则是调用这个方法的线程执行 run 方法。</p>
<h2 id="线程的生命周期"><a href="#线程的生命周期" class="headerlink" title="线程的生命周期"></a>线程的生命周期</h2><h3 id="操作系统层面的线程生命周期"><a href="#操作系统层面的线程生命周期" class="headerlink" title="操作系统层面的线程生命周期"></a>操作系统层面的线程生命周期</h3><p>![image (17)](Concurrent-Programming/image (17).png)</p>
<ul>
<li>初始状态：刚创建</li>
<li>可运行状态：可以分配 CPU 执行，等待分配 CPU </li>
<li>运行状态：获得 CPU，正在执行</li>
<li>休眠状态：阻塞，等待特定事件完成，进入可运行状态</li>
<li>终止状态：执行完成或者发生异常</li>
</ul>
<h3 id="Java-线程的生命周期"><a href="#Java-线程的生命周期" class="headerlink" title="Java 线程的生命周期"></a>Java 线程的生命周期</h3><p>![image (18)](Concurrent-Programming/image (18).png)</p>
<h3 id="线程状态直接的变化"><a href="#线程状态直接的变化" class="headerlink" title="线程状态直接的变化"></a>线程状态直接的变化</h3><p><img src="/2021/08/11/Concurrent-Programming/Sni_0110170903.png" alt="Sni_0110170903"></p>
<h3 id="LOCKSUPPORT-TODO"><a href="#LOCKSUPPORT-TODO" class="headerlink" title="LOCKSUPPORT  TODO"></a>LOCKSUPPORT  TODO</h3><p><img src="/2021/08/11/Concurrent-Programming/Sni_0110171235-3079572.png" alt="Sni_0110171235"></p>
<h1 id="wait-notify-机制"><a href="#wait-notify-机制" class="headerlink" title="wait-notify 机制"></a>wait-notify 机制</h1><p>在程序中，线程进入临界区后，因为某些条件不满足，可以使用 wait() 方法进入等待状态。调用 wait() 后，该线程被阻塞，进入该<strong>互斥锁的等待队列中且释放持有的锁</strong>。（这里的等待队列和锁竞争队列不一样。）</p>
<p>当线程要求的条件满足时，可以调用 notify() 和 notifyAll() 方法通知等待队列中的线程，<strong>条件满足过</strong>。<strong>这里注意调用 notify() 通知等待线程满足条件与线程再执行到条件判断的时间点是不同的，这期间条件可能又不满足。</strong>针对这种情况已经有了经典的做法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">while(条件不满足) &#123;</span><br><span class="line">  wait();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>通过 while 来不断检验条件的满足情况。<br>要注意调用 wait()、notify()、notifyAll() <strong>操作的对象是互斥锁的等待队列</strong>。也就是说调用这几个方法的当前线程肯定是<strong>已经持有了这个锁</strong>才能调用，否则会抛出IllegalMonitorStateException 异常。</p>
<p>![image (16)](Concurrent-Programming/image (16).png)</p>
<h3 id="notify-与-notifyAll-的区别"><a href="#notify-与-notifyAll-的区别" class="headerlink" title="notify() 与 notifyAll() 的区别"></a>notify() 与 notifyAll() 的区别</h3><p>在调用了 wait() 后，当前线程会释放持有的锁，并进入<strong>等待队列。</strong></p>
<p>之后，当其他线程调用了 notify() 时，只会 wake <strong>等待队列中</strong>的一个线程，该线程随之进入<strong>互斥锁的竞争队列竞争锁</strong>，然后在调用 wait() 后的位置继续执行；</p>
<p>如果调用的是 notifyAll()，则会 weak 所有等待队列中的线程，进入锁竞争队列竞争锁。</p>
<h3 id="Thread-sleep-和-Object-wait-的区别"><a href="#Thread-sleep-和-Object-wait-的区别" class="headerlink" title="Thread.sleep() 和 Object.wait() 的区别"></a>Thread.sleep() 和 Object.wait() 的区别</h3><p>调用 sleep() 会将当前线程挂起，操作系统层面会让出 CPU，当前线程不会释放持有的锁。</p>
<p>wait() 方法属于 Object 类。调用 wait() 方法会释放持有的锁，线程进入等待队列，等待 notify(）唤醒后在 wait() 后继续执行。</p>
<p>调用 wait() 必须在同步块中，要保证获取到锁才能调用。</p>
<h3 id="stop-和-interrupt-的区别"><a href="#stop-和-interrupt-的区别" class="headerlink" title="stop() 和 interrupt() 的区别"></a>stop() 和 interrupt() 的区别</h3><p>stop() 方法直接杀死线程，被 stop() 的线程可能没有机会释放锁。</p>
<p>interrupt() 方法会通知线程（<strong>根据被中断线程的状态有不同的反应</strong>）</p>
<ul>
<li>A 处于WAITING、TIME_WAITING，其他线程调用 A 的 interrupt() 方法，线程 A 会返回到 RUNNABLE 状态，同时线程 A 触发 InterruptedException 异常。</li>
<li>A 处于 RUNNABLE 状态，并阻塞在以下两种情况，当其他线程调用 A 的 interrupt(） 方法时：<ul>
<li> java.nio.channels.InterruptibleChannel，<strong>线程 A 会触发 java.nio.channels.ClosedByInterruptException 这个异常。</strong></li>
<li> java.nio.channels.Selector ，<strong>线程 A 的 java.nio.channels.Selector 会立即返回。</strong></li>
</ul>
</li>
</ul>
<p>需要注意抛出异常后，中断标识会自己清除。通过 Thread#isInterrupted() 重置成 false。</p>
<h1 id="线程池"><a href="#线程池" class="headerlink" title="线程池"></a>线程池</h1><h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p>创建线程池的 7 个核心参数。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ThreadPoolExecutor(</span><br><span class="line">  <span class="keyword">int</span> corePoolSize,</span><br><span class="line">  <span class="keyword">int</span> maximumPoolSize,</span><br><span class="line">  <span class="keyword">long</span> keepAliveTime,</span><br><span class="line">  TimeUnit unit,</span><br><span class="line">  BlockingQueue&lt;Runnable&gt; workQueue,</span><br><span class="line">  ThreadFactory threadFactory, <span class="comment">// 定义新线程的属性，比如指定线程名等等</span></span><br><span class="line">  RejectedExecutionHandler handler) </span><br></pre></td></tr></table></figure>



<p><img src="/2021/08/11/Concurrent-Programming/Sni_0210201216.png" alt="Sni_0210201216"></p>
<p>第 3 个步骤创建新线程的步骤需要获取全局锁。有可能导致性能问题，应该尽量避免。</p>
<h2 id="合理配置线程池"><a href="#合理配置线程池" class="headerlink" title="合理配置线程池"></a>合理配置线程池</h2><ul>
<li>使用有界队列</li>
<li>合理配置线程数（core num：Runtime.getRuntime().availableProcessors())<ul>
<li>io 密集型，2 * core num</li>
<li>cpu 密集型，core num + 1</li>
</ul>
</li>
</ul>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>通过 execute() 或者 submit(); 注意 execute() 没有返回值，可能无法知晓任务执行情况（吞异常）;</p>
<p>submit() 重载了 3 个版本:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1. 提交Runnable任务,返回的 Future 只能用来判断任务执行情况</span></span><br><span class="line">Future&lt;?&gt; </span><br><span class="line">  submit(Runnable task);</span><br><span class="line"><span class="comment">//2. 提交Callable任务, Callable 实现的方法有返回值</span></span><br><span class="line">&lt;T&gt; <span class="function">Future&lt;T&gt; </span></span><br><span class="line"><span class="function">  <span class="title">submit</span><span class="params">(Callable&lt;T&gt; task)</span></span>;</span><br><span class="line"><span class="comment">//3. 提交Runnable任务及结果引用, 经典用法是 result 传入给 task</span></span><br><span class="line"><span class="comment">// task 子线程和主线程可以共享 result 数据状态</span></span><br><span class="line">&lt;T&gt; <span class="function">Future&lt;T&gt; </span></span><br><span class="line"><span class="function">  <span class="title">submit</span><span class="params">(Runnable task, T result)</span></span>;</span><br></pre></td></tr></table></figure>

<h2 id="关闭"><a href="#关闭" class="headerlink" title="关闭"></a>关闭</h2><p>shutdown() 和 shutdownNow()</p>
<h1 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h1><p>通过 Future 接口可以很简单的获取到异步任务的执行结果。Future 接口提供了 5 个方法，可以判断任务状态，取消任务，阻塞/超时获取结果。</p>
<p><img src="/2021/08/11/Concurrent-Programming/Snipaste_1510482415.png" alt="Snipaste_1510482415"></p>
<p>在线程池部分我们讲到可以通过 submit() 方法提交任务，通过返回的 Future 获取任务的执行结果。下面代码是 submit() 配合 Future 的 demo。</p>
<p><img src="/2021/08/11/Concurrent-Programming/Snipaste_1510472015.png" alt="Snipaste_1510472015"></p>
<h1 id="CompletableFuture"><a href="#CompletableFuture" class="headerlink" title="CompletableFuture"></a>CompletableFuture</h1><h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><ol>
<li><p><a href="https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html">https://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html</a>，JSR-133 FAQ</p>
</li>
<li><p><a href="http://ifeve.com/jmm-faq/">Java内存模型FAQ | 并发编程网 – ifeve.com</a></p>
</li>
<li><p><a href="https://time.geekbang.org/column/intro/100023901?utm_source=u_nav_web&utm_medium=u_nav_web&utm_term=u_nav_web&tab=catalog">Java并发编程实战 (geekbang.org)</a></p>
</li>
<li><p><a href="https://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.4">https://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.4</a></p>
</li>
<li><p><a href="http://gee.cs.oswego.edu/dl/cpj/jmm.html">http://gee.cs.oswego.edu/dl/cpj/jmm.html</a>，Conucrrent Programming in Java，Doug Lea</p>
</li>
<li><p>《Java 并发编程的艺术》</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java 并发系列</tag>
      </tags>
  </entry>
  <entry>
    <title>GC 策略总结</title>
    <url>/2021/07/20/GC-Algo/</url>
    <content><![CDATA[<h1 id="GC-算法"><a href="#GC-算法" class="headerlink" title="GC 算法"></a>GC 算法</h1><ul>
<li>标记-复制。<ul>
<li>优点：存活的对象越少，复制需要的空间就越小；而且复制后的对象们内存空间排布紧凑，避免空间碎片的问题。</li>
<li>缺点：有一部分空间被浪费。如果存活对象大且多的话复制成本比较高。<span id="more"></span></li>
<li><strong>适用每次 GC 存活对象小而美的情况。年轻代</strong></li>
</ul>
</li>
<li>标记-清除。直接清除可回收对象，不进行内存整理。，<ul>
<li>优点：单次 STW 的时间可能要短一些。</li>
<li>缺点：但是产生的内存碎片，可能导致内存总空间足够，但是没一块连续的空间存放对象的问题，内存利用率降低；对象放不下，可能触发额外的 GC。</li>
<li><strong>适用对象存活率高的情况。老年代</strong></li>
</ul>
</li>
<li>标记-清除-整理。STW 时间可能会稍长一些，内存碎片问题得到解决。适用对象存活率高的情况。</li>
</ul>
<h1 id="Serial-GC"><a href="#Serial-GC" class="headerlink" title="Serial GC"></a>Serial GC</h1><p>-XX:+UseSerialGC</p>
<p>串行 GC 单线程执行，在 GC 期间其他业务线程均暂停，暂停的时间长。</p>
<p>串行 GC 对年轻代采用标记复制算法。对老年代使用标记-清除-整理算法。</p>
<p>串行 GC 简单直接，在单核 CPU 环境下比较适用。</p>
<h2 id="XX-UseParNewGC"><a href="#XX-UseParNewGC" class="headerlink" title="-XX:+UseParNewGC"></a>-XX:+UseParNewGC</h2><p>ParNew 收集器，多线程版本的 Serial。配合 CMS 使用。</p>
<h1 id="Parallel-GC"><a href="#Parallel-GC" class="headerlink" title="Parallel GC"></a>Parallel GC</h1><p>-XX:UseParallelGC -XX:UseParallelOldGC</p>
<p>使用的 GC 算法和串行的一样。</p>
<p>默认的 GC 线程数是 CPU core 数，该收集器的目标更倾向于<strong>提高系统吞吐量</strong>，有时候单次的 GC 暂停时间较长。</p>
<h1 id="CMS-GC"><a href="#CMS-GC" class="headerlink" title="CMS GC"></a>CMS GC</h1><p>-XX:UseConcMarkSweepGC</p>
<p>对老年代没有整理操作，使用 free-list 进行内存空间的管理。默认的核心线程数 CPU 核数 / 4。</p>
<p>可以和业务线程并发执行，GC 暂停时间少。</p>
<h1 id="G1-GC"><a href="#G1-GC" class="headerlink" title="G1 GC"></a>G1 GC</h1><p>打破整个分区的理论，把内存划分成多个小块进行管理。对每个小块的垃圾数量进行预估，优先回收垃圾多的 Region。可预期的垃圾停顿时间。</p>
<h1 id="验证总结"><a href="#验证总结" class="headerlink" title="验证总结"></a>验证总结</h1><p>首先需要提到的一点是 GC 的时间和存活的对象数量有关，和堆内存的大小关系没有那么大。</p>
<p>配置堆内存 512M，YGC 后年轻代存活对象大概 20M。</p>
<ul>
<li>串行 GC 利用单线程执行，GC 暂停的时间明显会比较长。在实际的测试下，在小堆内存空间的情况下，YGC 和并行 GC 的 YGC 差不多。FGC 使用的时间明显较长，<strong>大概是并行 GC 的一倍（存活对象 300M 左右）</strong>。老年代存活对象占用的空间大，整理移动的时间就长。</li>
<li>CMS GC 的老年代清理明显的暂停时间降低。在 GC 日志中有发现 concurrent mode failure 的情况。查询资料后明白，CMS 在 cleanup 是并发执行的，这时的对象引用关系发生改变，也可能有新的对象需要分配空间。如果没有预留足够的空间内存分配就会导致并发失败。可能重新 CMS ，或者 GC 退化成 Serial。</li>
<li>G1 GC 出现了 Humongous Allocation 因为大对象分配失败，触发了 initial-mark。也是重新标记，或者 GC 退化的问题。</li>
</ul>
<p>堆内存越大，内存中可容纳的对象越多，GC 的次数随之减少，单次 GC 的暂停时间可能更长（取决于存活对象的数量）。</p>
<p>总的来说，注意不同 GC 策略采用的算法，以及设计的目的。比如 CMS 在于并发执行，提高系统响应。Parallel 更倾向于提高吞吐量；G1 GC 倾向于可配置可预估的暂停时间。</p>
<p>CMS - 老年代 没有整理，使用 free-list 管理回收内存；真正的 STW 时间小，但是步骤多，还有浮动垃圾，GC 退化问题。G1 GC 也存在 GC 退化问题。</p>
<p>配置堆内存的时候，注意 JVM 自身需要的内存和系统需要的内存，预留一定的空间。</p>
<p>-Xms -Xmx 直接一步到位，扩容的时候有性能的抖动。</p>
<p>年轻代和老年代的比例默认 1:2，新生代:from:to = 8：1：1，根据情况来调整。</p>
<p>根据对象晋升回收速率的计算，进行空间，晋升年龄的配置。</p>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>jvm</tag>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title>HashMap 源码分析</title>
    <url>/2021/04/26/HashMap-SourceCode-Anaylise/</url>
    <content><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="哈希算法"><a href="#哈希算法" class="headerlink" title="哈希算法"></a>哈希算法</h2><p>哈希算法的作用是：对于输入的数据，输出<strong>固定长度</strong>的数据摘要。</p>
<p>哈希算法的特点：<span id="more"></span></p>
<ol>
<li>同一个输入，输出的哈希值一定是一致的。</li>
<li>不同输入，哪怕是一个 Bit 的差别，得到的哈希值有明显的差别。</li>
<li>对于不同的输入，是有可能得到相同的哈希值的。这种情况一般也称作哈希碰撞。原因很简单，因为计算得到的哈希值是固定长度的，总量有限制。而输入的值可能是无限的。</li>
</ol>
<p>一个优秀的哈希算法至少得满足两点：</p>
<ol>
<li>计算哈希值的速度很快。</li>
<li>出现哈希碰撞的概率很低。</li>
</ol>
<p>哈希算法的实际应用场景很多，常见的有这几种：</p>
<ol>
<li>数据加密。符合的点有两个：很难根据哈希值反推出原始数据；哈希冲突的概率低；</li>
<li>数据校验。校验文件的完整性。</li>
<li>哈希表。</li>
</ol>
<p>对于业务上保存用户的密码，有一些思路顺便记录一下。</p>
<ul>
<li>hash + 随机 salt。</li>
<li>采用计算时间慢的算法来降低硬件计算的速度。</li>
<li>不规律的计算时间，避免得到与密码有关的联系的信息，比如字符串的长度信息。</li>
<li>时序攻击：<a href="https://www.zhihu.com/question/20156213?fileGuid=9CXxDy6PxXx6yDVH">https://www.zhihu.com/question/20156213</a>。</li>
</ul>
<h2 id="HashMap"><a href="#HashMap" class="headerlink" title="HashMap"></a>HashMap</h2><p>HashMap 根据 key 的 hashCode 值，寻找对应的位置保存数据。在没有哈希冲突的前提下，可以通过 O(1) 的时间复杂度定位到的 key。</p>
<p>Java 的 HashMap 通过额外链表法来解决哈希冲突的问题。在 Java 1.8 之后，如果某个哈希槽上的链表元素个数超过了 TREEIFY_THRESHOLD ，会将链表树化为红黑树，进一步提高性能。</p>
<h1 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h1><h2 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a>存储结构</h2><p>HashMap 类中的字段 Node&lt;K,V&gt;[] table，即使用 Node&lt;K,V&gt; 数组来存储数据。</p>
<p><img src="/2021/04/26/HashMap-SourceCode-Anaylise/image-20210925122839366.png" alt="image-20210925122839366">Node&lt;K,V&gt; 中有一个 Node&lt;K,V&gt; next 字段，当某个哈希槽上已经存储了数据，next 就用来在该哈希槽上拉出链表来解决哈希冲突。</p>
<p>HashMap 的默认字段定义</p>
<h2 id><a href="#" class="headerlink" title></a><img src="/2021/04/26/HashMap-SourceCode-Anaylise/image-20210925122854595.png" alt="image-20210925122854595"></h2><h2 id="hash-Object-key-方法"><a href="#hash-Object-key-方法" class="headerlink" title="hash(Object key) 方法"></a>hash(Object key) 方法</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public V put(K key, V value) &#123;</span><br><span class="line">return putVal(hash(key), key, value, false, true);</span><br><span class="line">&#125;</span><br><span class="line">static final int hash(Object key) &#123;</span><br><span class="line">    int h;</span><br><span class="line">    return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>不管是在 put 或者 get，都会先通过 hash(key) ，根据 key 的 hashCode 重新计算一个 hash 值用于索引定位。<br>事先通过 hash(key) 将 key 的 hashCode 重新散列，将 hashCode 的高位向右移 16 位，异或计算后得到的 hash 值进行索引定位（否则这些高位可能由于 (table.length -1) &amp; hash 的取模方式永远参与不到取模的运算中，取模运算的结果其实就是 hash 中对应的后几位的值）。</p>
<h2 id="索引定位方式"><a href="#索引定位方式" class="headerlink" title="索引定位方式"></a>索引定位方式</h2><p>HashMap 的源码定位哈希槽的位置的方式是通过位运算计算哈希槽的位置，具体的计算方式是：**(table.length - 1) &amp; hash**。</p>
<p>由于 table.length 在初始化或者扩容后总是<strong>取 2 的某个幂次方数</strong>，在将其减去 1 之后，二进制的低位上数据都是 1，再于 hash 进行 &amp; 运算，将计算后的值限制在 table.length 内。相当于高效率的 % 运算。</p>
<h2 id="插入方法"><a href="#插入方法" class="headerlink" title="插入方法"></a>插入方法</h2><p><img src="/2021/04/26/HashMap-SourceCode-Anaylise/image-20210925122935800.png" alt="image-20210925122935800"></p>
<p>(图来自美团技术团队博客)</p>
<h2 id="扩容机制"><a href="#扩容机制" class="headerlink" title="扩容机制"></a>扩容机制</h2><p>HashMap 扩容默认是原始容量的两倍。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> Node&lt;K,V&gt;[] resize() &#123;</span><br><span class="line">    Node&lt;K,V&gt;[] oldTab = table;</span><br><span class="line">    <span class="keyword">int</span> oldCap = (oldTab == <span class="keyword">null</span>) ? <span class="number">0</span> : oldTab.length;</span><br><span class="line">    <span class="keyword">int</span> oldThr = threshold;</span><br><span class="line">    <span class="keyword">int</span> newCap, newThr = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (oldCap &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// 超过最大容量，随你碰撞好了</span></span><br><span class="line">        <span class="keyword">if</span> (oldCap &gt;= MAXIMUM_CAPACITY) &#123;</span><br><span class="line">            threshold = Integer.MAX_VALUE;</span><br><span class="line">            <span class="keyword">return</span> oldTab;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// oldCap 向左移动一位，newCap = 2 *oldCap </span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> ((newCap = oldCap &lt;&lt; <span class="number">1</span>) &lt; MAXIMUM_CAPACITY &amp;&amp;</span><br><span class="line">                 oldCap &gt;= DEFAULT_INITIAL_CAPACITY)</span><br><span class="line">            newThr = oldThr &lt;&lt; <span class="number">1</span>; <span class="comment">// double threshold</span></span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">    <span class="meta">@SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;)</span></span><br><span class="line">    Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node[newCap];</span><br><span class="line">    table = newTab;</span><br><span class="line">    <span class="comment">//数据重新进行索引定位</span></span><br><span class="line">    <span class="keyword">if</span> (oldTab != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; oldCap; ++j) &#123;</span><br><span class="line">            Node&lt;K,V&gt; e;</span><br><span class="line">            <span class="keyword">if</span> ((e = oldTab[j]) != <span class="keyword">null</span>) &#123;</span><br><span class="line">                oldTab[j] = <span class="keyword">null</span>;</span><br><span class="line">                <span class="comment">// 该哈希槽上没有哈希冲突，重新索引定位位置存储</span></span><br><span class="line">                <span class="keyword">if</span> (e.next == <span class="keyword">null</span>) </span><br><span class="line">                    newTab[e.hash &amp; (newCap - <span class="number">1</span>)] = e;</span><br><span class="line">                <span class="comment">// 该哈希槽上的节点是 TreeNode，</span></span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (e <span class="keyword">instanceof</span> TreeNode)</span><br><span class="line">                    ((TreeNode&lt;K,V&gt;)e).split(<span class="keyword">this</span>, newTab, j, oldCap);</span><br><span class="line">                <span class="comment">// 该哈希槽上的节点存在哈希冲突保存的其他元素</span></span><br><span class="line">                <span class="keyword">else</span> &#123; <span class="comment">// 链表数据重新定位</span></span><br><span class="line">                    <span class="comment">// loHead,loTail 的含义是重新进行索引定位后仍在原哈希槽位置上的节点和链表节点元素。比如 e = oldTab[j]，重新索引定位后，newTab[j] = e;</span></span><br><span class="line">                    Node&lt;K,V&gt; loHead = <span class="keyword">null</span>, loTail = <span class="keyword">null</span>;</span><br><span class="line">                    <span class="comment">// hiHead,hiTail 的含义是重新进行索引定位后的节点和链表节点元素在 j + oldCap 上。比如 e = oldTab[j]，重新索引定位后，newTab[j + oldCap] = e;</span></span><br><span class="line">                    Node&lt;K,V&gt; hiHead = <span class="keyword">null</span>, hiTail = <span class="keyword">null</span>;</span><br><span class="line">                    Node&lt;K,V&gt; next;</span><br><span class="line">                    <span class="keyword">do</span> &#123;</span><br><span class="line">                        next = e.next;</span><br><span class="line">                        <span class="comment">//计算哈希值的高一位是 0 还是 1</span></span><br><span class="line">                        <span class="keyword">if</span> ((e.hash &amp; oldCap) == <span class="number">0</span>) &#123; <span class="comment">// 为 0</span></span><br><span class="line">                            <span class="keyword">if</span> (loTail == <span class="keyword">null</span>)</span><br><span class="line">                                loHead = e;</span><br><span class="line">                            <span class="keyword">else</span></span><br><span class="line">                                loTail.next = e;</span><br><span class="line">                            loTail = e;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">else</span> &#123; <span class="comment">// 高一位为 1</span></span><br><span class="line">                            <span class="keyword">if</span> (hiTail == <span class="keyword">null</span>)</span><br><span class="line">                                hiHead = e;</span><br><span class="line">                            <span class="keyword">else</span></span><br><span class="line">                                hiTail.next = e;</span><br><span class="line">                            hiTail = e;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; <span class="keyword">while</span> ((e = next) != <span class="keyword">null</span>);</span><br><span class="line">                    <span class="keyword">if</span> (loTail != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        loTail.next = <span class="keyword">null</span>;</span><br><span class="line">                        <span class="comment">// 高一位为 0 时，元素的存储位置数组下标没有变化</span></span><br><span class="line">                        newTab[j] = loHead;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (hiTail != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        hiTail.next = <span class="keyword">null</span>;</span><br><span class="line">                        <span class="comment">// 为 1，元素存储位置为 旧数组下标 + 原容量 。</span></span><br><span class="line">                        newTab[j + oldCap] = hiHead;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> newTab;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>重点看一下 43 行对链表的 rehash 操作。代码为什么通过 (e.hash &amp; oldCap) == 0 || != 0 就可以判断元素经过 rehash 后在新哈希桶中的位置。<br>因为 HashMap 的索引定位方式是 (table.length - 1) &amp; hash ，且扩容之后的容量是原容量的 2 倍。索引定位方式舍弃了 table.length 的那个高位 1，经过扩容后，rehash 的索引定位方式相比于原来的只需要计算之前舍弃的那个高位 1 就可以确实位置。<strong>这样在扩容时重新定位元素时巧妙的避免了重新计算 hash(key) 值。</strong></p>
<p>举个例子，HashMap 的大小是 8 , 插入元素的时候触发了扩容，扩容后的大小是 16。</p>
<p>有一个 key A 经过 hash(key) 后的 hash 值是 0100 0110。</p>
<p><img src="/2021/04/26/HashMap-SourceCode-Anaylise/image-20210925123011727.png" alt="image-20210925123011727">11-14 行计算的是 k.hash &amp; oldCap，即如果结果是 0，说明 key A 的存储位置还是在 [6] 中。如果不为 0，说明第四位是 1，那么第 9 行的结果应该是 0000 1110 = 14 , 就是 j + oldCap 的位置。</p>
<h2 id="hashCode-与-equals-方法"><a href="#hashCode-与-equals-方法" class="headerlink" title="hashCode() 与 equals() 方法"></a>hashCode() 与 equals() 方法</h2><p>重写了 equals() 方法，没有重写 hashCode() 方法。当我们用该对象作为 key 时，且业务上有 “逻辑相等” 的概念时，可能会导致预期外的行为。没有重写 hashCode() 方法的话，会使用 Object#hashCode() 方法，该方法生成的哈希值无法提供 “逻辑相同” 的概念。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol>
<li>使用 HashMap 存储时，如果有 “逻辑相等” 的概念，需要同时重写 hashCode() 方法。</li>
<li>在初始化的时候根据需求给定一个合适的大小，避免频繁扩容。</li>
<li>HashMap 不是线程安全的容器，JDK 1.7 之前的实现并发使用在扩容搬移时可能出现 “无限循环” bug。</li>
<li>线程安全的类似 HashMap 有 Hashtable 和 ConcurrentHashMap 可以使用。</li>
</ol>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://tech.meituan.com/2016/06/24/java-hashmap.html?fileGuid=9CXxDy6PxXx6yDVH">https://tech.meituan.com/2016/06/24/java-hashmap.html</a>，美团技术团队博客</li>
<li><a href="https://en.wikipedia.org/wiki/Timing_attack?fileGuid=9CXxDy6PxXx6yDVH">https://en.wikipedia.org/wiki/Timing_attack</a>，时序攻击</li>
<li><a href="https://en.wikipedia.org/wiki/Hash_function?fileGuid=9CXxDy6PxXx6yDVH">https://en.wikipedia.org/wiki/Hash_function</a>，哈希函数</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java 集合</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Java 字节码分析</title>
    <url>/2021/09/10/JVM-Bytecode-Analysis/</url>
    <content><![CDATA[<p>Java 字节码由 byte 组成，理论上可以支持 2^8 个操作码，实际中 Java 只用到了 200 个左右的操作码。</p>
<p>在 JVM 执行过程中，每个线程都有私有的线程栈，线程执行程序时每一次方法调用都会创建一个 <strong>栈帧</strong>。栈帧由操作数栈、局部变量数组和 <a href="https://www.artima.com/insidejvm/ed2/jvm8.html">Frame Data</a> 组成。<span id="more"></span></p>
<h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.training.advanced.jvm;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 字节码分析四则运算，流程控制</span></span><br><span class="line"><span class="comment"> * 具体分析见 Analysis01.txt</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> crayon</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ByteCodeAnalysis01</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">double</span> b = <span class="number">5.0</span>;</span><br><span class="line">        <span class="keyword">boolean</span> c = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">int</span> a = <span class="number">6</span>;</span><br><span class="line">        <span class="keyword">short</span> d = <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">byte</span> e = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (; b &lt; <span class="number">9.0</span>; b++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (b &lt; <span class="number">6.0</span>) c = <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        add(a);</span><br><span class="line">        b--;</span><br><span class="line">        <span class="keyword">if</span> (c) b = a * b;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">add</span><span class="params">(<span class="keyword">double</span> num)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> num++;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="反编译字节码"><a href="#反编译字节码" class="headerlink" title="反编译字节码"></a>反编译字节码</h3><p>通过 javac 编译生成对应的 .class 文件后，使用 javap -c 进行反汇编即可查看可读性较好的字节码文件。一般同时会加上 -l 和 -verbose 。</p>
<h3 id="字节码调用流程分析"><a href="#字节码调用流程分析" class="headerlink" title="字节码调用流程分析"></a>字节码调用流程分析</h3><p>线程执行方法时分配的栈帧的本地变量表的操作数栈在编译期已经确定。可以看到主方法分配 stack=4, locals=7。在进行字节码调用分析的时候，主要就是 load - store 组成。load 表示从本地变量表加载到操作数栈顶进行操作。store 表示将栈顶元素保存的本地变量表上。</p>
<p><img src="/2021/09/10/JVM-Bytecode-Analysis/Snipaste_1012135213.png" alt="Snipaste_1012135213"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">  Last modified 2021-8-3; size 548 bytes</span><br><span class="line">  MD5 checksum ab425321e9039c3964daf5738da62537</span><br><span class="line">  Compiled from &quot;ByteCodeAnalysis.java&quot;</span><br><span class="line">public class jvm.ByteCodeAnalysis</span><br><span class="line">  minor version: 0</span><br><span class="line">  major version: 52</span><br><span class="line">  flags: ACC_PUBLIC, ACC_SUPER</span><br><span class="line">Constant pool:</span><br><span class="line">   #1 = Methodref          #10.#23        // java/lang/Object.&quot;&lt;init&gt;&quot;:()V</span><br><span class="line">   #2 = Double             5.0d</span><br><span class="line">   #4 = Double             9.0d</span><br><span class="line">   #6 = Double             6.0d</span><br><span class="line">   #8 = Methodref          #9.#24         // jvm/ByteCodeAnalysis.add:(D)D</span><br><span class="line">   #9 = Class              #25            // jvm/ByteCodeAnalysis</span><br><span class="line">  #10 = Class              #26            // java/lang/Object</span><br><span class="line">  #11 = Utf8               &lt;init&gt;</span><br><span class="line">  #12 = Utf8               ()V</span><br><span class="line">  #13 = Utf8               Code</span><br><span class="line">  #14 = Utf8               LineNumberTable</span><br><span class="line">  #15 = Utf8               main</span><br><span class="line">  #16 = Utf8               ([Ljava/lang/String;)V</span><br><span class="line">  #17 = Utf8               StackMapTable</span><br><span class="line">  #18 = Class              #27            // &quot;[Ljava/lang/String;&quot;</span><br><span class="line">  #19 = Utf8               add</span><br><span class="line">  #20 = Utf8               (D)D</span><br><span class="line">  #21 = Utf8               SourceFile</span><br><span class="line">  #22 = Utf8               ByteCodeAnalysis.java</span><br><span class="line">  #23 = NameAndType        #11:#12        // &quot;&lt;init&gt;&quot;:()V</span><br><span class="line">  #24 = NameAndType        #19:#20        // add:(D)D</span><br><span class="line">  #25 = Utf8               jvm/ByteCodeAnalysis</span><br><span class="line">  #26 = Utf8               java/lang/Object</span><br><span class="line">  #27 = Utf8               [Ljava/lang/String;</span><br><span class="line">&#123;</span><br><span class="line">  ...</span><br><span class="line">  public static void main(java.lang.String[]);</span><br><span class="line">    descriptor: ([Ljava/lang/String;)V</span><br><span class="line">    flags: ACC_PUBLIC, ACC_STATIC</span><br><span class="line">    Code:</span><br><span class="line">      stack=4, locals=7, args_size=1    // 栈深度 4（注意long，double），本地变量表所需槽位 7 个，参数 1 个</span><br><span class="line">         0: ldc2_w        #2    // 从常量池 push #2 到操作数栈                // double 5.0d</span><br><span class="line">         3: dstore_1            // 将操作数栈上的栈顶元素保存到局部变量表槽 1 的位置</span><br><span class="line">         4: iconst_0            // push 0 到操作数栈顶, boolean类型 0 1 代替</span><br><span class="line">         5: istore_3            // 保存到局部变量表槽 3 的位置</span><br><span class="line">         6: bipush        6     // push 6 到操作数栈顶</span><br><span class="line">         8: istore        4     // 保存到表槽 4 的位置</span><br><span class="line">        10: iconst_2</span><br><span class="line">        11: istore        5     // 将栈顶  元素 2 保存到表槽 5 的位置</span><br><span class="line">        13: iconst_5</span><br><span class="line">        14: istore        6</span><br><span class="line">        16: dload_1             // 将表槽 1 位置上的值load到操作数栈上, 表位置1上的是 double 5.0d, (for 循环比较条件数值)</span><br><span class="line">        17: ldc2_w        #4    // 加载运行时常量池 #4 9.0d 到操作数栈上，此时为 5.0d,9.0d              // double 9.0d</span><br><span class="line">        20: dcmpg               // 从操作数栈 pop value1-9.0d，value2-5.0d, 比较后，入栈 -1</span><br><span class="line">        21: ifge          41    // 栈顶元素和 0 做比较，value &gt;= 0 -&gt; succeed -&gt; 41, 这边是false，不跳转到偏移量41处</span><br><span class="line">        24: dload_1             // push 5.0d 到栈顶</span><br><span class="line">        25: ldc2_w        #6    // push 6.0d 到栈顶                  // double 6.0d</span><br><span class="line">        28: dcmpg               // pop 并比较 6.0d 和 5.0d，入栈 -1    // 第二次循环进入，比较 6.0d 和 6.0d,入栈 0</span><br><span class="line">        29: ifge          34    // false,表示 if(5.0d &lt; 6.0d) 成立，进入条件赋值语句    // 0 &gt;= 0,ture,跳到 34,条件不成立</span><br><span class="line">        32: iconst_1            // push int 1 ，这里表示 true</span><br><span class="line">        33: istore_3            // 赋值到槽 3 上，将 c 赋值为 true</span><br><span class="line">        34: dload_1</span><br><span class="line">        35: dconst_1</span><br><span class="line">        36: dadd                // b = b + 1,6.0d</span><br><span class="line">        37: dstore_1            // 更新局部变量表</span><br><span class="line">        38: goto          16    // 跳到 16 处继续执行</span><br><span class="line">        41: iload         4     // 槽 4 保存的是 int a = 6，push 到栈</span><br><span class="line">        43: i2d                 // to double，准备传给 add() 方法</span><br><span class="line">        44: invokestatic  #8                  // Method add:(D)D</span><br><span class="line">        47: pop2                // pop 7</span><br><span class="line">        48: dload_1</span><br><span class="line">        49: dconst_1</span><br><span class="line">        50: dsub                // d--</span><br><span class="line">        51: dstore_1            // 更新表</span><br><span class="line">        52: iload_3</span><br><span class="line">        53: ifeq          62    // succeeds if and only if value = 0,这边是 false,条件成立，不跳转，执行 b = a * b;</span><br><span class="line">        56: iload         4</span><br><span class="line">        58: i2d</span><br><span class="line">        59: dload_1</span><br><span class="line">        60: dmul</span><br><span class="line">        61: dstore_1</span><br><span class="line">        62: return</span><br><span class="line">      LineNumberTable:</span><br><span class="line">        ...</span><br><span class="line">  public static double add(double);</span><br><span class="line">    descriptor: (D)D</span><br><span class="line">    flags: ACC_PUBLIC, ACC_STATIC</span><br><span class="line">    Code:</span><br><span class="line">      stack=6, locals=2, args_size=1</span><br><span class="line">         0: dload_0         //load 0 位置，就是传过来的参数到栈顶,这里就是 6</span><br><span class="line">         1: dup2            // 复制栈顶元素 - 感觉这边可以证明 Java 方法参数是值传递（拷贝一个参数副本），不是引用传递 ???</span><br><span class="line">         2: dconst_1</span><br><span class="line">         3: dadd</span><br><span class="line">         4: dstore_0        // 0 的位置变成 7</span><br><span class="line">         5: dreturn</span><br><span class="line">      LineNumberTable:</span><br><span class="line">        line 27: 0</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br><span class="line">SourceFile: &quot;ByteCodeAnalysis.java&quot;</span><br></pre></td></tr></table></figure>

<h2 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h2><ul>
<li><a href="https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-6.html">https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-6.html</a></li>
<li><a href="https://en.wikipedia.org/wiki/Java_bytecode">https://en.wikipedia.org/wiki/Java_bytecode</a></li>
<li><a href="https://stackoverflow.com/questions/20441256/contents-of-stack-frame-in-java">https://stackoverflow.com/questions/20441256/contents-of-stack-frame-in-java</a></li>
<li><a href="https://www.artima.com/insidejvm/ed2/jvm8.html">https://www.artima.com/insidejvm/ed2/jvm8.html</a></li>
<li>《深入理解 Java 虚拟机》</li>
</ul>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>jvm</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>Java 异常处理</title>
    <url>/2021/01/15/Java-Exception/</url>
    <content><![CDATA[<h2 id="Throwable、Error、Exception"><a href="#Throwable、Error、Exception" class="headerlink" title="Throwable、Error、Exception"></a>Throwable、Error、Exception</h2><p>Throwable 接口可以理解成是 Java 异常世界中的 Object。因为 Java 中的所有异常一定都是 Throwable 的子类。</p>
<span id="more"></span>

<p><img src="/2021/01/15/Java-Exception/image-20210925115913274.png" alt="image-20210925115913274"></p>
<p>Error 描述的是那些系统本身出现的错误，比如 Java 虚拟机内部问题等等。这些错误一般和 Java 应用程序没什么关系，Java 程序则不应该在任何 Java 方法中使用 throws 关键字说明可能抛出 Error 及其子类相关的异常。</p>
<p>Exception 描述的是那些 Java 应用应该合理进行捕获的的异常。所有 Exception 的子类但不是 RuntimeException 的子类都被定义为<strong>受检异常</strong>。</p>
<p>对于<strong>运行时异常</strong>，比如数组范围越界，访问空指针，类型转换异常，这些异常都是由于 Java 代码的问题导致，最好在程序编写的时候就避免。</p>
<blockquote>
<p>总之，一个方法必须声明所有可能抛出的受查异常， 而非受查异常要么不可控制（ Error),<br>要么就应该避免发生（ RuntimeException)。如果方法没有声明所有可能发生的受查异常， 编译器就会发出一个错误消息。</p>
</blockquote>
<h2 id="try-catch-finally"><a href="#try-catch-finally" class="headerlink" title="try/catch/finally"></a>try/catch/finally</h2><p>如果方法中发生了某些异常但是没有进行捕获，程序则会在异常发生的地方终止然后输出异常信息。异常信息一般包含调用栈等信息。</p>
<p>程序可以通过 try/catch 代码块捕获包含在 try{} 中<strong>可能发生的异常</strong>，并在 catch 到<strong>对应的异常类型</strong>时执行 catch 中的代码。</p>
<p>try 中发生异常<strong>且</strong> catch 声明的异常类型<strong>可以匹配</strong>，try 中剩余的语句不执行，转而执行 catch 中的语句。没异常或不匹配就忽略 catch 中的内容。注意和 switch 语句不同，switch 需要在每个 case 后跟一个 break 避免执行后续的 case。</p>
<p>catch 块中可以继续抛出异常交给上层来处理。如果直接抛出其他类型的异常，这样会丢失原始的异常信息。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">  <span class="comment">// 刚捕获到的异常对象 e 被丢弃</span></span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;待会再试&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>更推荐的方法是**在抛出其他异常前保存原始异常或者将原始异常作为新异常的 cause(内因)**。且在抛出异常时，要提供合适的消息（对应下面代码的 “待会再试”）。</p>
<p><img src="/2021/01/15/Java-Exception/image-20210925115943257.png" alt="image-20210925115943257"></p>
<p>如果在 catch 块中<strong>直接重新抛出刚捕获的异常</strong>，那么 printStackTrace() 方法显示的是原本的调用栈信息，而不是新的抛出点信息。想更新抛出点信息的话，可以调用 fillInStackTrace() 方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">  <span class="keyword">throw</span> e.fillInStackTrace();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>有时候我们需要 <strong>finally 子句</strong>来回收特定的资源，不管异常有没有发生，finally 子句的内容在最终都会执行。</p>
<h2 id="try-with-resource"><a href="#try-with-resource" class="headerlink" title="try-with-resource"></a>try-with-resource</h2><p>但是要小心 finally 中的语句也可能发生异常，<strong>且在 finally 中抛出的异常会覆盖原始可能抛出的异常。</strong>这样就丢失了原始的异常。</p>
<p>我们可以在 finally 中添加 try/catch 来捕获并处理异常。或者将 try 中发生的异常贴附给 finally 中的异常。</p>
<p><img src="/2021/01/15/Java-Exception/image-20210925120011394.png" alt="image-20210925120011394"></p>
<p><img src="/2021/01/15/Java-Exception/image-20210925120030689.png" alt="image-20210925120030689"></p>
<p>结果：</p>
<p><img src="/2021/01/15/Java-Exception/image-20210925120052598.png" alt="image-20210925120052598"></p>
<p>其实对于在 finally 语句块中发生异常情况的上述处理（通过 addSuppressed() 依附异常），正是 try-with-resource 语法做的事情。对于实现了 AutoCloseable 接口的<strong>资源，</strong>可以使用 try-with-resource 语法来自动回收资源。</p>
<p><img src="/2021/01/15/Java-Exception/image-20210925120113611.png" alt="image-20210925120113611"></p>
<p>输出结果和上面的代码类似，说明 try-with-resource 内部的实现方式也是类似的：对 close() 方法可能抛出的异常，将其 Suppressed 到原始异常中抛出。</p>
<p><img src="/2021/01/15/Java-Exception/image-20210925120128233.png" alt="image-20210925120128233"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最后，结合《Effective Java》等相关资料，总结处理 Java 异常时的一些要点。</p>
<ul>
<li>异常只用在异常发生的情况，不该用来进行程序流程控制。</li>
<li>注意区分 RuntimeException 和 受检异常，以及使用条件。</li>
<li>抛出异常时结合带 String 参数的构造方法提供更多信息。</li>
<li>捕获异常后并重新抛出异常时，注意保留原始的异常对象。</li>
<li>优先用 try-with-resource，而不是 try / finally。</li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>Lambda 和 StreamAPI</title>
    <url>/2021/06/13/Lambda-StreamAPI/</url>
    <content><![CDATA[<h1 id="Lambda-表达式"><a href="#Lambda-表达式" class="headerlink" title="Lambda 表达式"></a>Lambda 表达式</h1><p><strong>Java 8 开始，可以用 Lambda 表示只有一个抽象方法的接口</strong>（<strong>函数式接口，该类型的接口一般标有 @FuntionalInterface 注解</strong>）。此前我们只能通过繁琐的匿名实现类表示。</p>
<p>比如，Runnable 接口就是一个<strong>函数式接口</strong>。<span id="more"></span></p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925121336491.png" alt="image-20210925121336491"></p>
<p>所以我们可以用 Lambda 表达式创建一个该接口的匿名实现。</p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925121400282.png" alt="image-20210925121400282"></p>
<p><strong>() 表示</strong>函数式<strong>接口中定义的抽象方法的参数</strong>，因为 Runnable 接口中的 run() 方法没有参数，所以用空的 () 表示。</p>
<p><strong>箭头后的内容表示</strong>该方法的实现，这里就是指 void run() 的具体实现，且因为其返回类型是 void，所以无需返回特定类型，这里只进行了一个输出语句。</p>
<p>再比如下图的这个函数式接口 Predicate<T>，抽象方法的返回类型是 boolean，方法参数为泛型 T。</T></p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925121415893.png" alt="image-20210925121415893"></p>
<p>使用 Lambda 表示式实现的几种方式：</p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925121427947.png" alt="image-20210925121427947"></p>
<p>第 6 行中，第一个 s 表示入参，其中 s 为 String 类型。</p>
<p>箭头后的代码调用了 String 的 equals 方法，该方法返回 boolean，要和<strong>函数接口中定义的方法一致。</strong></p>
<p>第 8 行到 第 16 行，说明 Lambda 的方法实现可以像普通方法的方法那样<strong>有方法体和显式的 return 语句（需要用 { } 包括</strong>）。</p>
<p>还有一个点就是 Lambda 很多参数类型都不需要声明，编译器会为我们自动进行类型推导。如果 Lambda 表达式的类型有歧义，编译器会告诉你需要指定对应的类型。</p>
<p>以下部分的代码几乎都由 List<Dish> 类型的 menu 变量作为构建流的基本元素。代码大致如下：</Dish></p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925121440379.png" alt="image-20210925121440379"></p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925121450024.png" alt="image-20210925121450024"></p>
<h1 id="Stream"><a href="#Stream" class="headerlink" title="Stream"></a>Stream</h1><p>了解 Lambda 表示式之后，来看看新的 Stream API。</p>
<h2 id="什么是流"><a href="#什么是流" class="headerlink" title="什么是流"></a>什么是流</h2><p>Stream<T> 接口的注释是这样的:</T></p>
<p><em>“A sequence of elements supporting sequential and parallel aggregate operations.”</em></p>
<p>支持有顺序，可以并行地聚合操作的一串<strong>元素序列</strong>。</p>
<p>流的数据源头可以从集合，数组获得。有了基础流之后，可以对流进行数据处理，Stream API 定义了一系列简易的方法供我们使用。</p>
<p>虽然流和集合都是包含特定的元素序列，但是他们之间是有一些明显区别的。这两者可以类比成我们生活中的 DVD 和在线看电影。DVD 相当于集合，已经有了电影所有的帧。流则需要我们从网络中进行数据加载再处理。</p>
<h2 id="操作流"><a href="#操作流" class="headerlink" title="操作流"></a>操作流</h2><p>图中是两种筛选 Dish 的实现。</p>
<p>第一种（第 16 行开始）使用常规的集合进行筛选。第二种（第 30 行开始）使用流进行筛选。</p>
<p>明显第二种方式简洁且可读性也好。</p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925121553922.png" alt="image-20210925121553922"></p>
<p>来看看使用流操作（第二种方式，第 30 行开始）中每个操作的含义：</p>
<p>第 31 行，<strong>stream()</strong> 方法从 menu 这一集合中（源头）产生流（Stream<Dish> 类型）供接下来操作。</Dish></p>
<p>第 32 行，<strong>filter()</strong> 方法筛选出卡路里高于 500 的菜肴，将筛选后的元素变成一个新的流（Stream<Dish> 类型）供接下来操作。</Dish></p>
<p>第 33 行，<strong>sorted()</strong> 方法按照卡路里高低排序，继续返回一个 Stream<Dish> 类型的流。</Dish></p>
<p>第 34 行，<strong>limit()</strong> 方法只从流中截断出只包含 2 个元素的流。</p>
<p>第 35 行，<strong>map()</strong> 方法将 Stream&lt;**Dish**&gt; 映射成 Stream&lt;**String**&gt;，返回一个菜肴名称的流。</p>
<p>第 36 行，<strong>collect()</strong> 方法将 Stream<String> 中的元素保存到 List 中。到此结束，所以 highCaloriesDish 变量的类型是 List<String> 类型的。</String></String></p>
<h2 id="Stream-API"><a href="#Stream-API" class="headerlink" title="Stream API"></a>Stream API</h2><h3 id="筛选"><a href="#筛选" class="headerlink" title="筛选"></a>筛选</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Stream&lt;T&gt; filter(Predicate&lt;? super T&gt; predicate);</span><br><span class="line">Stream&lt;T&gt; distinct();</span><br><span class="line">Stream&lt;T&gt; skip(long n);</span><br><span class="line">Stream&lt;T&gt; limit(long maxSize);</span><br></pre></td></tr></table></figure>

<h3 id="映射"><a href="#映射" class="headerlink" title="映射"></a>映射</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super T, ? extends R&gt; mapper);</span><br><span class="line">&lt;R&gt; Stream&lt;R&gt; flatMap(Function&lt;? super T, ? extends Stream&lt;? extends R&gt;&gt; mapper);</span><br></pre></td></tr></table></figure>

<p>map() 方法的功能是根据传入的实现将类型映射为其他类型。</p>
<p>map() 方法的参数 Function&lt;…&gt; 的定义是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@FunctionalInterface</span><br><span class="line">public interface Function&lt;T, R&gt; &#123;</span><br><span class="line">    R apply(T t);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>apply() 方法传入一个类型 T，然后根据方法实现返回类型 R。拿下图中的 map() 方法举例，传入参数类型为 Integer 的 i（对应 T），返回 i * i（也是 Integer 类型，对应 R）。</p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925121717843.png" alt="image-20210925121717843"></p>
<p><strong>扁平流 flatmap</strong> 看下图的箭头部分，String 的 split() 方法返回的是 String[]，经过 map 映射后流中包含了这两个元素  {[H，e，l，l，o]，[W，o，r，l，d]｝。之后如果我们直接调用 distinct()，意味着作用的对象是 [H，e，l，l，o] 和 [W，o，r，l，d] 这两个流中的 String[] 数组，它们肯定是不一样的，故达不到目的。</p>
<p>Arrays.stream() 方法可以将传入的数组元素产生一个流。其方法签名是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public static &lt;T&gt; Stream&lt;T&gt; stream(T[] array) &#123;</span><br><span class="line">        return stream(array, 0, array.length);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Arrays::stream 返回 Stream<String> 列表后，map() 方法又将整个内容分别映射到不同的流中。故还是有问题。</String></p>
<p>再看 flatMap ，flatMap() 方法将流中的每个值都映射到同一个流中。</p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925121734068.png" alt="image-20210925121734068"></p>
<h3 id="查找和匹配"><a href="#查找和匹配" class="headerlink" title="查找和匹配"></a>查找和匹配</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">boolean anyMatch(Predicate&lt;? super T&gt; predicate);</span><br><span class="line">boolean allMatch(Predicate&lt;? super T&gt; predicate);</span><br><span class="line">boolean noneMatch(Predicate&lt;? super T&gt; predicate);</span><br><span class="line">Optional&lt;T&gt; findFirst();</span><br><span class="line">Optional&lt;T&gt; findAny();</span><br></pre></td></tr></table></figure>

<h3 id="归约"><a href="#归约" class="headerlink" title="归约"></a>归约</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">T reduce(T identity, BinaryOperator&lt;T&gt; accumulator);</span><br><span class="line">Optional&lt;T&gt; reduce(BinaryOperator&lt;T&gt; accumulator);</span><br></pre></td></tr></table></figure>

<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925122004919.png" alt="image-20210925122004919"></p>
<p>第 4 行：reduce 接受一个初始值 0，然后逐个比较选择大的那个数。</p>
<p>第 5 行：一样的功能使用方法引用表示。</p>
<p>第 6 行：reduce() 方法只接受一个参数，不接受初始值。为了应对 stream() 调用后<strong>流中没有任何元素的可能性，</strong>所以这个方法的返回值是 Optional 类型。</p>
<h2 id="数值流"><a href="#数值流" class="headerlink" title="数值流"></a>数值流</h2><p>前面我们使用到的流都是针对<strong>对象类型</strong>的，在进行计算的时候其实包括了<strong>隐含的拆装箱</strong>操作。为此 Java 8 引入了三个针对特定<strong>原始类型</strong>的流来进一步简化操作，分别是 IntStream，LongStream 和 DoubleStream。可以通过 Stream<T> 中的 mapToInt/Long/Dubbo 方法将流转换为特性的数值流。</T></p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925122154169.png" alt="image-20210925122154169"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">将特化流装箱为对应的对象类型流  </span><br><span class="line">Stream&lt;Integer//Long//.&gt; boxed(); </span><br><span class="line">// 生成范围数值流</span><br><span class="line">public static IntStream range(int startInclusive, int endExclusive);</span><br><span class="line">// 包含起始值的范围数值流</span><br><span class="line">public static IntStream rangeClosed(int startInclusive, int endInclusive);</span><br></pre></td></tr></table></figure>

<h2 id="如何构建一个流"><a href="#如何构建一个流" class="headerlink" title="如何构建一个流"></a>如何构建一个流</h2><p>此前我们获得流的方式都是通过集合调用 stream() 方法生成流，或者使用数值流的 range/rangeClosed 生成特定范围的数值流。这部分就来介绍生成流的其他方式。</p>
<ul>
<li>由显式值构建流：<strong>Stream.of()</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public static&lt;T&gt; Stream&lt;T&gt; of(T... values) &#123; ... &#125;</span><br><span class="line">Stream&lt;String&gt; hello = Stream.of(&quot;hello&quot;, &quot;test&quot;, &quot;hi&quot;);</span><br></pre></td></tr></table></figure>

<ul>
<li>由数组构建流，接收各种参数并由之生成对应的流：<strong>Arrays.stream()</strong></li>
</ul>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925122218320.png" alt="image-20210925122218320"></p>
<ul>
<li>由文件获得流。Java 中用来处理文件的 API 更新后可以用来支持 Stream API。</li>
</ul>
<p>借此顺便再看一下 flatMap ，Array.stream 的用法。注意看返回不同类型的 Stream。</p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925122232527.png" alt="image-20210925122232527"></p>
<p>注意到第 11 行，我们使用流之前需要重新从文件中生成新的流。因为流只能被消费一次，你会发现如果重复消费会发生类似的异常。</p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925122245419.png" alt="image-20210925122245419"></p>
<ul>
<li>由函数生成流：<strong>iterate()，generate()</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public static&lt;T&gt; Stream&lt;T&gt; iterate(final T seed, final UnaryOperator&lt;T&gt; f) &#123;&#125;</span><br><span class="line">public static&lt;T&gt; Stream&lt;T&gt; generate(Supplier&lt;T&gt; s) &#123;&#125;</span><br></pre></td></tr></table></figure>

<p>iterate 接受一个 T 类型的初始值，以及一个<strong>每次都会作用在新值上的函数</strong>（合时宜的话可以说是 Lambda）。有点像 reduce 那样。不同的是<strong>iterate 会不断产生产生新元素</strong>到流中。</p>
<p>generate 接受<strong>一个不断产生新的值的 Lambda</strong>。看一下其参数 Supplier<T> 接口中定义的方法就知道 generate 做的事情是什么了。</T></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@FunctionalInterface</span><br><span class="line">public interface Supplier&lt;T&gt; &#123;</span><br><span class="line">    T get(); // 唯一要做的就是提供一个生成新元素的实现</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>需要注意的是对那些可以生成无限流一般需要通过 limit() 方法来截断。</p>
<h2 id="从流中收集数据"><a href="#从流中收集数据" class="headerlink" title="从流中收集数据"></a>从流中收集数据</h2><p>前面的代码中我们经常使用<strong>Stream</strong>的 <strong>collect()</strong> 方法配合入参 toList(）将流中的数据放到一个 List 中。其实这个 toList() 方法是定义在 Collectors 工厂类里面，其中还预定义了很多可以直接使用的方法。</p>
<p>当我们对流使用 collect() 方法的时候，就是为了使用传入该方法的**”参数”**进行对应的收集操作。</p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925122300501.png" alt="image-20210925122300501"></p>
<p>我们先来看看 API 为我们预先提供了哪些可以直接用的 “参数”（Collectors 类中提供的现有实现）</p>
<ul>
<li>groupingBy</li>
<li>maxBy</li>
<li>summarizingInt</li>
<li>joining 等等。。。</li>
</ul>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925122311199.png" alt="image-20210925122311199"></p>
<h3 id="归约-1"><a href="#归约-1" class="headerlink" title="归约"></a>归约</h3><p><strong>reducing</strong></p>
<p>前面提到的都是特定的收集方法，我们也可以通过 reducing() 方法来自己适应更广泛的收集情况。</p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925122319901.png" alt="image-20210925122319901"></p>
<p>先来看有三个入参的版本，</p>
<ul>
<li>第一个参数相当于给一个初始值。应对流中没有元素的情况。</li>
<li>第二个参数有点像 map() 方法，提供一个映射操作。</li>
<li>第三个参数就是针对映射后的参数类型进行操作。</li>
</ul>
<p>或者也可以直接只提供一个针对流元素进行操作的 Lambda 操作（只有一个参数的版本），这样的话如果流中没有元素应该返回什么呢？所以可以看到返回类型出现了 Optional 类型来应对这种情况。</p>
<h3 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h3><h4 id="Collectors-groupingBy"><a href="#Collectors-groupingBy" class="headerlink" title="Collectors.groupingBy()"></a>Collectors.groupingBy()</h4><p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925122336096.png" alt="image-20210925122336096"></p>
<p>groupingBy() 先根据传入的 Function 类型参数，将流中的元素映射为特定的类型作为分类的依据（Map 中的 key 类型）。</p>
<p>groupingBy() 方法也提供了两个参数的版本。</p>
<p>Map 的 value 类型是 List 则是因为单个参数的 groupingBy() 方法默认传入的第二个参数是 toList() 方法。</p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925122349828.png" alt="image-20210925122349828"></p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925122357176.png" alt="image-20210925122357176"></p>
<p>可以看到第二个参数的类型是 Collector 接口，说明还可以在第一层分组的基础上进行其他的操作。</p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925122405341.png" alt="image-20210925122405341"></p>
<p>主要看第 1 行和 第 5 行的输出结果，可以说明第二个参数作用在了第一个分组的 value 中，因为传入了 Dish::getType ,所有又在第一次分组中根据类型进行了分组。</p>
<p>第 10 行的方法的 counting() 方法，计算了每个类别中包含的元素个数。</p>
<h4 id="分区：Collectors-partitioningBy"><a href="#分区：Collectors-partitioningBy" class="headerlink" title="分区：Collectors.partitioningBy()"></a>分区：Collectors.partitioningBy()</h4><p>分区是分组的一种特殊情况。只是分区返回的分类都是通过 true 和 false 来区分。true 和 false 的定义就取决于你传入的那段 Predicate&lt;&gt; 实现。</p>
<p><img src="/2021/06/13/Lambda-StreamAPI/image-20210925122416154.png" alt="image-20210925122416154"></p>
<p>partitioningBy() 和 groupingBy() 一样也有重载包含两个参数的版本，用法也差不多。</p>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java 8</tag>
        <tag>stream api</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 零拷贝</title>
    <url>/2021/05/16/Linux-Zero-Copy/</url>
    <content><![CDATA[<blockquote>
<p><strong>原文地址</strong>：<a href="https://www.linuxjournal.com/article/6345?page=0,0">https://www.linuxjournal.com/article/6345?page=0,0</a></p>
<p>wiki词条：<a href="https://en.wikipedia.org/wiki/Zero-copy">https://en.wikipedia.org/wiki/Zero-copy</a></p>
</blockquote>
<span id="more"></span>

<h2 id="零拷贝"><a href="#零拷贝" class="headerlink" title="零拷贝"></a>零拷贝</h2><p><strong>前置知识</strong></p>
<ul>
<li><strong>系统调用：操作系统为了统一接口提供的函数以供操作资源，比如 read，write方法</strong></li>
<li><strong>用户态：用户程序运行的用户空间，无法直接访问底层硬件资源</strong></li>
<li><strong>内核态：用户程序进行系统调用，委托内核态间接操作硬件资源</strong></li>
<li><strong>DMA：直接存储技术，不通过 CPU 进行</strong></li>
</ul>
<p>假设你打算使用某软件将本机磁盘上的文件传输给某个客户端。</p>
<p>因为计算机系统的保护机制，用户应用程序是在用户空间上运行，在涉及到硬件资源操作时，通过系统调用（比如调用 read() 方法读取文件内容），借助内核间接访问资源。</p>
<p><img src="/2021/05/16/Linux-Zero-Copy/image-20210925123537658.png" alt="image-20210925123537658"></p>
<p>可以看到，整个过程有四次文件复制操作。过程中，因为涉及到内核态与用户态之间的上下文转换和频繁且缓慢的 I/O 操作，其读写性能是比较糟糕的。</p>
<p><img src="/2021/05/16/Linux-Zero-Copy/image-20210925123558017.png" alt="image-20210925123558017"></p>
<p>零拷贝技术可以简单的理解成将参与到整个过程中的 <strong>用户空间态</strong> 省去，省去上下文切换的开销，减少 I/O 操作，很好的提高了效率。</p>
<p><img src="/2021/05/16/Linux-Zero-Copy/image-20210925123623123.png" alt="image-20210925123623123"></p>
<p>图片中，从内核缓存到 socket buffer 的 copy 操作被优化了。这需要硬件技术的帮忙。</p>
<p> 通过支持聚集操作的网络接口，待传输的数据不必占用主存的连续空间，网卡的 DMA 引擎也可以将分布在不同位置的数据集中到一个数据传输中。</p>
<blockquote>
<p>有疑惑的话可以戳<a href="https://stackoverflow.com/questions/9770125/zero-copy-with-and-without-scatter-gather-operations">这里</a></p>
<p>大概的解释就是，如果网卡不支持聚集操作，那么就需要将内核缓冲区中物理分布分散的数据通过 CPU 拷贝，连续的存放在 socket buffer 中以供 DMA 引擎拷贝。</p>
</blockquote>
<p>在 Linux 的 2.4 内核版本，socket buffer 的描述符就被修改升级以支持适应刚才说的特性 - 这也是 Linux 零拷贝的基础。</p>
<p>不同之前的整个复制操作，内核将待传输数据的分布位置，待传输数据的长度等通过描述符复制给 socket buffer。 DMA 引擎（支持聚集操作）直接将数据从 kernel buffer 复制到协议引擎。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>实际上整个复制过程仍然包括了使用 DMA 引擎进行数据拷贝，所有不能说这是真正意义上的零拷贝。</p>
<p>但我们站在 CPU 角度，在没有引入零拷贝技术前，都会涉及到 CPU 拷贝的步骤。引入之后，CPU 参与的步骤变少，且没有了用户态和内核态的上下文切换，节省了 CPU 开销。</p>
<p>所以，可以在 CPU 角度看这个 “零”，而不是零次拷贝操作。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>翻译</tag>
        <tag>linux</tag>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL 学习记录</title>
    <url>/2021/07/10/Mysql-geektime-study-record/</url>
    <content><![CDATA[<h1 id="基础架构"><a href="#基础架构" class="headerlink" title="基础架构"></a>基础架构</h1><span id="more"></span>

<p><img src="/2021/07/10/Mysql-geektime-study-record/image-20210925163128286.png" alt="image-20210925163128286"></p>
<ul>
<li>show processlist;</li>
<li>一个连接长时间没有动静，会自动断开。由参数 wait_timeout 控制，defaule 8 hours</li>
<li>MySQL 在一个连接中，执行命令 <strong>使用到的临时内存</strong>会被一直保留，直到这个连接断开才会释放</li>
</ul>
<p>每课一问：</p>
<p>如果表 T 中没有字段 k，而你执行了这个语句 select * from T where k=1, 那肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是在我们上面提到的哪个阶段报出来的呢?</p>
<p>答：执行器。</p>
<p>正确答案：分析器。在分析词法和语法的时候，会判断表字段。</p>
<h1 id="日志系统"><a href="#日志系统" class="headerlink" title="日志系统"></a>日志系统</h1><h2 id="redo-log"><a href="#redo-log" class="headerlink" title="redo log"></a>redo log</h2><p>WAL，Write-Ahead-Logging，先写日志后写磁盘。redo log 是 InnoDB 特有的日志模块。</p>
<p>当有一条记录要更新的时候，执行器找 InnoDB 要这一行的记录，如果不在内存中的话，InnoDB 引擎从磁盘读取数据页到内存，返回给执行器。执行器把记录更新后，调用引擎提供的接口，InnoDB 会把数据更新到内存中，然后记录到 redo log 里。</p>
<p>InnoDB 会在系统比较空闲的时候把 redo log 里的内容更新到磁盘中。</p>
<p>redo log 是有大小限制的，当 redo log 满的时候，就需要把数据记录到磁盘，redo log 文件腾出空间后，再进行更新操作。</p>
<p>redo log 记录这个数据 “做了什么改动”，redo log 有 crash-safe 能力，即使数据库异常重启了，之前的提交记录都在，可以恢复。binlog 只能用来归档，从备库一个个重放。</p>
<h2 id="binlog"><a href="#binlog" class="headerlink" title="binlog"></a>binlog</h2><ul>
<li>binlog 是属于 Server 层面的，任何引擎都能用。</li>
<li>binlog 记录的内容是请求的原始语句逻辑，类似 Redis 的 AOF 记录的是执行的命令。</li>
<li>binlog 没有大小限制，理论上可以一直追加。binlog 文件写满后切换到下一个文件，不会覆盖以前的记录。</li>
</ul>
<h2 id="两阶段提交"><a href="#两阶段提交" class="headerlink" title="两阶段提交"></a>两阶段提交</h2><p><img src="/2021/07/10/Mysql-geektime-study-record/image-20210925163145506.png" alt="image-20210925163145506"></p>
<p>redolog prepare 和 commit 状态。</p>
<p><img src="/2021/07/10/Mysql-geektime-study-record/image-20210925163214981.png" alt="image-20210925163214981"></p>
<h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># redolog 和 binlog，每次事务完成持久化到磁盘</span><br><span class="line"># 在配置文件中设置，记得重启服务</span><br><span class="line">innodb_flush_log_at_trx_commit<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line">sync_binlog<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;log_%&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h1 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h1><p>索引是在引擎层面实现的，意味着不同的存储引擎采用的索引实现可能是不一样的。</p>
<p>从底层的数据结构来分析索引的各项特性</p>
<ul>
<li>哈希表</li>
<li>有序数组</li>
<li>搜索树</li>
</ul>
<h2 id="InnoDB-的索引模型"><a href="#InnoDB-的索引模型" class="headerlink" title="InnoDB 的索引模型"></a>InnoDB 的索引模型</h2><p>B+ 树，在 InnoDB 中，采用的是索引组织表，也就是说，每张表是通过 B+树来存储的。</p>
<p>每个索引对应着一棵 B+ 树。</p>
<p>因为 B+ 树需要维护索引的有序性，在数据改变的时候，就要进行调整。</p>
<p>这就需要先知道数据页的概念，InnoDB 操作数据时，需要将磁盘中的数据读取到内存中进行操作，读取和刷入磁盘都是以数据页为单位的。</p>
<p>当插入的新数据所在的位置的数据页刚好满了，就需要进行调整。-页分裂等等。</p>
<h2 id="最佳实践-1"><a href="#最佳实践-1" class="headerlink" title="最佳实践"></a>最佳实践</h2><p>定义一个业务无关的自增 id，也就意味着：</p>
<ul>
<li>每次新增数据，在主键索引树上，总是在最后的位置添加数据，很有效的减少了可能的 IO 成本。叶子节点分裂的可能性降低。</li>
<li>如果用整型做自增主键，这样<strong>对普通索引</strong>来说，叶子节点存储占用的空间也小很多。</li>
<li>还是要看具体的业务情况</li>
</ul>
<h1 id="事务的隔离性"><a href="#事务的隔离性" class="headerlink" title="事务的隔离性"></a>事务的隔离性</h1><h2 id="事务隔离性和隔离级别"><a href="#事务隔离性和隔离级别" class="headerlink" title="事务隔离性和隔离级别"></a>事务隔离性和隔离级别</h2><p>说到事务，就会提到事务的 ACID 特性，这一部分主要分析事务的 I（Isolaction），隔离性。</p>
<p>而隔离级别就是数据库对于事务隔离性的设计，事务源源不断的被开启，提交。不同事务之间可能操作同一部分的数据，就可能出现脏读（读未提交，可能读到脏数据）、幻读（读到的行数不一样）、不可重复读（同一个事务内前后读到的数据不一样）。</p>
<p>数据库为此，定义了四个隔离级别，隔离级别越高，隔离性就越好，但是性能可能就比较差。</p>
<ul>
<li>读未提交。在事务期间内修改了数据且未 commit，修改后的数据就能被其他事务读取。</li>
<li>读提交。一个事务做的修改，只有提交后才能被其他事务读取到。</li>
<li>可重复读。一个事务期间读到的数据和事务启动时看到的数据一致。</li>
<li>串行化。读加读锁，写加写锁。直接阻塞。</li>
</ul>
<h2 id="事务隔离的实现方式是什么-MVCC、数据的视图。"><a href="#事务隔离的实现方式是什么-MVCC、数据的视图。" class="headerlink" title="事务隔离的实现方式是什么 - MVCC、数据的视图。"></a>事务隔离的实现方式是什么 - MVCC、数据的视图。</h2><p>在 MySQL 中，每一次更新记录的时候都会同时记录一条回滚操作。也就是说，可以通过最新的记录的值，一次次的回滚取到之前不同状态的值。(以下针对“可重复读”这一隔离级别来分析。)</p>
<p>那总不能所以的回滚记录统统记录下来吧，一些版本的数据可能已经没有用了，因为在“可重复读”的隔离级别下，系统中可能没有事务要用到这个版本的数据了，就会被删除。</p>
<p>不同时刻启动的事务都有自己对于数据的视图。在这个事务内，采用这个一致性视图来读取值。（所以，事务内看到的数据和事务启动时的一致。）</p>
<p>MVCC，多版本并发控制。在 MySQL 中，同一条记录可以有多个不同的版本。</p>
<h2 id="最佳实践-2"><a href="#最佳实践-2" class="headerlink" title="最佳实践"></a>最佳实践</h2><ul>
<li>避免长事务</li>
<li>set autocommit=0，自动提交事务功能关闭。要注意这个语句只对当前 session 有效。当你 begin 或者 start transaction 后，如果不在命令行显示使用 commit 或者 rollback，事务会一直持续到你显示结束。</li>
<li>set autocommit=1，并记得及时显示提交。</li>
</ul>
<p><img src="/2021/07/10/Mysql-geektime-study-record/image-20210925163235867.png" alt="image-20210925163235867"></p>
<h2 id="相关命令和语句"><a href="#相关命令和语句" class="headerlink" title="相关命令和语句"></a>相关命令和语句</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">#查看数据库隔离级别</span><br><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;transaction_isolation&#x27;</span>;</span><br><span class="line"></span><br><span class="line">#查询超过 <span class="number">60</span>s 的长事务</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> information_schema.innodb_trx <span class="keyword">where</span> TIME_TO_SEC(timediff(now(),trx_started))<span class="operator">&gt;</span><span class="number">60</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">show</span> session variables <span class="keyword">like</span> <span class="string">&#x27;autocommit&#x27;</span>;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">global</span> variables <span class="keyword">like</span> <span class="string">&#x27;autocommit&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h1 id="索引-1"><a href="#索引-1" class="headerlink" title="索引"></a>索引</h1><ul>
<li>底层数据结构：B+ 树。叶子节点存的是 页。</li>
<li>回表</li>
<li>索引维护。页分裂</li>
<li>索引下推</li>
<li>索引覆盖</li>
<li>最左前缀</li>
<li>联合索引</li>
</ul>
<h1 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h1><h2 id="全局锁"><a href="#全局锁" class="headerlink" title="全局锁"></a>全局锁</h2><p>对整个数据库加锁。</p>
<p>典型的使用场景是对整库进行备份。</p>
<ul>
<li>Flush tables with read lock，让库处于只可读状态，其他任何写操作都被拒绝。</li>
<li>还有一种方式是用 set global readonly=true 的方式，不建议使用，原因在于<ul>
<li>readonly 的值可能用在其他逻辑</li>
<li>客户端设置库为 readonly 后，如果客户端端发生异常断开，数据库会一直保持 readonly 状态。Flush tables with read lock 的话，MySQL 会自动释放这个全局锁。</li>
</ul>
</li>
</ul>
<p>官方自带的逻辑备份工具是 mysqldump。使用 mysqldump 带 -single-transaction 参数时，备份是会启动一个<strong>事务</strong>，保证备份过程中数据的一致性。因为有 MVCC 的支持，期间数据库可以正常响应写请求。</p>
<p><strong>使用该种方式需要引擎支持数据库，MyISAM 就不支持，所以需要设置全局读锁。</strong></p>
<h2 id="表级锁"><a href="#表级锁" class="headerlink" title="表级锁"></a>表级锁</h2><p><img src="/2021/07/10/Mysql-geektime-study-record/image-20210925163248267.png" alt="image-20210925163248267"></p>
<ul>
<li>一般意义的锁，DML（Data Manipulation Language）操作。（DDL Data Definition Language）</li>
<li>MDL。在 MySQL 5.5 引入，系统会自动默认添加。主要目的是<strong>针对改变表结构操作（DDL）</strong>的锁。如果对表结构进行改变的话，<strong>会加 MDL 写锁</strong>。正常的 CRUD 的话，加 MDL 读锁。</li>
</ul>
<p><strong>给一个表加字段、修改字段、加索引，需要扫描全表的数据。</strong></p>
<p><strong>Session A</strong>：开启事务，查询数据，且未提交（默认添加 MDL 读锁）</p>
<p><img src="/2021/07/10/Mysql-geektime-study-record/image-20210925163309460.png" alt="image-20210925163309460"></p>
<p><strong>Session B</strong>：对表进行 DML 操作，需要 MDL 写锁，但 A 中的 MDL 读锁没有释放，被阻塞。</p>
<p><img src="/2021/07/10/Mysql-geektime-study-record/image-20210925163328495.png" alt="image-20210925163328495"></p>
<p><strong>Session C</strong>：对表进行查询，发现被阻塞</p>
<p><img src="/2021/07/10/Mysql-geektime-study-record/image(11)-2558947.png" alt="image(11)"></p>
<p>之后，在 <strong>Session A</strong>进行事务提交操作</p>
<p><img src="/2021/07/10/Mysql-geektime-study-record/image(12).png" alt="image(12)"></p>
<p><strong>Session B</strong>：</p>
<p><img src="/2021/07/10/Mysql-geektime-study-record/image(13).png" alt="image(13)"></p>
<p><strong>Session C</strong>：</p>
<p><img src="/2021/07/10/Mysql-geektime-study-record/image(14).png" alt="image(14)"></p>
<p>事务中的 MDL 锁，在语句开始是申请，在事务结束时才释放。但是这也不太好解释为什么 C 中的查询语句会被阻塞，因为 B 申请 MDL 写锁失败了。（从设计初衷来说为了防止 C 被饿死，因为后续可能一直来 MDL 读锁，导致 C 一直不能被执行）。</p>
<h2 id="行锁"><a href="#行锁" class="headerlink" title="行锁"></a><strong>行锁</strong></h2><p>MySQL 的行锁是在引擎层实现的。但不是所有的引擎都支持行锁。</p>
<p>行锁的两阶段锁协议：在 InnboDB 的事务中，行锁在需要时才加上，等到事务结束时才释放。</p>
<h2 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h2><h1 id="事务到底是隔离的还是不隔离的？"><a href="#事务到底是隔离的还是不隔离的？" class="headerlink" title="事务到底是隔离的还是不隔离的？"></a>事务到底是隔离的还是不隔离的？</h1><p>事务的两种启动方式</p>
<ul>
<li>begin/start transaction，直到执行第一个操作表的语句才开启事务</li>
<li>start transaction with consistent snapshot，语句一声明就开启事务</li>
</ul>
<p>InnoDB 中每个事务都有一个唯一 ID，是在事务开始的时候，按照申请顺序严格递增的。</p>
<p>每次事务对数据的更新操作，随之会把事务 ID 和数据版本对应起来。</p>
<p>在可重复读的隔离级别下，事务启动的时候，在整个事务期间采用一个一致性视图。即这个事务是看得到在它启动前已经提交的事务的更新结果。</p>
<p>在实现上，InnoDB 为每个事务构造了一个数组，用来存储在该事务启动时，系统中还在“活跃”的事务 id。“活跃”指的是已经启动，但是没有提交的事务。</p>
<p>数组中存在着最小和最大的事务 ID 。</p>
<p>事务就会根据这个 “活跃”数组，来<strong>找自己认可的数据版本（对应着 transaction id）</strong></p>
<p> </p>
<p>事务判断要操作的这个数据的当前版本，<strong>版本 ID 和最小最大 ID 的大小比较</strong>（不是存不存在）：</p>
<ul>
<li>小于数组中的最小 ID，说明这个版本是在其启动前已经提交的，认可它。</li>
<li>大于最大 ID，说明是未来已提交的，不认可。</li>
<li>在中间，那么存在两种情况<ul>
<li>在这个数组中有相同的 ID ，说明这个事务是“活跃”的，说明还没提交，不认可。</li>
<li>没有相同的 ID，说明已经提交，要认可，即对于这个事务来说是可见的。</li>
</ul>
</li>
</ul>
<p><img src="/2021/07/10/Mysql-geektime-study-record/image(15).png" alt="image(15)"></p>
<p><img src="/2021/07/10/Mysql-geektime-study-record/image(16).png" alt="image(16)"></p>
<h2 id="当前读"><a href="#当前读" class="headerlink" title="当前读"></a>当前读</h2><p>库中有数据（id，k）- (1，1)，事务 A get 到的 k 是 1，这个好理解。</p>
<p>但是事务 B get 到的 k 是 3，这就让人很疑惑了。</p>
<p><strong>当前读</strong>说的是：在<strong>更新</strong>的时候，总是先读再写，读的值必须的<strong>当前的值。</strong></p>
<p>事务 C 改成下面这样呢？</p>
<p><img src="/2021/07/10/Mysql-geektime-study-record/image(17).png" alt="image(17)"></p>
<p>事务 C’ 没有马上提交，而 B 要更新 k 值，需要进行当前读。因为 C’ 没有提交，该行的写锁还没释放，B 就会被阻塞。</p>
<h1 id="第二部分"><a href="#第二部分" class="headerlink" title="第二部分"></a>第二部分</h1><h1 id="普通索引和唯一索引应该怎么选择？"><a href="#普通索引和唯一索引应该怎么选择？" class="headerlink" title="普通索引和唯一索引应该怎么选择？"></a>普通索引和唯一索引应该怎么选择？</h1><p>老样子，看业务场景。</p>
<p>唯一索引的缺点是 <strong>在插入的时候</strong>一定要在内存中进行唯一判断才能插入。</p>
<p>普通索引的缺点是 <strong>在查询的时候</strong>相比与唯一索引<strong>可能需要</strong>多一次判断。</p>
<p>但是，存储引擎一般都是按页从磁盘读取数据，所以普通索引在查询时候的多一次判断的数据大概率已经被读取到内存中，所以只要多一次指针寻找和计算。开销可以说是微乎其微的。</p>
<h2 id="change-buffer"><a href="#change-buffer" class="headerlink" title="change buffer"></a>change buffer</h2><p><strong>对于更新操作</strong>，普通索引和唯一索引就存在不同的开销了。这都要归因于 change buffer。</p>
<p>当需要更新一个数据时，如果数据页在内存中就直接更新。如果不在内存中，InnoDB 会先把更新操作记录到 change buffer，暂时不从磁盘读取数据以减少 IO 开销。</p>
<p>等到有查询该数据的请求的时候，才会进行磁盘 IO，并把 change buffer 中的操作合并，返回结果。这样就避免了数据不一致。</p>
<p>change buffer 中存的操作应用到实际的数据页上，有三种情况</p>
<ul>
<li>后台线程定期 merge</li>
<li>访问到这个数据页的时候</li>
<li>数据库正常关闭的时候</li>
</ul>
<p>change buffer 还是可以持久化的，这应该是避免数据库不正常关闭的情况。</p>
<p>对于普通索引和唯一索引，change buffer 对更新会有什么影响呢？</p>
<ul>
<li>更新的唯一索引需要判断唯一性，总是要读入内存来判断，利用不了 change buffer 带来的优势。</li>
<li>对于更新已经在内存中的数据，二者没差。</li>
</ul>
<h2 id="最佳实践-3"><a href="#最佳实践-3" class="headerlink" title="最佳实践"></a>最佳实践</h2><p>都得看业务场景，对于读多的场景，更新请求记录在 change buffer，避免了磁盘 IO。但是马上对该数据的读请求来了，这是一定要读取的内存的请求，change buffer 就没啥用了，可能反而增加了记录和 merge 的开销。</p>
<p><strong>“尽量选择普通索引”</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">show variables like &#x27;%innodb_change_buffer%&#x27;;</span><br></pre></td></tr></table></figure>

<h1 id><a href="#" class="headerlink" title></a></h1><h1 id="MySQL-有时候为什么会选错索引？"><a href="#MySQL-有时候为什么会选错索引？" class="headerlink" title="MySQL 有时候为什么会选错索引？"></a>MySQL 有时候为什么会选错索引？</h1><p>选择索引是优化器的工作，其中的影响因素包括了</p>
<ul>
<li>扫描行数</li>
<li>是否采用临时表</li>
<li>是否排序</li>
<li>等等。。</li>
</ul>
<p>采样分析计算索引信息，所以可能不太准确。</p>
<p>通过 explain 来判断一个语句的执行情况</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">explain select * from t where (a between 1 and 1000)  and (b between 50000 and 100000) order by b limit 1;</span><br></pre></td></tr></table></figure>

<p><img src="/2021/07/10/Mysql-geektime-study-record/image(18)-2559058.png" alt="image(18)"></p>
<ul>
<li>rows 代表可能扫描的行数</li>
<li>Extra 中说明使用了索引</li>
<li>key 代码会选择索引 b 进行查询</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">explain select * from t where (a between 1 and 1000)  and (b between 50000 and 100000) order by b,a limit 1;</span><br></pre></td></tr></table></figure>

<p><img src="/2021/07/10/Mysql-geektime-study-record/image(19).png" alt="image(19)"></p>
<h2 id="直接最佳实践"><a href="#直接最佳实践" class="headerlink" title="直接最佳实践"></a>直接最佳实践</h2><ul>
<li>人为强制选择索引，使用 force index()</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t force index(a) <span class="keyword">where</span> a <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">and</span> <span class="number">10000</span> ...;</span><br></pre></td></tr></table></figure>

<ul>
<li>analyze table <table name>，矫正一下，重新计算索引信息</table></li>
</ul>
<h2 id="怎么给字符串字段加索引？"><a href="#怎么给字符串字段加索引？" class="headerlink" title="怎么给字符串字段加索引？"></a>怎么给字符串字段加索引？</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table SUser add index index1(email);</span><br><span class="line">或</span><br><span class="line">alter table SUser add index index2(email(6));</span><br></pre></td></tr></table></figure>

<p>使用前缀索引，定义合适的长度，既可以节省索引的空间，又能减少查询的开销。</p>
<h1 id="为什么表数据删掉一半，表文件大小不变？"><a href="#为什么表数据删掉一半，表文件大小不变？" class="headerlink" title="为什么表数据删掉一半，表文件大小不变？"></a>为什么表数据删掉一半，表文件大小不变？</h1><ul>
<li>innodb_file_per_table，5.6.6 后，默认是 ON。这个参数 ON 的时候代表每个表都用单独的文件存储。OFF 代表表数据存储在共享空间中。</li>
</ul>
<p>MySQL 中记录数据的删除实际上不是我们理解的真正的删除，而是标记删除，表示该位置可复用。</p>
<p>当整个数据页没有数据的时候，MySQL 通过把它标记为可复用（即空间没有变小），但是该数据页可以复用到任何位置。（也就是说不受）</p>
<p>更新索引上的值也和删除一样，会造成数据页中的空洞。因为索引需要保证有序性。</p>
<h2 id="重建表"><a href="#重建表" class="headerlink" title="重建表"></a>重建表</h2><p>重建表的大概步骤是重新读取表 A 中的值，依次放到临时表中，紧凑的排列，然后交换表名，删除旧表，启用新表。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table &lt;table name&gt; engine=InnoDB;</span><br></pre></td></tr></table></figure>

<h1 id="“order-by”-到底是怎么工作的？"><a href="#“order-by”-到底是怎么工作的？" class="headerlink" title="“order by” 到底是怎么工作的？"></a>“order by” 到底是怎么工作的？</h1>]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>专栏学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis IO多路复用</title>
    <url>/2021/04/14/Redis-Multi-IO-Model/</url>
    <content><![CDATA[<p>常说 Redis 利用 I/O 多路复用，单线程处理来自许多客户端的网络请求。本文简单的从网络通信、I/O 模型、Redis 大致如何利用 I/O 多路复用模型了解下相关知识。</p>
<span id="more"></span>

<h2 id="什么是-socket-？"><a href="#什么是-socket-？" class="headerlink" title="什么是 socket ？"></a>什么是 socket ？</h2><ul>
<li><a href="https://en.wikipedia.org/wiki/Network_socket">https://en.wikipedia.org/wiki/Network_socket</a></li>
<li><a href="https://docs.oracle.com/javase/tutorial/networking/sockets/definition.html">https://docs.oracle.com/javase/tutorial/networking/sockets/definition.html</a></li>
<li><a href="https://man7.org/linux/man-pages/man2/socket.2.html">https://man7.org/linux/man-pages/man2/socket.2.html</a></li>
<li><a href="https://www.geeksforgeeks.org/socket-programming-cc/#:~:text=Socket%20programming%20is%20a%20way,other%20to%20form%20a%20connection.">https://www.geeksforgeeks.org/socket-programming-cc/#:~:text=Socket%20programming%20is%20a%20way,other%20to%20form%20a%20connection.</a></li>
</ul>
<h2 id="socket-网络模型的基本函数"><a href="#socket-网络模型的基本函数" class="headerlink" title="socket 网络模型的基本函数"></a>socket 网络模型的基本函数</h2><ul>
<li><a href="https://man7.org/linux/man-pages/man2/socket.2.html">socket()</a>，为了进行网络 I/O 通信，<strong>进程必须做的第一件事情就是调用 socket() 函数</strong>，指定期望的通信协议类型等。socket() 调用成功会返回一个非负整数值，称为 sockfd，一般称作特指 socket 的<a href="https://en.wikipedia.org/wiki/File_descriptor">文件描述符</a>。</li>
<li><a href="https://man7.org/linux/man-pages/man2/connect.2.html">connect()</a>，客户端通过调用 connect() 函数来请求连接。</li>
<li>bind()，绑定 socket 和给定的地址和端口。如果服务端或者客户端没有调用 bind() 进行绑定，当调用 connect() 或 listen() 时，内核会为相应的 socket 选择一个临时端口。对于服务端来说，<strong>因为需要对外提供服务，所以服务端一般会主动调用 bind() 指定特定的端口和 socket 绑定来对外提供服务。</strong></li>
<li><a href="https://stackoverflow.com/questions/4696812/passive-and-active-sockets">listen()</a>，当通过 socket() 创建一个新的 socket 时，一般这个 socket 被称为<strong>主动套接字，也就是说该套接字被看作可能调用 connect() 函数发起连接请求的。</strong>在向一个未连接的套接字调用 listen() 函数后，<strong>套接字转换为被动套接字，</strong>内核就知道应该接受指向该套接字的连接请求。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int listen(int sockfd, int backlog);</span><br></pre></td></tr></table></figure>

<p>其中的第二个参数 backlog 定义了可以为当前套接字进行连接的最大队列数。也就是说，如果客户端连接请求到达时，该套接字的处理队列长度达到 backlog 时，返回 error。</p>
<ul>
<li><a href="https://man7.org/linux/man-pages/man2/accept.2.html">accept()</a>，accept() 会从待处理连接队列头中取出连接请求，并在传入的第一个参数 sockfd 指向的监听套接字（或者说被动套接字）上创建一个<strong>新的已连接套接字，返回的 int 就是指向生成新套接字的 sockfd。</strong>原本的监听套接字不受影响。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);</span><br></pre></td></tr></table></figure>

<p><strong>内核会为服务器对每个客户端的连接创建一个已连接套接字，</strong>且当服务结束后，相应的已连接套接字会被关闭（服务端的监听套接字一般持续存在）。</p>
<h2 id="5-种-I-O-模型"><a href="#5-种-I-O-模型" class="headerlink" title="5 种 I/O 模型"></a>5 种 I/O 模型</h2><blockquote>
<p><a href="https://notes.shichao.io/unp/ch6/#io-models">https://notes.shichao.io/unp/ch6/#io-models</a></p>
</blockquote>
<ul>
<li>阻塞 I/O</li>
<li>非阻塞 I/O</li>
<li>I/O 复用</li>
<li>信号驱动 I/O</li>
<li>异步 I/O</li>
</ul>
<h3 id="阻塞-I-O-模型"><a href="#阻塞-I-O-模型" class="headerlink" title="阻塞 I/O 模型"></a>阻塞 I/O 模型</h3><p><img src="/2021/04/14/Redis-Multi-IO-Model/image-20210925114649571.png" alt="image-20210925114649571"></p>
<h3 id="I-O-多路复用模型"><a href="#I-O-多路复用模型" class="headerlink" title="I/O 多路复用模型"></a>I/O 多路复用模型</h3><p><img src="/2021/04/14/Redis-Multi-IO-Model/image-20210925114705214.png" alt="image-20210925114705214"></p>
<p>在这个模型里，程序会阻塞在 select 调用上。select 函数允许程序<strong>同时监听多个 fd</strong>的就绪状态。</p>
<h2 id="select-poll-epoll-kqueue-等调用"><a href="#select-poll-epoll-kqueue-等调用" class="headerlink" title="select/poll/epoll, kqueue 等调用"></a>select/poll/epoll, kqueue 等调用</h2><ul>
<li><a href="https://man7.org/linux/man-pages/man2/select.2.html">https://man7.org/linux/man-pages/man2/select.2.html</a></li>
<li><a href="https://en.wikipedia.org/wiki/Select_(Unix)">https://en.wikipedia.org/wiki/Select_(Unix)</a></li>
<li>……</li>
</ul>
<h2 id="Redis-与-I-O-多路复用模型"><a href="#Redis-与-I-O-多路复用模型" class="headerlink" title="Redis 与 I/O 多路复用模型"></a>Redis 与 I/O 多路复用模型</h2><p>通常我们说 Redis 采用单线程架构且提供高并发访问。<strong>这里说的单线程其实是指 Redis 对于命令执行和网络 I/O 处理采用单个主线程</strong>。但是像 bgsave 等功能其实会使用到其他进程。</p>
<p>（<em>在 Redis 6.0 中，Redis 对</em><em><strong>网络请求</strong></em><em>模块采用了多线程处理</em>）</p>
<p>Redis 采用 Client/Server 访问架构，需要同时处理许多来自外部客户端的请求，也就意味着 Redis Server 会为每个客户端在本地维护一个对应的 socket。Redis 通过统一封装不同支持 I/O 多路复用的系统函数供上层使用，比如 select/epoll，kqueue 等系统调用，在不同平台上提供服务。即 Redis 不阻塞在单一的 sockfd 读写等待上，而是同时监听多个 sockfd 的就绪状态，不断处理就绪可处理的 sockfd。</p>
<p>具体来说，Redis 通过提供自己的文件事件处理器来实现相关的功能。</p>
<ul>
<li>I/O 多路复用程序同时监听多个 sockfd，当其中有 sockfd 准备就绪(产生对应的事件)，I/O 多路复用程序将就绪的 sockfd 放入准备好的队列，同步有序地一个一个将套接字给 file event dispatcher。</li>
<li>file event dispatcher 根据传来的对应事件分配给对应的事件处理器进行处理。</li>
</ul>
<p><img src="/2021/04/14/Redis-Multi-IO-Model/image-20210925114735701.png" alt="image-20210925114735701"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Redis 通过利用 I/O 多路复用模型，结合简洁的模块设置，让 Redis 在单线程架构的基础上同时为多个客户端提供服务。</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://draveness.me/redis-io-multiplexing/">Redis 和 I/O 多路复用 - 面向信仰编程</a></li>
<li><a href="https://notes.shichao.io/unp/ch6/#io-models">https://notes.shichao.io/unp/ch6/#io-models</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/63179839">epoll的本质 知乎</a></li>
</ul>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>io</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringBoot 起步依赖和自动配置</title>
    <url>/2021/02/28/SpringBoot-autoconfiguration-starter/</url>
    <content><![CDATA[<h1 id="起步依赖"><a href="#起步依赖" class="headerlink" title="起步依赖"></a>起步依赖</h1><p>不用定义版本，起步依赖中包含的依赖版本都交由 SpringBoot 的版本进行管理。<span id="more"></span></p>
<p>如果你想要排除某个起步依赖中包含的不需要的依赖，在起步依赖中加 <exclusions><exclusion><groupId>即可</groupId></exclusion></exclusions></p>
<p>如果需要制定某个依赖的版本，额外正常定义指定即可，Maven 会使用最近定义的依赖，覆盖传递依赖引入的另一个依赖。如果用 Gradle，<strong>因为 Gradle 倾向于引入较新版本<strong><strong>的</strong></strong>依赖</strong>，如果你需要<strong>指定较低<strong><strong>版本</strong></strong>的依赖</strong>，需要先排除起步依赖中的相关依赖，再引入特需版本依赖。</p>
<h1 id="自动配置"><a href="#自动配置" class="headerlink" title="自动配置"></a>自动配置</h1><p>每当程序启动的时候，SpringBoot 的自动配置都需要根据 ClassPath 里是否有某个类来做合适的 Bean 配置。根据各种内置的自动配置类的条件是否匹配进行一些配置方便用户直接使用。</p>
<h2 id="条件化配置"><a href="#条件化配置" class="headerlink" title="条件化配置"></a>条件化配置</h2><p>自动配置的类都在 spring-boot-autoconfigure 这个jar 包中。里面的自动配置类使用到了 Spring 提供的<strong>条件化配置</strong>（Spring 4.0开始引入）。Condition 类与 @Conditional 注解。</p>
<p><img src="/2021/02/28/SpringBoot-autoconfiguration-starter/image-20210925164916991.png" alt="image-20210925164916991"></p>
<p>SpringBoot 提供的丰富的条件化注解</p>
<p><img src="/2021/02/28/SpringBoot-autoconfiguration-starter/image-20210925164929129.png" alt="image-20210925164929129"></p>
<h2 id="覆盖自动配置"><a href="#覆盖自动配置" class="headerlink" title="覆盖自动配置"></a>覆盖自动配置</h2><p>自动配置的 @ConditionalOnMissingBean 注解，是覆盖自动配置的关键。</p>
<h2 id="自动配置的各种属性配置微调"><a href="#自动配置的各种属性配置微调" class="headerlink" title="自动配置的各种属性配置微调"></a>自动配置的各种属性配置微调</h2><p><img src="/2021/02/28/SpringBoot-autoconfiguration-starter/image-20210925164941741.png" alt="image-20210925164941741"></p>
<h1 id="相关书籍"><a href="#相关书籍" class="headerlink" title="相关书籍"></a>相关书籍</h1><p>《SpringBoot 实战》</p>
]]></content>
      <categories>
        <category>Spring Framework</category>
      </categories>
      <tags>
        <tag>spring boot</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构与算法学习记录概览</title>
    <url>/2021/02/18/ds-algo-study-record-overview/</url>
    <content><![CDATA[<h1 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h1><p><strong>时间复杂度：</strong>代码执行时间随着数据规模变化的趋势</p>
<p><strong>空间复杂度：</strong>代码消耗空间随着数据规模变化的趋势</p>
<span id="more"></span>

<h1 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h1><p>一组连续的，存储相同数据类型的内存空间。</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>提供 根据下标 的 时间复杂度为 O(1) 的元素访问</p>
<p>插入，删除操作。<strong>交换，标记</strong></p>
<h1 id="链表"><a href="#链表" class="headerlink" title="链表"></a><strong>链表</strong></h1><p><strong>通过 指针(引用) 将零散的内存块连接起来</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public class ListNode &#123;</span><br><span class="line">    int value;</span><br><span class="line">    ListNode next;</span><br><span class="line">    public ListNode (int value) &#123;</span><br><span class="line">        this.value = value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="单链表"><a href="#单链表" class="headerlink" title="单链表"></a><strong>单链表</strong></h2><h2 id="循环链表"><a href="#循环链表" class="headerlink" title="循环链表"></a>循环链表</h2><h2 id="双向链表，双向循环链表"><a href="#双向链表，双向循环链表" class="headerlink" title="双向链表，双向循环链表"></a><strong>双向链表，双向循环链表</strong></h2><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><p>理论上，删除，插入操作只需要修改指针的指向，为 O(1) 的操作。但是，在实际生产环境中，删除，插入操作有两种情况（双向链表的好处）</p>
<ol>
<li>操作<strong>给定值</strong>条件的结点。这种情况需要遍历链表，找到目标</li>
<li>操作<strong>给定指针</strong>指向的结点。这时已经可以访问到目标结点，但是要对它进行操作，需要知道它的前驱结点。这时双向链表就可以提供 O(1) 的操作，单链表则需进行遍历。</li>
</ol>
<h2 id="链表代码"><a href="#链表代码" class="headerlink" title="链表代码"></a>链表代码</h2><p><strong>务必理解 指针 或者 引用的概念</strong></p>
<h3 id="链表的边界条件处理"><a href="#链表的边界条件处理" class="headerlink" title="链表的边界条件处理"></a>链表的边界条件处理</h3><pre><code>1. 空链表
2. 链表只剩一个结点
3. 链表剩两个结点
4. 对于代码处理到 **头结点，尾结点**的情况
</code></pre>
<p>例如删除操作，正常情况下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">p.next = p.next.next;</span><br></pre></td></tr></table></figure>

<p>如果链表只剩一个结点，则会出现错误，改进方法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (p.next == null) p = null;</span><br></pre></td></tr></table></figure>

<p>对于插入操作</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">newNode.next = p.next;</span><br><span class="line">p.next = newNode;</span><br></pre></td></tr></table></figure>

<p>如果是空链表，p.next 会出问题（null.next），改进方法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (p == null) p = newNode;</span><br></pre></td></tr></table></figure>

<h3 id="哨兵结点"><a href="#哨兵结点" class="headerlink" title="哨兵结点"></a>哨兵结点</h3><p>对 <strong>插入第一个结点</strong> 和 <strong>删除最后一个结点</strong> 需要进行特殊处理</p>
<p>在任何时候，不管链表是否为空，head 指针始终指向该虚拟结点。</p>
<p>对于插入操作的空链表情况，删除操作的只剩一个结点情况</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 插入操作，head 始终指向 dummy</span><br><span class="line">Node dummy = new Node(..);</span><br><span class="line">newNode.next = p.next; //此时 p 指向 dummy</span><br><span class="line">p.next = newNode</span><br><span class="line"></span><br><span class="line">// 删除操作</span><br><span class="line">p.next = p.next.next; // 此时 p 指向 dummy 结点</span><br></pre></td></tr></table></figure>

<h1 id="堆栈"><a href="#堆栈" class="headerlink" title="堆栈"></a>堆栈</h1><p>栈，操作受限的线性表。只允许在其一端进行操作(pop,push)</p>
<h2 id="简单栈的实现"><a href="#简单栈的实现" class="headerlink" title="简单栈的实现"></a>简单栈的实现</h2><p>顺序栈，链式栈 (一个 top 指针)</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="函数调用栈"><a href="#函数调用栈" class="headerlink" title="函数调用栈"></a>函数调用栈</h3><p>操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。<strong>每</strong>进入一个函数，就会将临时变量作为<strong>一个栈帧</strong>入栈，当被调用函数执行完成，返回之后，将这个<strong>函数对应的栈帧</strong>出栈。</p>
<h3 id="表达式计算"><a href="#表达式计算" class="headerlink" title="表达式计算"></a>表达式计算</h3><p>编译器通过两个栈，操作数栈，运算符栈。从左向右扫描表达式，遇到数字压入操作数栈，遇到运算符，和运算符栈的栈顶元素比较运算符优先级，如果较高，压入栈顶。如果优先级相同或者比较低，取出栈顶运算符，从操作数栈取两个数运算。把结果压入操作数栈顶，运算符继续和运算符栈顶元素比较。</p>
<h3 id="括号匹配"><a href="#括号匹配" class="headerlink" title="括号匹配"></a>括号匹配</h3><h1 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h1><p>先进先出，后进后出。</p>
<p>enqueue，dequeue</p>
<h2 id="队列实现"><a href="#队列实现" class="headerlink" title="队列实现"></a>队列实现</h2><p>顺序队列，链式队列</p>
<p>循环队列</p>
<p><img src="/2021/02/18/ds-algo-study-record-overview/1.png" alt="1"></p>
<h2 id="应用-1"><a href="#应用-1" class="headerlink" title="应用"></a>应用</h2><h1 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h1><h2 id="分析排序算法的一些指标"><a href="#分析排序算法的一些指标" class="headerlink" title="分析排序算法的一些指标"></a>分析排序算法的一些指标</h2><ul>
<li><strong>稳定的排序算法。对于相等的元素，他们的前后顺序会不会改变。</strong></li>
<li><strong>是否是原地排序。没有使用额外空间，空间复杂度为 O(1) 的排序算法</strong></li>
<li><strong>最好，最坏，平均时间复杂度</strong></li>
<li><strong>比较，交换次数</strong></li>
</ul>
<h2 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a><strong>冒泡排序</strong></h2><p>在比较相邻元素时，相等时不交换彼此之间的位置。<strong>稳定。</strong></p>
<p>最好情况下，发现没有交换元素操作，提前退出。O(n)</p>
<p>最坏，O(n^2)</p>
<h2 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h2><p>把第一个元素先作为<strong>已处理区间</strong>，之后每次在<strong>未处理区间</strong>中选择元素插入到已处理区间的合适位置。</p>
<h2 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h2><p>一开始全视为未处理区间，在未处理区间中选择最小（最大）的元素，依次放到已处理区间中。（交换思想）</p>
<h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><p>分区函数。</p>
<h2 id="线性排序"><a href="#线性排序" class="headerlink" title="线性排序"></a>线性排序</h2><p>时间复杂度为 O(n) 的排序算法。对待排序数据的要求苛刻，即比较小众。</p>
<h3 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h3><h3 id="计数排序"><a href="#计数排序" class="headerlink" title="计数排序"></a>计数排序</h3><h3 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h3><h1 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 无重复的已排序区间</span><br><span class="line">public int binarySearch(int[] nums, int value) &#123;</span><br><span class="line">    int low = 0;</span><br><span class="line">    int high = nums.length - 1;</span><br><span class="line">    while(low &lt;= high) &#123;</span><br><span class="line">        int mid = (low + high) &gt;&gt; 2; // 傻逼了</span><br><span class="line">        int mid = (low + high) &gt;&gt; 1;</span><br><span class="line">        int mid = （low &gt;&gt; 1）+ （high &gt;&gt; 1）; // 位运算符的优先级和 int 溢出问题。 </span><br><span class="line">        if (nums[mid] == value) &#123;</span><br><span class="line">            return mid;</span><br><span class="line">        &#125; else if (value &lt; nums[mid]) &#123;</span><br><span class="line">            high = mid - 1;</span><br><span class="line">        &#125; else low = mid + 1;</span><br><span class="line">    &#125;</span><br><span class="line">    return -1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>二分虽然效率高，但是要求苛刻。</p>
<ol>
<li><p>底层需要是数组，可以提供 O(1) 的随机访问</p>
</li>
<li><p>数据严格有序，且不重复（看情况）。</p>
</li>
<li><h1 id="跳表"><a href="#跳表" class="headerlink" title="跳表"></a>跳表</h1></li>
</ol>
<h1 id="散列表"><a href="#散列表" class="headerlink" title="散列表"></a>散列表</h1><h2 id="散列函数"><a href="#散列函数" class="headerlink" title="散列函数"></a>散列函数</h2><h2 id="散列冲突"><a href="#散列冲突" class="headerlink" title="散列冲突"></a>散列冲突</h2><h3 id="解决散列冲突"><a href="#解决散列冲突" class="headerlink" title="解决散列冲突"></a>解决散列冲突</h3><pre><code>1. 开放寻址
    1. 线性探测。散列表中的 **空闲位置 ，**往下探测。
    2. 2 次探测
    3. 双重散列
2. 链表法。后面不一定
</code></pre>
<p><img src="/2021/02/18/ds-algo-study-record-overview/image(1).png" alt="image(1)"></p>
<h1 id="哈希算法"><a href="#哈希算法" class="headerlink" title="哈希算法"></a>哈希算法</h1><p>将任意长度的二进制串映射成 <strong>固定长度</strong> 的二进制串。</p>
<h1 id="树，二叉树"><a href="#树，二叉树" class="headerlink" title="树，二叉树"></a>树，二叉树</h1><ul>
<li>叶子节点</li>
<li>根节点，在第一层，深度为 零</li>
<li>层</li>
<li>深度</li>
<li>高度</li>
</ul>
<h2 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h2><h2 id="满二叉树"><a href="#满二叉树" class="headerlink" title="满二叉树"></a>满二叉树</h2><h2 id="完全二叉树"><a href="#完全二叉树" class="headerlink" title="完全二叉树"></a>完全二叉树</h2><h2 id="二叉树的存储方式"><a href="#二叉树的存储方式" class="headerlink" title="二叉树的存储方式"></a>二叉树的存储方式</h2><p>链式存储</p>
<p>顺序存储。基于数组的顺序存储方式，<strong>完全二叉树</strong> 的组织结构，数组空间连续被填满，中间不会浪费过多数组空间。</p>
<h2 id="遍历二叉树"><a href="#遍历二叉树" class="headerlink" title="遍历二叉树"></a>遍历二叉树</h2><p><strong>前中后序遍历的代码实现</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public void preorderTraversal(ListNode p) &#123;</span><br><span class="line">    if (p == null) return;</span><br><span class="line">    sys(p.getVal());</span><br><span class="line">    preorderTraversal(p.left);</span><br><span class="line">    preorderTraversal(p.right);</span><br><span class="line">&#125;</span><br><span class="line">public void midorderTraversal(ListNode p) &#123;</span><br><span class="line">    if (p == null) return;</span><br><span class="line">    midorderTraversal(p.left);</span><br><span class="line">    sys(p.getVal());</span><br><span class="line">    midorderTraversal(p.right);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="二叉查找树"><a href="#二叉查找树" class="headerlink" title="二叉查找树"></a>二叉查找树</h1><p><strong>二叉查找树 中的<strong><strong>每个节点</strong></strong>，其左子树中的<strong><strong>每个节点都小于这个节点</strong></strong>，右子树中的都大于这个节点。（相对于 散列表，二叉查找树数据的组织方式相对有序，且中序遍历出的数据有序）也称为 二叉排序树。</strong></p>
<h1 id="红黑树"><a href="#红黑树" class="headerlink" title="红黑树"></a><strong>红黑树</strong></h1><h1 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a><strong>堆排序</strong></h1><p><strong>堆，是一颗完全二叉树，且树中的每个节点都必须大于等于（或小等于）其所有子树中的每个节点的值。</strong></p>
<h2 id="操作堆"><a href="#操作堆" class="headerlink" title="操作堆"></a>操作堆</h2><h2 id="堆排序-1"><a href="#堆排序-1" class="headerlink" title="堆排序"></a>堆排序</h2><h2 id="堆应用"><a href="#堆应用" class="headerlink" title="堆应用"></a>堆应用</h2><h1 id="Trie-字典树"><a href="#Trie-字典树" class="headerlink" title="Trie 字典树"></a>Trie 字典树</h1><h1 id="图的表示"><a href="#图的表示" class="headerlink" title="图的表示"></a>图的表示</h1><h2 id="邻接矩阵"><a href="#邻接矩阵" class="headerlink" title="邻接矩阵"></a>邻接矩阵</h2><p><img src="/2021/02/18/ds-algo-study-record-overview/image(2)-2562505.png" alt="image(2)"></p>
<p>好处是 计算简单，获取顶点之间的关系快速。</p>
<p>比较费空间。对于无向图来说，A[i][j] 和 A[j][i] 的值相同，矩阵一半的空间都被浪费掉了。</p>
<h2 id="邻接表、逆邻接表"><a href="#邻接表、逆邻接表" class="headerlink" title="邻接表、逆邻接表"></a>邻接表、逆邻接表</h2><p><img src="/2021/02/18/ds-algo-study-record-overview/image(3).png" alt="image(3)"></p>
<p>好处是，比较节省空间，拉出的结构可以是链表，也可以是其他更高效的结构，比如红黑树，跳表等。</p>
<p>链表的存储方式对缓存不友好。</p>
]]></content>
      <categories>
        <category>Data Structures &amp; Algo</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>select 到 epoll 的多路复用</title>
    <url>/2021/09/01/epoll/</url>
    <content><![CDATA[<h2 id="从网卡接收数据说起"><a href="#从网卡接收数据说起" class="headerlink" title="从网卡接收数据说起"></a>从网卡接收数据说起</h2><ol>
<li>网卡收到传输的数据</li>
<li>并将收到的数据写到内存<span id="more"></span></li>
<li>cpu 中断。网卡将数据写入内存后，向 cpu 发出中断信号，cpu 执行网卡中断程序。（硬件的信号优先级高）</li>
</ol>
<p><img src="/2021/09/01/epoll/Snipaste_1011152521-3968094.png" alt="Snipaste_1011152521"></p>
<ol start="4">
<li>中断程序主要有两个功能。<ol>
<li>将数据写到对应的 socket （与端口对应）接收缓冲区中。执行过程中对应的进程进入<strong>对应的 socket 等待队列</strong>（阻塞状态）。</li>
<li>socket 接收到数据后，操作系统唤醒该进程，进入工作队列（运行状态）。</li>
</ol>
</li>
</ol>
<h2 id="服务端如何同时监视多个-socket-的数据状态？"><a href="#服务端如何同时监视多个-socket-的数据状态？" class="headerlink" title="服务端如何同时监视多个 socket 的数据状态？"></a>服务端如何同时监视多个 socket 的数据状态？</h2><h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><blockquote>
<p><a href="https://man7.org/linux/man-pages/man2/select.2.html">https://man7.org/linux/man-pages/man2/select.2.html</a></p>
</blockquote>
<p>假如进程 A 需要同时监控 socket1，socket2，socket3。调用 select 后，操作系统把进程 A 分别加入到<strong>这 3 个</strong> socket 的等待队列中。</p>
<p>进程A 阻塞在 select 调用上。</p>
<p>当任何一个 socket 有数据后，中断程序唤醒进程，将进程 A 从<strong>所有等待队列</strong>中移除，加入到工作队列中。进程 A 被唤醒后，说明至少有一个 socket 收到了数据，<strong>遍历 socket_sets</strong> 就可知道哪个有数据。</p>
<p>我们知道，如今的服务端需要同时管理的客户端连接数比较大。select 调用中需要多次遍历，需要同时监听的 socket 越多，select 调用的效率越低。也是因此，select 最多支持同时监听 1024 个 socket。</p>
<p><img src="/2021/09/01/epoll/Snipaste_1011162146-3968094.png" alt="Snipaste_1011162146"></p>
<h3 id="epoll"><a href="#epoll" class="headerlink" title="epoll"></a>epoll</h3><blockquote>
<p><a href="https://man7.org/linux/man-pages/man7/epoll.7.html">https://man7.org/linux/man-pages/man7/epoll.7.html</a></p>
</blockquote>
<h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4><p><img src="/2021/09/01/epoll/Snipaste_1011162455-3968094.png" alt="Snipaste_1011162455"></p>
<p><img src="/2021/09/01/epoll/Snipaste_1011162612-3968094.png" alt="Snipaste_1011162612"></p>
<p>假如进程 A 需要同时监控 socket1，socket2，socket3。先调用 <a href="https://man7.org/linux/man-pages/man2/epoll_create.2.html">epoll_create</a> 方法创建一个 eventpoll 对象（返回一个 fd 指向该对象），和 socket 类似，eventpoll 也有自己对应的等待队列。</p>
<p>调用 <a href="https://man7.org/linux/man-pages/man2/epoll_ctl.2.html">epoll_ctl</a> 添加/删除 监听的 socket（interest list（rbr））。操作系统将 eventpoll 对象添加到 socket1/2/3 的等待队列中。</p>
<p>当任何一个 socket 有数据后，中断程序为 eventpoll 的 <strong>ready list（双向链表）</strong> 添加对应的 socket 引用。<strong>唤醒等待队列中的进程。</strong></p>
<p>进程 A 执行到 <a href="https://man7.org/linux/man-pages/man2/epoll_wait.2.html">epoll_wait</a> ，等待数据，如果 <strong>ready list</strong> 不为空，epoll_wait 返回，<strong>唤醒 eventpoll 等待队列中的进程</strong>。如果为空，进程 A 阻塞在 epoll_wait 调用上等待数据，操作系统会把进程 A 加入到 <strong>eventpoll 的等待队列</strong>中。</p>
<p><img src="/2021/09/01/epoll/Snipaste_1011170111-3968094.png" alt="Snipaste_1011170111"></p>
<h4 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h4><p>epoll 通过一个中间层 <strong>eventpoll 对象及其对应的等待队列</strong>，进程被阻塞时只需要一次添加到等待队列中。</p>
<p>通过 <strong>ready list</strong> 引用有数据的 socket 。进程被唤醒后，可以直接知道哪些 socket 有数据。</p>
<h4 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h4><ul>
<li><a href="https://zhuanlan.zhihu.com/p/63179839">https://zhuanlan.zhihu.com/p/63179839</a></li>
<li><a href="https://man7.org/linux/man-pages/man7/epoll.7.html">https://man7.org/linux/man-pages/man7/epoll.7.html</a></li>
<li><a href="https://man7.org/linux/man-pages/man2/select.2.html">https://man7.org/linux/man-pages/man2/select.2.html</a></li>
<li>上面提到的直接引用 socket，操作系统中间都有一些间接结构，并非直接引用。</li>
</ul>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>io</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>FastDFS Clinet Java 源码分析</title>
    <url>/2021/08/01/fastdfs-record/</url>
    <content><![CDATA[<h2 id="什么是-FastDFS"><a href="#什么是-FastDFS" class="headerlink" title="什么是 FastDFS"></a>什么是 FastDFS</h2><blockquote>
<p><a href="https://github.com/happyfish100/fastdfs">FastDFS</a></p>
</blockquote>
<p>根据项目的描述，FastDFS 是一个开源的高性能分布式文件系统。 它的主要功能包括：文件存储，文件同步和文件访问，以及高容量和负载平衡。<span id="more"></span></p>
<h2 id="FastDFS-框架设计思路"><a href="#FastDFS-框架设计思路" class="headerlink" title="FastDFS 框架设计思路"></a>FastDFS 框架设计思路</h2><p><img src="/2021/08/01/fastdfs-record/image(4)-2663185.png" alt="image(4)"></p>
<p>FastDFS 文件系统总体上划分为三个部分，Client，Tracker Server 和 Storage Server。</p>
<ol>
<li>Storage Server ，存储服务器，负责存储数据。Storage Server 以组 （group）为单位组织，一个 group 可以有多台 Storage，数据互为冗余备份。</li>
<li>Tracker Server，负责管理 Storage Server。客户端在操作时统一通过 Tracker Server 来访问服务。</li>
<li>Client 选择 Tracker Server 进行操作。</li>
</ol>
<p>客户端上传文件，Tracker Server 选择存储的 group，在 group 中选择一个 Storage。当文件存储后，会为该文件生成一个特定的文件名，后续客户端需要通过该文件名来访问文件。</p>
<h2 id="FastDFS-Client-Java"><a href="#FastDFS-Client-Java" class="headerlink" title="FastDFS-Client-Java"></a>FastDFS-Client-Java</h2><blockquote>
<p><a href="https://github.com/happyfish100/fastdfs-client-java">FastDFS-Client-Java</a></p>
</blockquote>
<p>引入依赖，中心仓库没有的话可以手动下载源码 install。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">      &lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;org.csource&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;fastdfs-client-java&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;1.29-SNAPSHOT&lt;/version&gt;</span><br><span class="line">      &lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>ClientGlobal 加载配置，有多种方式，.conf，.properties ，Properties 对象注入均可，具体看官方链接说明（上面链接）</p>
<p>ClientGlobal 中的 TrackerGroup g_tracker_group; 字段用来保存配置的地址等信息。</p>
<p>初始化客户端，通过 StorageClient  提供的 API 操作文件。</p>
<p><img src="/2021/08/01/fastdfs-record/image(5).png" alt="image(5)"></p>
<p>以文件上传为例：</p>
<p>客户端调用 StorageClient 提供的<strong>upload_file</strong>(… args)  方法。</p>
<p><img src="/2021/08/01/fastdfs-record/image(6).png" alt="image(6)"></p>
<p>所有暴露出来的接口，最终都由 <strong>do_upload_file()</strong> 执行。</p>
<p><strong>do_upload_file(）</strong>先保证当前 Client 有可用的 storageServer 连接，通过 <strong>newxxxxStorageConnection()</strong> 保证。</p>
<p><img src="/2021/08/01/fastdfs-record/image(7).png" alt="image(7)"></p>
<p>后通过 storageServer 获取 Connection。</p>
<p><img src="/2021/08/01/fastdfs-record/image(8).png" alt="image(8)"></p>
<p>上传成功后，返回 group 和根据内部规则生成的 remote_file_name（后续访问文件则需要根据这个信息进行访问）。</p>
<p><img src="/2021/08/01/fastdfs-record/image(9).png" alt="image(9)"></p>
<p>最后根据是否开启了连接池配置，返回连接或者关闭连接。</p>
<p><img src="/2021/08/01/fastdfs-record/image(10).png" alt="image(10)"></p>
]]></content>
      <categories>
        <category>FastDFS</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>fastdfs</tag>
      </tags>
  </entry>
  <entry>
    <title>GC 策略结合日志分析</title>
    <url>/2021/07/09/gc-algo-in-action/</url>
    <content><![CDATA[<h2 id="Java-8-默认-GC-策略，并行-GC"><a href="#Java-8-默认-GC-策略，并行-GC" class="headerlink" title="Java 8 默认 GC 策略，并行 GC"></a>Java 8 默认 GC 策略，并行 GC</h2><p>java -Xloggc:gc.java_8_default.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xms512m -Xmx512m week02.work01.GCLogAnalysis  <span id="more"></span></p>
<p>共生成对象次数：10277 次</p>
<p>因为Java 8 默认采用 ParallelGC 策略，而策略默认会打开 AdaptiveSizePolicy 自适应大小策略。</p>
<p>添加参数关闭 -XX:-UseAdaptiveSizePolicy 后，可以看到 Y 和 O 按照默认的 1：2 分配。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CommandLine flags: -XX:InitialHeapSize=536870912 -XX:MaxHeapSize=536870912 -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:-UseAdaptiveSizePolicy -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC </span><br></pre></td></tr></table></figure>

<p>首先日志打印了部分相关的启动参数配置，可以看到默认使用了 ParallelGC，开启了压缩指针。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0.231: [GC (Allocation Failure) [PSYoungGen: 131501K-&gt;21491K(153088K)] 131501K-&gt;47291K(502784K), 0.0243533 secs] [Times: user=0.02 sys=0.08, real=0.02 secs] </span><br></pre></td></tr></table></figure>

<p>第一次 GC 发生在 Y 区，原因是 Allocation Failure，Eden 区没有足够的空间分配对象触发的。 此次 GC 后 Y 区占用空间少了了大概 110 M，总空间占用大概少了 80 M。大概晋升了 30M 到老年代中。用时 24 ms。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0.497: [Full GC (Ergonomics) [PSYoungGen: 21495K-&gt;0K(153088K)] [ParOldGen: 315331K-&gt;232451K(349696K)] 336826K-&gt;232451K(502784K), [Metaspace: 2575K-&gt;2575K(1056768K)], 0.1176023 secs] [Times: user=0.07 sys=0.35, real=0.11 secs] </span><br></pre></td></tr></table></figure>

<p>大概 8 次 YGC 后，发生第一次 FGC。这次的 FGC ，Y 区被清空，释放大概 20 M空间，O 区释放了大概 80 M 。总空间大概释放了 100 M，暂停时间 117 ms。<br>发生 FGC 的原因： <a href="https://www.zhihu.com/question/41922036/answer/93079526">Major GC和Full GC的区别是什么？触发条件呢？ - 知乎</a>。</p>
<p>不同的 GC 策略有着不同的触发条件，Full GC 的执行方式也可能不一样。</p>
<p>并行 GC 默认启用 CPU 核心数的 GC 线程，**-XX:ParallelGCThreads=N** 控制。也是就说并行 GC 时 CPU 资源倾力去 GC。<strong>使用并行 GC 策略目标更倾向于提高系统整体的吞吐量，单次的 GC 暂停时间有时可能会比较长</strong>，业务线程的响应可能比较慢。</p>
<h2 id="串行-GC"><a href="#串行-GC" class="headerlink" title="串行 GC"></a>串行 GC</h2><p>java -Xloggc:gc.serialGC.log -XX:+UseSerialGC -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xms512m -Xmx512m week02.work01.GCLogAnalysis </p>
<p>共生成对象次数:11672</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0.245: [GC (Allocation Failure) 2021-08-10T22:00:12.680-0800: 0.245: [DefNew: 139776K-&gt;17472K(157248K), 0.0273220 secs] 139776K-&gt;46776K(506816K), 0.0275854 secs] [Times: user=0.00 sys=0.02, real=0.02 secs] </span><br></pre></td></tr></table></figure>

<p>YGC 和 并行 GC 的 log 日志大概是一样，DefNew -年轻代释放了大概 120 M 的空间，总空间减少了大概 90 M。可以知道此次 YGC 大概有 30M 跑到了老年代。用时 27 ms。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1.054: [Full GC (Allocation Failure) 2021-08-10T22:00:13.490-0800: 1.054: [Tenured: 349521K-&gt;341013K(349568K), 0.0228613 secs] 506738K-&gt;341013K(506816K), [Metaspace: 2575K-&gt;2575K(1056768K)], 0.0229624 secs] [Times: user=0.02 sys=0.00, real=0.03 secs] </span><br></pre></td></tr></table></figure>

<p>大概发生 10 次 YGC 后，触发了一次 FGC。Tenured - 老年代释放了 10 M 的对象，总空间少了大概 160。也可以看出 DefNew 被清空了。用时 230 ms。<strong>相比并行 GC 大概慢了一倍。</strong></p>
<h2 id="CMS-GC"><a href="#CMS-GC" class="headerlink" title="CMS GC"></a>CMS GC</h2><p>java -Xloggc:gc.CMSGC.log -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xms512m -Xmx512m week02.work01.GCLogAnalysis </p>
<p>共生成对象次数:12707</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CommandLine flags: -XX:InitialHeapSize=536870912 -XX:MaxHeapSize=536870912 -XX:MaxNewSize=178958336 -XX:MaxTenuringThreshold=6 -XX:NewSize=178958336 -XX:OldPLABSize=16 -XX:OldSize=357912576 </span><br></pre></td></tr></table></figure>

<ul>
<li><strong>-XX:MaxTenuringThreshold=6，对象年龄，控制晋升速率。本身由 JVM 自动调整，6 设置一个最大的限制值。</strong></li>
<li>free-list，老年代采用<strong>标记-清除，不整理</strong>，通过维护 free-list 分配空间。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2021-08-10T22:26:09.781-0800: 0.273: [GC (Allocation Failure) 2021-08-10T22:26:09.781-0800: 0.273: [ParNew: 139776K-&gt;17471K(157248K), 0.0223003 secs] 139776K-&gt;46040K(506816K), 0.0225398 secs] [Times: user=0.02 sys=0.07, real=0.02 secs] </span><br></pre></td></tr></table></figure>

<p><strong>ParNew</strong>**<del>新生代</del>**<strong>（区分新生代（Eden 区）和年轻代）并行回收</strong>，释放了大概 120 M 空间，GC 暂停 22ms。总空间释放了大概 90 M，大概也是晋升了 30 M 的内存。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0.472: [GC (CMS Initial Mark) [1 CMS-initial-mark: 212269K(349568K)] 229884K(506816K), 0.0009085 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] </span><br><span class="line">0.474: [CMS-concurrent-mark-start]</span><br><span class="line">0.477: [CMS-concurrent-mark: 0.003/0.003 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] </span><br><span class="line">0.477: [CMS-concurrent-preclean-start]</span><br><span class="line">0.478: [CMS-concurrent-preclean: 0.000/0.000 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] </span><br><span class="line">0.478: [CMS-concurrent-abortable-preclean-start]</span><br></pre></td></tr></table></figure>

<p>针对老年代的回收，触发了 CMS 的 6 个处理阶段。可以看到</p>
<ol>
<li><strong>初始标记</strong>：<strong>STW</strong>  0.9 ms。标记所有的 Root 对象，Root 引用的对象，年轻代存活对象引用的对象。（跨代引用）</li>
<li>并发标记：遍历老年代，标记存活对象。</li>
<li>并发预清理：因为并发执行，业务线程还在执行，引用关系一直在发生改变，这一步通过卡表的概念，标记脏表。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0.581: [GC (CMS Final Remark) [YG occupancy: 18010 K (157248 K)]</span><br><span class="line">0.581: [Rescan (parallel) , 0.0002754 secs]</span><br><span class="line">0.581: [weak refs processing, 0.0000085 secs]</span><br><span class="line">0.581: [class unloading, 0.0003406 secs]</span><br><span class="line">0.582: [scrub symbol table, 0.0001719 secs]</span><br><span class="line">0.582: [scrub string table, 0.0000610 secs][1 CMS-remark: 343397K(349568K)] 361407K(506816K), 0.0011452 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] </span><br><span class="line">0.582: [CMS-concurrent-sweep-start]</span><br><span class="line">0.583: [CMS-concurrent-sweep: 0.001/0.001 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] </span><br><span class="line">0.583: [CMS-concurrent-reset-start]</span><br><span class="line">0.584: [CMS-concurrent-reset: 0.001/0.001 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] </span><br></pre></td></tr></table></figure>

<ol start="4">
<li><strong>最终标记：STW</strong>来准确的标记对象，为接下来的删除对象做准备。</li>
<li>并发清除：回收不使用的对象占用的空间</li>
<li>并发重置：重置 CMS 算法为了回收内部维护的相关数据，为下一次做准备。</li>
</ol>
<p>CMS 默认使用 CPU 核心数 / 4 的 GC 线程进行回收。<strong>CMS 的目的在于降低系统的延迟，保证业务响应，用户体验更好些</strong>。</p>
<p>在吞吐量方面，相比并行 GC 投入所有的核来GC，会差一些。</p>
<p><strong>（假设并行GC自定义线程数和 CMS 一样，并行 GC 的吞吐量的优势还在吗？</strong></p>
<p>个人理解：应该还是有的。CMS 的并发清除步骤多，分配到的 GC 线程次数多的话，业务线程分配不到资源，并发执行的优势没有了；采用的算法（没有整理，用 free-list）内存空间的利用率也是个问题。相当于分配了那么多资源，但是没发挥出来。</p>
<p>CMS 的存在的问题：</p>
<ul>
<li>浮动垃圾：最终标记后，并发清除的操作不需要 STW，在清除的过程中可能要分配新的对象内存等等，所以需要预留一定的内存空间给业务线程使用。如果没有足够的空间就会导致并发失败。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0.647: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line">0.660: [GC (Allocation Failure) 2021-08-10T22:26:10.168-0800: 0.660: [ParNew: 157246K-&gt;17471K(157248K), 0.0154900 secs] 437850K-&gt;342263K(506816K), 0.0155564 secs] [Times: user=0.09 sys=0.01, real=0.02 secs] </span><br><span class="line">0.690: [GC (Allocation Failure) 2021-08-10T22:26:10.198-0800: 0.690: [ParNew: 157247K-&gt;157247K(157248K), 0.0000176 secs]2021-08-10T22:26:10.198-0800: 0.690: [0.690: [CMS-concurrent-abortable-preclean: 0.001/0.043 secs] [Times: user=0.11 sys=0.01, real=0.04 secs] </span><br><span class="line"> (concurrent mode failure): 324792K-&gt;289452K(349568K), 0.0785084 secs] 482039K-&gt;289452K(506816K), [Metaspace: 2575K-&gt;2575K(1056768K)], 0.0785991 secs] [Times: user=0.03 sys=0.04, real=0.08 secs] </span><br><span class="line">0.782: [GC (Allocation Failure) 2021-08-10T22:26:10.290-0800: 0.782: [ParNew: 139542K-&gt;17470K(157248K), 0.0045904 secs] 428995K-&gt;335339K(506816K), 0.0046549 secs] [Times: user=0.03 sys=0.00, real=0.00 secs] </span><br><span class="line">0.787: [GC (CMS Initial Mark) [1 CMS-initial-mark: 317869K(349568K)] 335860K(506816K), 0.0001277 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] </span><br><span class="line">0.787: [CMS-concurrent-mark-start]</span><br></pre></td></tr></table></figure>

<p>可以看到在 pre-clean-start 后，日志出现了 concurrent mode failure 字样后，经历几次 Minor GC 后，重新从 Inital Mark 开始进行 CMS 操作。</p>
<p><strong>这里有个疑问，从日志看没有退化 GC 的情况，而是重新进行了 CMS。从《深入理解 Java 虚拟机 第 3 版》说会直接退化成 Serial Old GC，不知该如何验证？</strong></p>
<h2 id="G1-GC"><a href="#G1-GC" class="headerlink" title="G1 GC"></a>G1 GC</h2><p>java -Xloggc:gc.G1GC-30ms-clean.log -XX:+UseG1GC -XX:MaxGCPauseMillis=30  -XX:+PrintGCDateStamps -Xms512m -Xmx512m week02.work01.GCLogAnalysis</p>
<p>共生成对象次数：5910</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CommandLine flags: -XX:InitialHeapSize=536870912 -XX:MaxGCPauseMillis=30 -XX:MaxHeapSize=536870912 -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseG1GC -XX:-UseLargePagesIndividualAllocation </span><br><span class="line"></span><br><span class="line">6.629: [GC pause (G1 Evacuation Pause) (young) 264M-&gt;210M(512M), 0.0096778 secs]</span><br><span class="line">[GC pause (G1 Humongous Allocation) (young) (initial-mark) 234M-&gt;222M(512M), 0.0039857 secs]</span><br></pre></td></tr></table></figure>

<p>看到 G1 Humongous Allocation 因为大对象分配失败，触发了 initial-mark。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">6.650: [GC concurrent-root-region-scan-start]</span><br><span class="line">6.650: [GC concurrent-root-region-scan-end, 0.0003724 secs]</span><br><span class="line">6.650: [GC concurrent-mark-start]</span><br><span class="line">6.685: [GC concurrent-mark-end, 0.0347063 secs]</span><br><span class="line">6.698: [GC remark, 0.0030734 secs]</span><br><span class="line">6.709: [GC cleanup 319M-&gt;307M(512M), 0.0005021 secs]</span><br><span class="line">6.709: [GC concurrent-cleanup-start]</span><br><span class="line">6.709: [GC concurrent-cleanup-end, 0.0000177 secs]</span><br><span class="line">6.959: [GC pause (G1 Evacuation Pause) (young)-- 431M-&gt;394M(512M), 0.0097375 secs]</span><br><span class="line">6.978: [GC pause (G1 Evacuation Pause) (mixed) 398M-&gt;383M(512M), 0.0051953 secs]</span><br></pre></td></tr></table></figure>

<p>后开始 region 的扫描，标记，重标记，cleanup 日志都有体现。<br>mix 模式类似 FGC。</p>
]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>jvm</tag>
        <tag>java</tag>
        <tag>gc</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo+GithubPages+Mac+Win 部署问题记录</title>
    <url>/2021/01/01/hexo+win+mac/</url>
    <content><![CDATA[<h2 id="本地初始化一个Hexo项目"><a href="#本地初始化一个Hexo项目" class="headerlink" title="本地初始化一个Hexo项目"></a>本地初始化一个Hexo项目</h2><p><strong>注意：本地的目录不要动</strong>，<strong>可以重命名</strong>。</p>
<p>重新新建一个空目录，作为你的博客目录。进入该目录，初始化一个Hexo项目：<span id="more"></span></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo init</span><br><span class="line">npm install</span><br><span class="line">npm install hexo-deployer-git *--save</span><br></pre></td></tr></table></figure>

<p>然后用自己原来博客里的文件替换掉这里的<code>source\</code>, <code>scaffolds\</code>, <code>themes\</code>,<code>_config.yml</code>替换成自己原来博客里的。<strong>注意，一定要把themes/next中的.git/目录删除</strong></p>
<p>然后上传到代码仓库，同时初始化了 main 分支。</p>
<p>最后切换 git checkout -b hexo, 之后基于这个分支做修改，hexo d 部署在配置的分支上，这边就是设置的 main 分支（和 github page 里设置的分支一致可直接在 page 中访问到）。</p>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>使用到的插件列表：</p>
<p><img src="/2021/01/01/hexo+win+mac/Sni_2409222314.png" alt="Sni_2409222314"></p>
<p><code>.gitignore</code>文件中过滤了<code>node_modules\</code>，所以 clone 来的目录里没有<code>node_modules\</code>，这是hexo所需要的组件，所以要在该目录中重新安装hexo，<strong>但不需要hexo init</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo</span><br><span class="line">npm install</span><br><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>

<h3 id="不过滤-gitignore-的内容协作尝试"><a href="#不过滤-gitignore-的内容协作尝试" class="headerlink" title="不过滤 .gitignore 的内容协作尝试"></a>不过滤 .gitignore 的内容协作尝试</h3><p>hexo clean，hexo generate 正常，hexo d 部署的时候报错</p>
<p><img src="/2021/01/01/hexo+win+mac/Sni_2409230640.png" alt="Sni_2409230640"></p>
<p>.deploy_git 文件夹冲突，猜想应该是 hexo d 的时候操作 .deploy_git 的时候文件无法覆盖类似的冲突。</p>
<p>解决方法：删除 .deploy_git，重新 hexo d 生成即可。（我们可以在 .gitignore 里过滤这个文件夹）</p>
<h2 id="常见语法"><a href="#常见语法" class="headerlink" title="常见语法"></a>常见语法</h2><ul>
<li>tag: <ul>
<li>-&nbsp;tag1</li>
<li>-&nbsp;tag2</li>
</ul>
</li>
<li>空格：’&amp;nbsp + ;’</li>
<li>文章缩略显示：&lt;!–more– &gt;</li>
</ul>
<h2 id="样式设置"><a href="#样式设置" class="headerlink" title="样式设置"></a>样式设置</h2><p><a href="https://github.com/iissnan/hexo-theme-next/issues/928">关于footer修改问题 · Issue #928 · iissnan/hexo-theme-next (github.com)</a></p>
<p><a href="https://blog.csdn.net/Wonz5130/article/details/84666519">Hexo 显示分类、标签问题</a></p>
<p><a href="https://blog.csdn.net/as480133937/article/details/100138838">Hexo-Next 主题博客个性化配置</a></p>
<ul>
<li>目录自动展开和换行：主题配置文件搜索 toc。</li>
</ul>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://wandouduoduo.github.io/articles/902dbefe.html">mac和windows协同写hexo博客</a></p>
<p><a href="https://www.caoayu.xyz/post/hexo/">hexo 图片显示+typora</a></p>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL 索引概要</title>
    <url>/2021/05/28/mysql-index-3star-principle/</url>
    <content><![CDATA[<h1 id="磁盘-I-O"><a href="#磁盘-I-O" class="headerlink" title="磁盘 I/O"></a>磁盘 I/O</h1><blockquote>
<p>《数据库索引设计与优化》第二章</p>
</blockquote>
<h2 id="随机-I-O"><a href="#随机-I-O" class="headerlink" title="随机 I/O"></a>随机 I/O</h2><p>每次数据库从磁盘随机读取一个页大约会花费<strong>10 ms</strong>左右，10 ms 是根据磁盘活动等情况大致估算出来的。我们只要意识到每次随机 I/O 的成本是很高的即可。</p>
<span id="more"></span>

<h2 id><a href="#" class="headerlink" title></a><img src="/2021/05/28/mysql-index-3star-principle/image-20210925124904465.png" alt="image-20210925124904465"></h2><h2 id="顺序-I-O"><a href="#顺序-I-O" class="headerlink" title="顺序 I/O"></a>顺序 I/O</h2><p>顺序读取的速度大约在<strong>40 MB/s</strong>，对于一个 4 KB 大小的页来说，平均的页读取时间为 0.1 ms，相比随机 I/O 的 10 ms提升了两个数量级。</p>
<p>顺序读取的优势在于，DBMS 意识到将要读取多个页，将发出多页 I/O 请求。且由于 DBMS 事先知道哪些页需要被读取，可能预先将其读取。</p>
<h1 id="索引模型"><a href="#索引模型" class="headerlink" title="索引模型"></a>索引模型</h1><p>实现索引的方式有很多种，采用不同方式设计的索引在不同的场景的效率也不同。对应的效率可以类比到数据结构的特性。例如依赖哈希表设计的索引天然不适合范围查询。</p>
<ul>
<li>B-Tree</li>
<li>哈希索引</li>
<li>空间数据索引</li>
<li>全文索引</li>
</ul>
<h1 id="InnoDB-采用的索引模型"><a href="#InnoDB-采用的索引模型" class="headerlink" title="InnoDB 采用的索引模型"></a>InnoDB 采用的索引模型</h1><p>不同的存储引擎的索引的工作方式不尽相同。本文主要分析 MySQL 中最常用的存储引擎 InnoDB 的索引。</p>
<p>InnoDB 采用 B+Tree 实现索引，每张表通过主键以索引的形式存放（建表时没有指定主键，MySQL 会自动给一个 ROW_ID 作为主键），这种存储方式一般也称为索引组织表（index organized table，iot）。</p>
<p>InnoDB 实现的 B+Tree 只在叶子节点存储数据，非叶子节点只用作索引定位使用。</p>
<p>B+Tree 的特点</p>
<p><a href="https://www.javatpoint.com/b-plus-tree?fileGuid=PD9KHHH3T6GPvkdY">https://www.javatpoint.com/b-plus-tree</a></p>
<p>B+Tree 的树高低，在索引定位的过程中，访问不同数据快的次数和树高相当，可以很好的减少磁盘随机 I/O 的次数。而且一般根节点，一级索引很可能已经在内存中，磁盘随机 I/O 的次数更低了。</p>
<h1 id="主键索引与普通索引"><a href="#主键索引与普通索引" class="headerlink" title="主键索引与普通索引"></a>主键索引与普通索引</h1><p>主键索引叶子节点需要存储整行的数据。主键索引一般也成为聚簇索引（clustered index）。</p>
<p>普通索引的叶子节点存储的是主键的数据。普通索引一般称为二级索引。</p>
<p>一个查询使用到普通索引时，有可能需要回到主键索引获取对应的数据，一般称作<strong>回表</strong>。</p>
<h1 id="使用-MySQL-索引"><a href="#使用-MySQL-索引" class="headerlink" title="使用 MySQL 索引"></a>使用 MySQL 索引</h1><p><a href="https://www.mysqltutorial.org/mysql-index/?fileGuid=PD9KHHH3T6GPvkdY">https://www.mysqltutorial.org/mysql-index/</a></p>
<h1 id="索引设计、优化"><a href="#索引设计、优化" class="headerlink" title="索引设计、优化"></a>索引设计、优化</h1><p>在具体索引设计前，先了解下使用到索引的查询语句的一些特性。</p>
<ul>
<li>索引覆盖，查询的列如果在索引中存在直接通过索引中的值返回。</li>
<li>索引下推，判断谓词如若在索引内部存在，优先通过索引内部字段进行谓词判断，减少回表的过程。</li>
<li>索引最左前缀原则，比如一个索引（age，name，sex）,相当于覆盖了 （age），（age，name）这两种索引。</li>
<li>前缀索引。通过截断前缀作为索引字段。可以节省空间。但是可能会损失一些查询性能，因为数据库需要根据主键回表判断这个值。前缀索引还可以影响到索引覆盖，同理无法直接从索引树返回结果。</li>
</ul>
<p>假设有表 user（id，age，name）,主键 id，表上有索引（age，name）。</p>
<p>对于查询语句 SELECT name FROM user WHERE age = 3;</p>
<p><strong>索引覆盖</strong>：使用普通索引定位到 age = 3 的位置后，在索引中扫描返回对应的 name 字段。而<strong>不需要根据对应的主键 id 回到主索引</strong>获取 name 的值。</p>
<p>对于查询语句SELECT id FROM user WHERE age = 3 and name = “fang”;</p>
<p><strong>索引下推</strong>：使用普通索引定位到 age = 3 的位置后，在<strong>索引内部</strong>获取 name 继续判断是否符号谓词条件，最后直接访问 id。</p>
<p>但是在 MySQL 5.6 之前，只能<strong>回到主索引</strong>一个个判断 name 的值。</p>
<h1 id="理想的索引"><a href="#理想的索引" class="headerlink" title="理想的索引"></a>理想的索引</h1><blockquote>
<p>《数据库索引设计与优化》第四章提出的三星索引的概念，即对于一个查询语句可能的最好索引。如果查询语句使用了三星索引，一次查询通常只需要一次磁盘随机读以及一次窄索引片的扫描。</p>
</blockquote>
<p>对特定的查询语句，三颗星的定义如下：</p>
<ul>
<li>第一颗：查询使用到的谓词的列作为索引的开头。（通过等值谓词最小化操作集）</li>
<li>第二颗：ORDER BY 使用到的列添加到索引中。（避免结果排序）</li>
<li>第三颗：查询语句需要返回的结果集的列全部被包含在索引中。（避免回表查询，对应多次的随机磁盘 I/O）</li>
</ul>
<p>比如有表 user（id，name，age，city，gender）</p>
<p>查询语句 SELECT * FROM user WHERE age = 3 and name = “fang” order by city；</p>
<ul>
<li>第一颗：索引设计为（age，name）</li>
<li>第二颗：在后面添加 city，（age，name，city）</li>
<li>第三颗：SELECT *，索引中未包含 gender，（age，name，city，gender）</li>
</ul>
<p>书中提出说<strong>第三颗星通常是最重要的</strong>，原因是如果索引中未包含查询需要返回值时，需要回表进行多次速度较慢的<strong>磁盘随机读</strong>。</p>
<p>但是在实际场景中，想要同时满足三颗星咩有那么简单。虽然我们总是可以向索引添加所有查询需要的字段来满足第三颗星。但是这样第一颗星和第二颗星就可能会冲突。</p>
<p>比如查询语句 SELECT name, city FROM user WHERE age BETWEEN 4 AND 24 and city = “fujian” ORDER BY name;</p>
<p>首先，添加索引 （city）满足第一颗星，然后可以添加 age，（city，age）满足第三颗星，同时刚好为 between 语句避免了回表判断。现在如果为了满足第二颗星以避免排序，name 这个索引的位置应该在 age 的前面才是预期的行为。</p>
<p>联合索引会按照索引字段的顺序组织数据。对于 age BETWEEN 4 AND 24，ORDER BY name 来说，任一字段排在其他字段的前面就始终无法满足条件。（age，name) 先按照 age 的顺序排序后，name 的有序性只能在 age 相同的行之间保证。（name，age）同理。</p>
<h1 id="索引维护"><a href="#索引维护" class="headerlink" title="索引维护"></a>索引维护</h1><p>为了保证索引有序性，插入新数据时可能触发页分裂，影响到性能。</p>
<p>页分裂后会影响到页的利用率，影响到空间。</p>
<p>可以重建索引来重新组织索引。</p>
<p>alter table user drop index age; // 重建索引 age</p>
<p>alter table user drop primary key; // 重建主键索引</p>
<h1 id="索引分析"><a href="#索引分析" class="headerlink" title="索引分析"></a>索引分析</h1><p>通过 explain 分析语句的执行情况。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.6/en/explain-output.html#explain_rows?fileGuid=PD9KHHH3T6GPvkdY">https://dev.mysql.com/doc/refman/5.6/en/explain-output.html#explain_rows</a></p>
<p>使用索引的一些坑</p>
<ol>
<li><strong>谓词条件字段</strong>通过函数操作，可能导致优化器放弃选择索引。因为通过函数计算后的索引得到的值，无法通过原本有序的索引树定位数据。</li>
<li>隐式类型转换。可能会触发对索引字段做函数操作进行转换。放弃走索引树定位的原因同上。</li>
<li>隐式字符编码转换。</li>
</ol>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://draveness.me/sql-index-intro/">MySQL 索引设计概要 - 面向信仰编程</a></li>
<li><a href="https://www.javatpoint.com/b-plus-tree?fileGuid=PD9KHHH3T6GPvkdY">https://www.javatpoint.com/b-plus-tree</a></li>
<li><a href="https://www.mysqltutorial.org/mysql-index/?fileGuid=PD9KHHH3T6GPvkdY">https://www.mysqltutorial.org/mysql-index/</a></li>
</ul>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>索引设计</tag>
      </tags>
  </entry>
  <entry>
    <title>《鸟哥的Linux私房菜》学习记录</title>
    <url>/2021/03/17/niao-ge-linux-dishes-study-record/</url>
    <content><![CDATA[<p>硬件 - 内核 - 系统调用 - 应用程序。操作系统（Linux 系统）就在 内核 - 系统调用那两层。</p>
<span id="more"></span>

<p>因为不同的硬件提供的功能函数不同，所以一个操作系统可以在 Intel 的 x86 架构的硬件平台上运行，但是无法在采用其他架构的硬件上运行。早期苹果公司在 IBM 的 PowerPC CPU 硬件架构上发展的 Mac 电脑，就无法运行 Windows 系统（基于 Intel x86 架构开发）。</p>
<p>每种操作系统都是在针对特定的硬件平台上运行的。不过因为 Linux 是开源的，即大家可以获取到它的源代码，就可以在此基础上针对不同的硬件平台修改代码来运行。</p>
<h2 id="磁盘分区"><a href="#磁盘分区" class="headerlink" title="磁盘分区"></a>磁盘分区</h2><p><a href="https://en.wikipedia.org/wiki/Mount_(computing)#:~:text=Mounting%20is%20a%20process%20by,via%20the%20computer's%20file%20system.">挂载</a><strong>，</strong>文件系统和存储设备的关系。挂载就是指将文件系统中的目录<strong>挂载</strong>在存储设备的某个位置上，用户访问这个目录下的文件时，操作系统就会该目录对应的<strong>挂载点</strong>读取文件。一般这个进入的目录也称为挂载点。</p>
<p>Linux 中根目录的重要性不言而喻，所有根目录一定是挂载到某个分区的。其他的目录用户可以根据自己的需求挂载到不同的分区。</p>
<p>如果 / 目录挂载在分区 1，home 目录挂载在分区 2，那么 /test/home/myfile/two，那么 two 这个文件是在 home 所在的分区还是 根目录所在的分区呢？<strong>通过反向查找挂载点即可，先找到的挂载点在哪就是哪个挂载点。</strong>这里 two 使用的就是 /home 这个挂载点下对应的分区进行存储。</p>
<h2 id="命令行模式下的一些基础概念"><a href="#命令行模式下的一些基础概念" class="headerlink" title="命令行模式下的一些基础概念"></a>命令行模式下的一些基础概念</h2><p>基础格式：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ command [-options] parameter1 paremeter2 ...</span><br><span class="line">$ ll -al ../my</span><br><span class="line">$ ll -a -l ../my</span><br><span class="line">$ ll ../my</span><br></pre></td></tr></table></figure>

<p>Shift + PageUP 或 PageDown 在<strong>命令行模式下</strong>进行翻页。要是没有 PageUP 和 Down 键怎么办？</p>
<p>比如执行 cat 在命令行输出满屏了，可以通过**管道 |**把输出结果给可以翻页的命令，比如 less 或者 more。就可以通过 b 或者 空格键快速上下翻页了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cat fullfile | less</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ command --help，命令的简单说明，比如有哪些参数可用。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ man command，命令的详细操作手册。</span><br></pre></td></tr></table></figure>

<p>执行 man 后，会进入一个类似 Vim 的界面，可用通过 PageUP 或 PageDown（或者空格键 ）进行翻页。/String，或者 ？String，向下 向上查找出现了 String 的内容。并通过 n/N 查找匹配的下一个/上一个。<br>巨简单的一种文本编辑器：nano。图片底部的 ^ + 字母表示，Ctrl + 字母就会执行对应的操作。</p>
<p><img src="/2021/03/17/niao-ge-linux-dishes-study-record/image(1).png" alt="image(1)"></p>
<h2 id="文件属性管理"><a href="#文件属性管理" class="headerlink" title="文件属性管理"></a>文件属性管理</h2><h3 id="chgrp，chown，chmod"><a href="#chgrp，chown，chmod" class="headerlink" title="chgrp，chown，chmod"></a>chgrp，chown，chmod</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">修改文件所属用户组， -R 递归操作所有子目录</span><br><span class="line">$ chgrp [-R] groupName filename/dirname</span><br><span class="line"></span><br><span class="line">修改文件拥有者，-R 递归操作所有子目录</span><br><span class="line">$ chown [-R] useName filename/dirname</span><br><span class="line"></span><br><span class="line">修改文件权限</span><br><span class="line">1）数字形式： read = 4 = 2^2，write = 2 = 2^1，x = 1 = 2^0</span><br><span class="line">$ chmod [-R] 761 filename/dirname</span><br><span class="line">2）符号形式：a - 全部用户，o - ohters，u - user，g - group</span><br><span class="line">$ chmod [-R] u+rwx,g=rx,o-rwx,a+rwx filename/dirname</span><br></pre></td></tr></table></figure>

<p><strong>权限对于文件和目录的不同作用意义</strong>：</p>
<p>对于文件，指对<strong>文件内容</strong>的操作权限。要注意的是对文件的 write 权限，并不具备删除该文件的功能。</p>
<p>对于目录，read 权限表示可以读取目录结构列表的能力。w 则是可以删除，新增文件等等。<strong>对于目录的 x（执行权限）则代表的是用户是否有进入该目录的能力</strong>。</p>
<h2 id="Linux-目录配置"><a href="#Linux-目录配置" class="headerlink" title="Linux 目录配置"></a>Linux 目录配置</h2><p>Linux 的世界中所有东西的抽象为文件。Linux 的目录配置指的是各个不同版本的 Linux 各种目录大致应该存放什么文件，因此也诞生了 FHS（Filesystem Hierarchy Standard） 标准。</p>
<h2 id="目录管理"><a href="#目录管理" class="headerlink" title="目录管理"></a>目录管理</h2><h3 id="cd，pwd，mkdir"><a href="#cd，pwd，mkdir" class="headerlink" title="cd，pwd，mkdir"></a>cd，pwd，mkdir</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">切换到当前用户的 home 目录</span><br><span class="line">$ cd ~</span><br><span class="line"></span><br><span class="line">回到上一次的工作目录</span><br><span class="line">$ cd -</span><br><span class="line"></span><br><span class="line">$ pwd [-P], -P 选项执行输出真正路径，而非链接路径（对链接文件来说有用）</span><br><span class="line"></span><br><span class="line">-p 选项，直接创建多级目录</span><br><span class="line">$ mkdir -p my1/my2/my3</span><br></pre></td></tr></table></figure>

<p>使用 <strong>mkdir</strong> 创建的新目录默认权限是什么呢？这和 umask 有关。不过你可以也在 <strong>mkdir</strong> 时使用 <strong>-m 777</strong> 来指定权限。</p>
<h3 id="cp，rm，mv"><a href="#cp，rm，mv" class="headerlink" title="cp，rm，mv"></a>cp，rm，mv</h3><ul>
<li>cp [源文件] [目标文件]<ul>
<li>-a，一般来说 cp 复制后的文件拥有者一般是操作命令者本身。添加 -a 选项就可完完全全复制文件属性，包括权限，创建时间等等。</li>
<li>-r，目录的话可能你需要递归复制</li>
</ul>
</li>
<li>rm 文件或者目录<ul>
<li>-f，强制删除</li>
<li>-r，删除目录需要进行递归删除</li>
</ul>
</li>
<li>mv</li>
</ul>
<h2 id="文件管理"><a href="#文件管理" class="headerlink" title="文件管理"></a>文件管理</h2><h3 id="less，cat，head，tail，od，touch"><a href="#less，cat，head，tail，od，touch" class="headerlink" title="less，cat，head，tail，od，touch"></a>less，cat，head，tail，od，touch</h3><ul>
<li>head [-n number] filename</li>
<li>less，和 man 命令执行后的操作很像，比如 空格 对应 Page Down，b 对应 Page Up 等等</li>
<li>od，查看非文本文件</li>
<li>touch，新建文件或者修改文件时间<ul>
<li>atime，access time</li>
<li>mtime，modify time</li>
<li>ctime，status time，比如文件权限改变的时间。</li>
</ul>
</li>
</ul>
<h3 id="文件和目录的默认权限：umask-S，umask-新的-umask-值"><a href="#文件和目录的默认权限：umask-S，umask-新的-umask-值" class="headerlink" title="文件和目录的默认权限：umask -S，umask 新的 umask 值"></a>文件和目录的默认权限：umask -S，umask 新的 umask 值</h3><p><img src="/2021/03/17/niao-ge-linux-dishes-study-record/image(2).png" alt="image(2)"></p>
<p>0022 的数字指的是该默认权限需要减掉的权限。第一位的 0 个人猜测是 root 用户的，似乎没办法改变。后三位 022 的 0 代表 u = rwx，2 代表 group-w（即 g = rx）,同理最后一位的 2 一样，只是作用的用户是 others。</p>
<h3 id="查找文件"><a href="#查找文件" class="headerlink" title="查找文件"></a>查找文件</h3><ul>
<li>locate regexWord，从已建的数据库中查询，所以不用到处查磁盘。但是数据库更新频率不高，CentOS 7 是一天一更。可以使用 updatedb 命令更新，这个命令会花一些时间。<ul>
<li>locate /etc/sh</li>
<li>locate  ~/m</li>
<li>locate -i ~/m</li>
</ul>
</li>
<li>find <strong>[PATH]</strong><ul>
<li>find ./ -ctime 4，<strong>当前目录下</strong>，4 天前的那一天修改过 status 的文件</li>
<li>find ./ -mtime -4，4 天内被修改过内容的文件</li>
<li>find ./ -mtime +5，5 天包括更久之前修改过内容的文件</li>
</ul>
</li>
<li>find . -name <ul>
<li><a href="http://www.ruanyifeng.com/blog/2009/10/5_ways_to_search_for_files_using_the_terminal.html">Linux的五个查找命令 - 阮一峰的网络日志 (ruanyifeng.com)</a></li>
</ul>
</li>
<li>whereis</li>
<li>which</li>
</ul>
<h2 id><a href="#" class="headerlink" title></a></h2><h2 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h2><h3 id="磁盘和目录的容量：df，du"><a href="#磁盘和目录的容量：df，du" class="headerlink" title="磁盘和目录的容量：df，du"></a>磁盘和目录的容量：df，du</h3><ul>
<li>df，列出文件系统整体的磁盘使用情况<ul>
<li>-h，以我们易理解的方式输出。比如多少 G，多少 M</li>
</ul>
</li>
<li>du [options] [文件名称或者目录名称]</li>
</ul>
<h3 id="硬链接和符号链接：ln"><a href="#硬链接和符号链接：ln" class="headerlink" title="硬链接和符号链接：ln"></a>硬链接和符号链接：ln</h3><p>在 CentOS 7.x 后，默认文件系统采用 xfs 系统。</p>
<p>有一些文件相关的特点需要了解：</p>
<ul>
<li>每个文件占用一个 inode，<strong>文件内容</strong>由 inode 记录来指向。</li>
<li>想要读取文件内容，需要正确的 inode 号码才能进行读取。</li>
</ul>
<p><strong>硬链接</strong>在某个目录下新增一个文件名并链接到某个 inode 号码指向的内容。也就是说同一处的文件内容可以通过不同的文件名来进行操作。和符号链接（软链接）不同的点在于硬链接删除了其中任何一个文件，其实 inode 是还在的。</p>
<p><strong>符号链接</strong>则是在某个目录下新建一个文件名指向某个文件，这个文件名的虚的，只起到一个引用的作用。</p>
<ul>
<li>ln 源头文件 新建链接文件<ul>
<li>-s，添加个该选择设置符号链接，不添加默认硬链接</li>
</ul>
</li>
</ul>
<h2 id="-1"><a href="#-1" class="headerlink" title></a></h2><h3 id="文件的压缩"><a href="#文件的压缩" class="headerlink" title="文件的压缩"></a>文件的压缩</h3><p>压缩文件我们非常常见，一般我们可以通过文件后缀名区分文件是否被压缩且使用的压缩技术。比如 .zip，.gzip，.tar.gzip 等等。Linux 不像 Windows 通过文件后缀名辨别各种文件类型，比如 .exe，.txt，.mp3，.doc  。还记得 ll 命令或者 ls 命令的输出结果，其中第一个字符才表示对应的文件类型，- 表示普通文件，d 表示文件夹，l 表示链接文件等等。所以在 Linux 中文件后缀名对文件类型是没有什么意义的，但是有时候我们可以通过合适的文件后缀名来清晰文件类型。压缩文件也是如此。</p>
<p>简单理解一下压缩原理，操作系统通过机器码存储文件，比如 1000 0000 ，压缩技术类似将 1000 0000 处理为1 0*8 的方式处理并存储，解压缩的时候规则将实际的机器码复原即可。</p>
<h3 id="Linux-中常见的压缩命令"><a href="#Linux-中常见的压缩命令" class="headerlink" title="Linux 中常见的压缩命令"></a>Linux 中常见的压缩命令</h3><ul>
<li>.zip，zip 程序压缩</li>
<li>.gz，gzip 程序压缩</li>
<li>.tar.gz，tar 程序打包的文件通过 gzip 压缩</li>
<li>.tar.bz2，tar 程序打包的文件通过 bzip2 程序压缩</li>
</ul>
<p>通过 压缩命令仅对一个文件进行压缩解压缩，所以通过 tar 将多个文件打包为一个文件，在通过压缩命令来提高效率。</p>
<h4 id="gzip"><a href="#gzip" class="headerlink" title="gzip"></a><strong>gzip</strong></h4><p>运行 gzip 产生的文件后缀为 .gz，当你使用 gzip 压缩文件的时候，源文件会被压缩为 .gz 文件，就是说源文件不存在了（这和 Windows 上很不一样）。</p>
<ul>
<li>gzip<ul>
<li>-c，<strong>压缩，</strong>并把压缩的数据输出到屏幕上。<strong>可以配合数据重定向到压缩文件并保留源文件。</strong></li>
<li>-d，<strong>解压缩</strong></li>
<li>-t，检验压缩的一致性。-t filename1 filename2</li>
<li>-v，显示压缩信息</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gzip -c mytxt.txt &gt; mytxt.gz</span><br></pre></td></tr></table></figure>

<p>cat，less，more 读取未压缩的纯文本文件，对应的可以使用 zcat，zless，zmore 读取。还有 zgrep，等等。</p>
<h4 id="bzip2，xz"><a href="#bzip2，xz" class="headerlink" title="bzip2，xz"></a>bzip2，xz</h4><p>用法是 gzip 大致相同，生成的后缀名为 .bz2，且 bzip2 的压缩率比较高，但是花费的时间可能会更多一些。</p>
<p>xz 生成的压缩文件后缀名 .xz，压缩率更高，时间可能更久些。</p>
<h3 id="打包命令：tar"><a href="#打包命令：tar" class="headerlink" title="打包命令：tar"></a>打包命令：tar</h3><p>虽然 gzip 也可以针对目录使用，添加 -r 选项即可，不过作用是<strong>对目录中的文件分别进行压缩。</strong>这时候可以用 tar 命令将多个文件打包，再进行压缩。</p>
<ul>
<li>tar（-c,t,x 。-z,j,J 不同时出现在一个命令行中）<ul>
<li>-c，建立打包文件(tar 文件？)，可搭配 -v</li>
<li>-t，查看打包文件(tar 文件？) 中含有哪些文件名</li>
<li>-x，解包或者解压缩文件，可以搭配 -C ，把文件解压到特定的目录<ul>
<li>-z，通过 gzip 支持压缩/解压缩，最好把后缀命名成 .tar.gz</li>
<li>-j，通过 bzip2 支持压缩/解压缩，.tar.bz2</li>
<li>-J，通过 xz 支持压缩解压缩，.tar.xz<ul>
<li>-v，过程中显示正在处理的文件名</li>
<li>-f，后紧跟处理的文件名<ul>
<li>-p</li>
<li>-P</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="实战："><a href="#实战：" class="headerlink" title="实战："></a><strong>实战</strong>：</h4><ul>
<li>打包压缩：tar -zcv -f filename.tar.gz 要被压缩的文件或目录名</li>
<li>查询：tar -ztv -f filename.tar.gz</li>
<li>查询：tar -jtv -f filename.tar.bz2<ul>
<li>234ASD在：tar -xjv -f filename.tar.xz -C 指定的在哪个目录解压</li>
</ul>
</li>
</ul>
<h3 id="其他常见的压缩和备份工具：dd，cpio"><a href="#其他常见的压缩和备份工具：dd，cpio" class="headerlink" title="其他常见的压缩和备份工具：dd，cpio"></a>其他常见的压缩和备份工具：dd，cpio</h3><h2 id="vim-和-Shell"><a href="#vim-和-Shell" class="headerlink" title="vim 和 Shell"></a>vim 和 Shell</h2><h3 id="vim-的缓存，恢复和重新打开时的警告信息"><a href="#vim-的缓存，恢复和重新打开时的警告信息" class="headerlink" title="vim 的缓存，恢复和重新打开时的警告信息"></a>vim 的缓存，恢复和重新打开时的警告信息</h3><p>使用 vim 编辑文件时，vim 会在编辑文件的同个目录下建立一个 .<strong>原文件名.swp</strong>的文件保存你对原文件的操作记录。这样可以在意外的情况下恢复你上次可能未保存编辑的操作。</p>
<p>因为 vim 被异常结束，导致交换文件没有按照正常流程结束，所以交换文件会保留下来。</p>
<p><img src="/2021/03/17/niao-ge-linux-dishes-study-record/image(3)-2561646.png" alt="image(3)"></p>
<p>当你重新打开文件的时候，会提示你存在交换文件，你可以选择最后一行提供的 6 种操作。</p>
<p><img src="/2021/03/17/niao-ge-linux-dishes-study-record/image(4).png" alt="image(4)"></p>
<ul>
<li>E，不加载交换文件的内容直接编辑。</li>
<li>R，从交换文件恢复操作，但是交换文件还是存在目录中，可以手动删除避免每次打开出现类似提示。</li>
</ul>
<h3 id="数据重定向"><a href="#数据重定向" class="headerlink" title="数据重定向"></a>数据重定向</h3><p>一般执行一个命令的时候，从文件读取数据，通过标准输出/标准错误输出到屏幕中。命令正确执行通过标准输出，错误通过标准错误输出。且有对应的代码表示：</p>
<ul>
<li>1，默认表示标准输出。</li>
<li>2，表示标准错误输出。</li>
<li>0，默认表示标准输入。</li>
</ul>
<p><img src="/2021/03/17/niao-ge-linux-dishes-study-record/image(5).png" alt="image(5)"></p>
<p>有了对应的代码后，我们可以通过对应的信息将本应该<strong>输出</strong>到屏幕中的内容重定向（&gt;,&gt;&gt;）到文件中。</p>
<p>实战：将正确和错误结果分别重定向到不同文件中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ find /ect/test -name fhx.txt &gt; writePut 2&gt; wrongPut</span><br></pre></td></tr></table></figure>

<p>那如何将正确和错误的结果输入到同一个文件中？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ find /etc/test -name fhx.txt &gt; list 2&gt; list</span><br></pre></td></tr></table></figure>

<p>上面的做法理论上是对的，但是 list 文件可能会很混乱，因为无法保证正确和错误按照顺序写到文件中。应该这样做：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ find /etc/test -name fhx.txt &gt; list 2&gt;&amp;1</span><br></pre></td></tr></table></figure>

<p><strong>标准输入 “&lt;”，就是将原本本该由键盘输入获得的内容改为从文件来获取</strong>。</p>
<p>“&lt;&lt;”，表示进行结束操作的输入字符。&lt;&lt; “stttop”,从键盘获得了 stttop 输入后就会停止输入操作。</p>
<h2 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h2><p>一个<strong>程序被加载到内存</strong>中运行，在<strong>内存中</strong>的那部分数据就被称为一个进程。在 Linux 中，所有的东西都被视为文件，但我们执行一个命令的时候，其实就是在运行其中的某一个文件。</p>
<p>比如我们执行 bash 命令，其实是将 /bin/bash 这个文件加载到内存运行。这部分数据就称为为一个进程，<strong>Linux 会为其分配一个 PID（process id）,同时根据执行该进程的用户的相关属性，赋予该进程一组相关的权限设置（UID/GID）。</strong></p>
<p>执行 bash 命令后，相当于为用户新建了一个交互的 shell，我们在这个 shell 下执行其他命令时，产生的新进程其实是衍生自 bash 命令产生的进程。<strong>由一个进程衍生出来的其他进程，在一般状态下会沿用父进程的相关权限属性。</strong>可以执行 ps -l，观察 PPID（parent PID）了解进程的父进程。</p>
<p>Linux 的程序调用流程通常是 fork and exec，由父进程复制一个完全相同的子进程（PID 不同），然后 exec 执行实际要执行的进程。</p>
<h3 id="任务管理：-amp-（后台执行），ctrl-z（后台暂停），fg，bg"><a href="#任务管理：-amp-（后台执行），ctrl-z（后台暂停），fg，bg" class="headerlink" title="任务管理：&amp;（后台执行），ctrl+z（后台暂停），fg，bg"></a>任务管理：&amp;（后台执行），ctrl+z（后台暂停），fg，bg</h3><ul>
<li><strong>&amp;<strong>，在你要执行的命令后面添加 <strong>&amp;</strong>,表示你将该命令放到</strong>后台中执行</strong>。</li>
<li>ctrl + z，将当前的命令放到<strong>后台中暂停</strong>。（ctrl + c 是直接强制中断执行）</li>
<li>jobs，查看后台的状态。<ul>
<li>-l，同时列出 pid</li>
<li>输出结果中，[1][2].. 代表任务编号。**+ 号<strong>则表示最近那个被放到后台的任务，</strong>- 号**表示最近第二个被放到后台的任务。其他则不显示。</li>
</ul>
</li>
</ul>
<p><img src="/2021/03/17/niao-ge-linux-dishes-study-record/image(6).png" alt="image(6)"></p>
<ul>
<li>fg，（foreground），将后台任务取出到前台运行，不加参数默认取 + 号的那个任务。<ul>
<li>fg  jobNumber，取出对应编号的任务到前台执行。</li>
</ul>
</li>
<li>bg，将任何在后台中任务的状态变为“<strong>后台中执行</strong>”。用法和 fg 类似。</li>
</ul>
<p><strong>连接终端的个人 bash 的后台和整个系统的后台是两个概念</strong>。<strong>你在某个特定的 bash 下将任务放到后台运行，当你与主机退出连接的时候，该后台任务会中断，而不是你想的那样会一直运行。想要在整个主机中运行后台任务的话可以使用 nohup 命令。</strong></p>
<h3 id="进程管理"><a href="#进程管理" class="headerlink" title="进程管理"></a>进程管理</h3><p>同样的进程查看也是，当你连接主机，登录到一个 bash 下之后你执行的命令产生的子进程一般只和该 bash 下的父进程有关。</p>
<ul>
<li>你可以使用 <strong>ps -l</strong> 查看只和自己的 bash 有关的进程。</li>
</ul>
<p><img src="/2021/03/17/niao-ge-linux-dishes-study-record/image(7).png" alt="image(7)"></p>
<pre><code>* F：process flags，进程标识。用来说明进程的权限。
* S：STAT。
    * R，Running
    * S，Sleep。该进程处于睡眠状态（idle），但可以被唤醒（signal）
    * D：不可被唤醒的睡眠状态。可能在等待 I/O。
    * T：Stop。可能被手动暂停。
    * Z：Zombie。进程终止，但是无法被清出内存。
* PRI/NI，优先级，越小优先级越高。
</code></pre>
<ul>
<li><p>使用 <strong>ps aux</strong> 查看整个系统的进程。</p>
</li>
<li><p><strong>top。top</strong>执行后，会处于动态查看系统状态的界面，如下图。</p>
<p><img src="/2021/03/17/niao-ge-linux-dishes-study-record/image(8).png" alt="image(8)"></p>
<ul>
<li>第 3 行中的 wa 指的是系统 I/O 的 wait，平时可以多注意这一项。</li>
<li>最后一行的交换区（虚拟内存）用量也需要注意，用的越多说明系统内存可能告急。<ul>
<li>-d 秒数，top 更新的频率，默认 5s。</li>
<li>-b，按照批次输出 top 结果，可以配合 -n。</li>
<li>-n 次数，执行几次 top 命令的结果。</li>
<li>-p pid，只看特定 pid 的执行结果。</li>
</ul>
</li>
</ul>
</li>
<li><p>free，查看内存信息，-h，更可读的方式</p>
</li>
</ul>
<h3 id="-2"><a href="#-2" class="headerlink" title></a></h3>]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Project Reactor 响应式编程分享</title>
    <url>/2021/06/27/reactor-project/</url>
    <content><![CDATA[<h1 id="Reactive-Programming"><a href="#Reactive-Programming" class="headerlink" title="Reactive Programming"></a>Reactive Programming</h1><p>在计算机领域中，响应式编程是一种面向<strong>数据流和变化</strong>传播的编程范式。这意味着可以在编程语言中很方便地表达静态或动态的数据流，而相关的计算模型会自动将变化的值通过数据流进行传播。<span id="more"></span></p>
<p>举个例子来说，对于表达式 a = b + c，a 的计算结果由 b 和 c 的值确定下来后，之后 b 或 c 的值改变不会影响到 a 的值。如果是套用响应式的概念，对于 a = b + c，a 的值会随着数据流的变化实时发生变化，也就是说 b 和 c 的值改变，会影响表达式的结果 a。</p>
<p>之后，从微软在 .NET 上实现 Rreactive Extensions（Rx）库，随之 JVM 也出现了 RxJava 实现了响应式的编程范式那一套东西。</p>
<p>如今，Java 已经将响应式相关的内容纳入规范（<strong>Reactive Streams</strong>），定义了响应式库的接口的交互规则。</p>
<blockquote>
<p>The specification developed with the intent of future inclusion in the official Java standard library, if proven successful and adopted by enough libraries and vendors.<br>Reactive Streams were proposed to become part of <a href="https://en.wikipedia.org/wiki/Java_(software_platform)">Java</a> 9 by <a href="https://en.wikipedia.org/wiki/Doug_Lea">Doug Lea</a>, leader of <a href="https://en.wikipedia.org/wiki/JSR_166">JSR 166</a><a href="https://en.wikipedia.org/wiki/Reactive_Streams#cite_note-7">[7]</a> as a new Flow class<a href="https://en.wikipedia.org/wiki/Reactive_Streams#cite_note-8">[8]</a> that would include the interfaces currently provided by Reactive Streams.<a href="https://en.wikipedia.org/wiki/Reactive_Streams#cite_note-infoq-4">[4]</a><a href="https://en.wikipedia.org/wiki/Reactive_Streams#cite_note-jep266-9">[9]</a> After a successful 1.0 release of Reactive Streams and growing adoption, the proposal was accepted and Reactive Streams was included in <a href="https://en.wikipedia.org/wiki/Java_(software_platform)">JDK9</a> via the <a href="https://en.wikipedia.org/wiki/JDK_Enhancement_Proposal">JEP</a>-266.<a href="https://en.wikipedia.org/wiki/Reactive_Streams#cite_note-jep266-9">[9]</a></p>
<blockquote>
<p>– <a href="https://en.wikipedia.org/wiki/Reactive_Streams">https://en.wikipedia.org/wiki/Reactive_Streams</a></p>
</blockquote>
</blockquote>
<h2 id="响应式编程的优势"><a href="#响应式编程的优势" class="headerlink" title="响应式编程的优势"></a>响应式编程的优势</h2><p>比如常见的 Web 服务，在传统的阻塞编程模型里，每个请求都需要一个线程去处理，请求过程可能需要调用数据库 I/O，远程调用服务等等，都需要线程阻塞等待结果返回。如果是非阻塞的编程模型，请求调用某些服务后，可以保存相关的信息后直接返回，线程继续处理其他新的请求。线程无需一直阻塞在那些耗时的操作上，而是等那些耗时的操作有结果后主动通知线程就绪。这样做的好处是线程不必频繁阻塞，可以利用少量的线程处理大量的请求。</p>
<h2 id="响应式编程为何不够普及？"><a href="#响应式编程为何不够普及？" class="headerlink" title="响应式编程为何不够普及？"></a>响应式编程为何不够普及？</h2><ul>
<li>基础服务不够普及。前面提到的响应式编程的优势在于调用某些 API 后可以直接返回，但是就目前来说，响应式的 API 普及度远远不够。比如 JDBC，没有提供非阻塞 API ，线程调用后需要等待响应，响应式的优势难以体现（目前也出现了一些响应式的数据库客户端 API，比如 R2DBC，但都不普及）。</li>
<li>应用学习的成本问题。</li>
</ul>
<h1 id="Project-Reactor"><a href="#Project-Reactor" class="headerlink" title="Project Reactor"></a>Project Reactor</h1><p>Project Reactor 基于 Reactive Stream 规范，提供了可以构造非阻塞（异步）应用的响应库。</p>
<h2 id="非阻塞（异步）"><a href="#非阻塞（异步）" class="headerlink" title="非阻塞（异步）"></a>非阻塞（异步）</h2><p>Java 对于异步编程也提供了支持，我们可以为方法传入一个 <strong>Callable</strong>参数或者立即返回一个 <strong>Future<T></T></strong> ，当前线程则不用一直等待直到有结果返回，而是当有结果可以返回的时候再获取。相比于同步的阻塞调用，线程不用阻塞等待结果返回，可以先服务其他请求，在结果返回后再去获取。</p>
<h2 id="Reactor-与-Callback、Future"><a href="#Reactor-与-Callback、Future" class="headerlink" title="Reactor 与 Callback、Future"></a>Reactor 与 Callback、Future<T></T></h2><p>假设我们要异步实现这样一个服务：通过 userService 获取用户前五个点赞列表（如果有的话通过 favoriteService 获取详细的点赞信息），如果没有点赞列表则调用 suggestionService 获取推荐列表。</p>
<h3 id="回调写法"><a href="#回调写法" class="headerlink" title="回调写法"></a>回调写法</h3><p><img src="/2021/06/27/reactor-project/image(11).png" alt="image(11)"></p>
<p>可以看到每个服务调用都需要 onSuccess，onError 处理来处理服务调用的问题，调用正确时，调用其他服务时又需要 Callback 代码。</p>
<h3 id="使用-Reactor"><a href="#使用-Reactor" class="headerlink" title="使用 Reactor"></a>使用 Reactor</h3><p><img src="/2021/06/27/reactor-project/image(12).png" alt="image(12)"></p>
<ol>
<li>根据 userId 获取 Favorites 列表</li>
<li>从 Favorites 列表获取对应的 Detail 信息</li>
<li>如果 Favorites 是 empty 的话，改成获取推荐列表</li>
<li>只需要 5 个信息</li>
<li>通过 UI thread 处理数据</li>
<li>最后，通过 subscribe 触发数据流，然后通过 show 方法展示数据。</li>
</ol>
<p>Reactor 提供了需许多易用的操作，可以应对复杂的业务。</p>
<h2 id="Reactor-一些核心的概念"><a href="#Reactor-一些核心的概念" class="headerlink" title="Reactor 一些核心的概念"></a>Reactor 一些核心的概念</h2><h3 id="Publisher-Subscriber"><a href="#Publisher-Subscriber" class="headerlink" title="Publisher-Subscriber"></a>Publisher-Subscriber</h3><p>Publisher 负责生产数据；但是默认情况下 Publisher 不会做任何事情，直到有 Subscriber 订阅了它，Publisher 被订阅后，就会主动向 Subscriber <strong>push</strong> 数据。</p>
<h3 id="Nothing-Happens-Until-You-subscribe"><a href="#Nothing-Happens-Until-You-subscribe" class="headerlink" title="Nothing Happens Until You subscribe()"></a>Nothing Happens Until You subscribe()</h3><p>定义了 Publisher，以及一系列对数据流的操作后，默认地数据还没有被进行任何操作。只有调用了 subscribe 后，才会触发数据流开始操作。</p>
<h3 id="Flux-0…N-，Mono-0…1"><a href="#Flux-0…N-，Mono-0…1" class="headerlink" title="Flux - [0…N]，Mono - [0…1]"></a>Flux - [0…N]，Mono - [0…1]</h3><p>Reactor 中，Flux 和 Mono 就是标准的 Publiser<T> 。Flux 的含义是可能有 0 到 N 个数据的流。Mono 则表示至多只有一个。</T></p>
<h3 id="Backpressure"><a href="#Backpressure" class="headerlink" title="Backpressure"></a>Backpressure</h3><p>当 Publisher 生产的数据 Subscriber 来不及消费的时候，数据过多得积压可能发生错误，比如内存占用变大等等。Reactor 可以通过订阅者的 onRequest,onCancle 来指定需要处理的数据量避免 Backpressure 问题。</p>
<h2 id="运用-Reactor"><a href="#运用-Reactor" class="headerlink" title="运用 Reactor"></a>运用 Reactor</h2><h3 id="核心概念的简单示例"><a href="#核心概念的简单示例" class="headerlink" title="核心概念的简单示例"></a>核心概念的简单示例</h3><p><img src="/2021/06/27/reactor-project/image(13).png" alt="image(13)"></p>
<ul>
<li><p>Flux.range 生成数据流</p>
</li>
<li><h3 id="R2DBC、WebFlux"><a href="#R2DBC、WebFlux" class="headerlink" title="R2DBC、WebFlux"></a>R2DBC、WebFlux</h3></li>
</ul>
<p><img src="/2021/06/27/reactor-project/image(14).png" alt="image(14)"></p>
<p><img src="/2021/06/27/reactor-project/image(15).png" alt="image(15)"></p>
<h1 id="debug"><a href="#debug" class="headerlink" title="debug"></a>debug</h1><h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><ul>
<li><a href="https://projectreactor.io/docs/core/3.2.2.RELEASE/reference/index.html#about-doc">https://projectreactor.io/docs/core/3.2.2.RELEASE/reference/index.html#about-doc</a></li>
<li><a href="https://projectreactor.io/">https://projectreactor.io/</a></li>
<li><a href="https://wiki.jikexueyuan.com/project/android-weekly/issue-145/introduction-to-RP.html">https://wiki.jikexueyuan.com/project/android-weekly/issue-145/introduction-to-RP.html</a></li>
<li><a href="https://docs.spring.io/spring-framework/docs/current/reference/html/web-reactive.html#spring-webflux">https://docs.spring.io/spring-framework/docs/current/reference/html/web-reactive.html#spring-webflux</a></li>
<li><a href="https://r2dbc.io/">https://r2dbc.io/</a></li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>reactive programming</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 学习记录</title>
    <url>/2021/04/01/redis-gee-study-record/</url>
    <content><![CDATA[<h1 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h1><h2 id="基础数据类型的用法"><a href="#基础数据类型的用法" class="headerlink" title="基础数据类型的用法"></a>基础数据类型的用法</h2><p>在学习操作具体数据类型前，先来学习可以应用在所有 key 上的常用命令。多多利用 Tab 键补全可以使用的 Redis 命令。<span id="more"></span></p>
<blockquote>
<p><a href="https://redis.io/commands#generic">https://redis.io/commands#generic</a></p>
</blockquote>
<ul>
<li>help [command]。 对 [command] 的简单介绍</li>
<li>TTL [key]。seconds - 过期时间，1 - key 存在且没有过期时间，2 - key 不存在</li>
<li>expire [key] [seconds]，设置过期时间</li>
<li>dbsize，key 的个数</li>
<li>exists [key]，0 不存在，1 存在</li>
<li>type [key]</li>
</ul>
<h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><h4 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h4><blockquote>
<p><a href="https://redis.io/commands#string">https://redis.io/commands#string</a></p>
</blockquote>
<ol>
<li><strong>新增、更新</strong></li>
</ol>
<ul>
<li>set key value [expiration EX seconds|PX milliseconds] [NX|XX]<ul>
<li>NX，不存在 key 才会执行 set - 可以避免覆盖，用于新增</li>
<li>XX，只存在 key 才会 set - 更新场景</li>
</ul>
</li>
<li>setex key seconds value，同时设置 expire time</li>
<li>setnx key value，和 NX 选项一样的作用，用于新增</li>
<li>mset key value [key value …]，批量设置键值对</li>
</ul>
<ol start="2">
<li><strong>获取</strong></li>
</ol>
<ul>
<li>get key</li>
<li>mget key [key …]</li>
</ul>
<ol start="3">
<li><strong>数字计算</strong></li>
</ol>
<p>如果 key 的 value 是整数，则可以进行自增减操作。</p>
<ul>
<li>incr，decr [key]。如果 key 不存在，先设置 key 的值为 0，然后 +- 1</li>
<li>incrby，decrby [key] [num]。自增减指定数字大小。</li>
</ul>
<p>利用批量设置 mset，批量获取 mget 代替多次重复的 set 和 get 操作可以很好的节省 C/S 之间的多次网络 I/O 成本，和多次等待命令执行的时间，因为 Redis 采用单线程串行执行命令。</p>
<p>进行批量操作的时候要注意同时操作的个数，避免阻塞线程。</p>
<h4 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h4><ul>
<li>访问限制。在一定时间内限制频繁访问，配置 expire time 并通过 incr(key) 小于某个数值进行限制。比如发送验证码短信的 api，限制一分钟内获取多少次次，将 phoneNumber 作为 key set 进去，设置过期时间，incr 限制次数。</li>
<li>字符串有很多适用场景，结合提供的命令进行使用。比如 setnx ，incr。</li>
</ul>
<h3 id="哈希"><a href="#哈希" class="headerlink" title="哈希"></a>哈希</h3><p>第一部分讨论的数据类型指的是存储的 value 的数据类型。Redis 本身会维护一张全局哈希表，用来快速获取 key 对应的 value 。注意不要把这两个 hash 搞混了。</p>
<h4 id="命令-1"><a href="#命令-1" class="headerlink" title="命令"></a>命令</h4><p>hash 类型的 value 存储多个<strong>field-value</strong> 映射。</p>
<ol>
<li><strong>设置</strong></li>
</ol>
<ul>
<li>hset key field value。设置新的 field-value</li>
<li>hsetnx ….</li>
<li>hmset key field value [field value …]。批量设置 field 值</li>
</ul>
<ol start="2">
<li><strong>获取</strong></li>
</ol>
<ul>
<li>hkeys key，获取 key 的所有 field </li>
<li>hget key field，获取 key 的特定的 field 的 value</li>
<li>hmget key field [field …]，批量获取 field</li>
<li>hgetall key，获取 key 的所有 field-value</li>
<li>hlen key，获取 key 的 field 的个数</li>
<li>hstrlen key field，获取 field 的 value 的长度</li>
</ul>
<ol start="3">
<li><strong>删除 key 的 field</strong></li>
</ol>
<ul>
<li>hdel key field [field …]。针对 field 。想要删除整个 key-value 直接用 del [key] 即可</li>
<li>hexists key field。判断 key 是否存在对应的 field 值</li>
</ul>
<ol start="4">
<li><strong>数字计算</strong></li>
</ol>
<ul>
<li>hincrby key field increment</li>
<li>hincrbyfloat key field increment。hincrbyfloat 可以对 22 进行操作</li>
</ul>
<p>对 hash 的<strong>操作主要针对的类型是 hash 类型里的 field 域 ，</strong>会帮助我们更好理解。</p>
<h4 id="使用场景-1"><a href="#使用场景-1" class="headerlink" title="使用场景"></a>使用场景</h4><p>哈希类型用来存储类似关系表展示的信息看起来相比于 字符串 类型更加直观。</p>
<p><img src="/2021/04/01/redis-gee-study-record/image.png" alt="image"></p>
<h3 id="list"><a href="#list" class="headerlink" title="list"></a>list</h3><p>list 类型可以存储多个有序的字符串。支持双端的 push 和 pop，且因为是有序存储，可以通过索引下标获取元素。</p>
<h4 id="命令-2"><a href="#命令-2" class="headerlink" title="命令"></a>命令</h4><ol>
<li><strong>插入，特定位置插入，修改特定位置的值</strong></li>
</ol>
<ul>
<li>rpush/lpush key value [value …]，从右往左/从左往右依次插入元素</li>
<li>linsert key BEFORE|AFTER pivot value，在 list 中的 pivot 元素前/后插入 value</li>
<li>lset key index value，修改 index 索引的值，变成 value</li>
</ul>
<ol start="2">
<li><strong>获取：通过 index 获取特定元素、获取索引范围内的元素</strong></li>
</ol>
<ul>
<li>lrange key start stop。获取的元素会包含 stop 索引在的元素（如果有的话）<ul>
<li>lrange key 0 -1 ，获取从左到右所有元素</li>
</ul>
</li>
<li>lindex key index，获取指定 index 的元素</li>
<li>llen key，获取 list 的长度</li>
<li>list 的索引下标从左到右是 0 和 length -1。从右到左是 -1 到 -N？？？？？？？</li>
</ul>
<ol start="3">
<li><strong>删除：头尾弹出单个元素、指定保留索引访问内的元素、删除指定的 value 元素</strong></li>
</ol>
<ul>
<li>lpop/rpop key，从list 左/右弹出一个元素</li>
<li>lrem key count value。从 list 中删除指定的 value 值，如果 count = 0，删除所有匹配的 value；如果count &gt; 0，从左到右至多删除 count 个匹配的 value。如果 count &lt; 0 ，从右到左至多删除 -count 个匹配的 value</li>
<li>ltrim key start end，截取 list，保留范围内内的元素。同样的 end 索引值的位置会被包含（如果有的话）</li>
<li>blpop/brpop key [key …] timeout。<strong>阻塞 timeout 秒，直到 pop 出数据</strong></li>
</ul>
<h4 id="运用场景"><a href="#运用场景" class="headerlink" title="运用场景"></a>运用场景</h4><p><img src="/2021/04/01/redis-gee-study-record/image(1).png" alt="image(1)"></p>
<h3 id="set"><a href="#set" class="headerlink" title="set"></a>set</h3><p>set 也用于保存多个字符串元素。set 和 list 不同之处在于 set 是存储的元素是<strong>无序</strong>的且<strong>不允许有重复的元素。</strong></p>
<p>set 除了常规的 crud 外，还支持计算集合键的 交集，并集，差集等。利用这些计算可以很好的运用在实际问题中。</p>
<h4 id="命令-3"><a href="#命令-3" class="headerlink" title="命令"></a>命令</h4><ol>
<li><strong>操作集合内部元素的命令</strong></li>
</ol>
<ul>
<li>sadd key member [member …]，添加元素</li>
<li><ul>
<li>spop key [count]，随机从 set 中<strong>弹出</strong>指定 count 个元素</li>
</ul>
</li>
<li>srem key member [member …]，删除指定的元素</li>
<li><ul>
<li>srandmember key [count]，无 count 则默认为 1。从 set 中<strong>随机返回</strong> count 个元素</li>
</ul>
</li>
<li>smembers key，返回 set 中的所有元素</li>
<li><ul>
<li>scard key，计算 set 中的元素个数。O(1)</li>
</ul>
</li>
<li>sismember key element，判断元素是否存在</li>
</ul>
<ol start="2">
<li><strong>集合之间的操作</strong></li>
</ol>
<ul>
<li>sinter key [key …]，多个集合之间的交集</li>
<li>sunion key [key …]，多个集合之间的并集</li>
<li>sdiff key [key …]，集合之间的差集</li>
<li><ul>
<li>sinterstore destination key [key …]，将求交集结果直接存储在 destination 中。destination 也是 set 类型。</li>
</ul>
</li>
<li>sunionstore，sdiffstore 也是一样的。</li>
</ul>
<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>标签场景。</p>
<p><img src="/2021/04/01/redis-gee-study-record/image(2).png" alt="image(2)"></p>
<h3 id="sorted-set"><a href="#sorted-set" class="headerlink" title="sorted set"></a>sorted set</h3><p>sorted set 保留了 set 不能存储重复元素的特点，新增了 score 属性，作为<strong>排序</strong>的依据。也就是说 sorted set 是有序的 set，但是内部存储元素的方式不同，sorted set 通过 score-member 存储。注意 sorted set 中不能重复的元素指的是 score-member 中的 <strong>member</strong>，score 可以一样。</p>
<h4 id="命令-4"><a href="#命令-4" class="headerlink" title="命令"></a>命令</h4><ol>
<li><strong>新增</strong></li>
</ol>
<ul>
<li>zadd key [NX|XX] [CH] [INCR] score member [score member …]<ul>
<li>nx，xx 和之前提到的作用一样，nx 保证 member 不存在才能设置，用于新增。xx 保证 member 存在才能设置，用于更新</li>
<li>ch，返回操作后发生变化 score-member 的个数</li>
<li>incr [num]，将 member 对应的 score + num</li>
</ul>
</li>
<li>zcard key，计算 key 中 member 的个数</li>
<li>zrem key member [member …]，删除 key 中对应的 member</li>
<li>zscore key member，获取 member 的 score</li>
<li></li>
</ul>
<ol start="2">
<li><strong>根据 score 操作</strong></li>
</ol>
<ul>
<li><a href="https://redis.io/commands/zrangebyscore">zrangebyscore</a> key min max [WITHSCORES] [LIMIT offset count]，返回<strong>指定 score</strong>在 min-max 范围内（默认都包含）的 member。<ul>
<li>LIMIT 的 offset 限制 rank 的起始位置（0 表示第 1 位），count 限制返回的个数。</li>
<li>zrangebyscore z:rank (10 +inf limit 2 5<ul>
<li>(10 表示 &gt; 10，如果不加 “(“ 默认表示 &gt;= 10，+inf 表示无穷大，-inf 表示无穷小。 </li>
<li>limit 2 5 表示返回从排名 第 2+1 位开始，往下 5 个输出结果。</li>
</ul>
</li>
</ul>
</li>
<li>zcount key min max，返回指定 score 范围内的 <strong>member 个数</strong></li>
<li><strong>fd</strong></li>
<li>zremrangebyscore key min max，删除指定 score 范围内的 member</li>
</ul>
<ol start="3">
<li><strong>根据排名操作</strong></li>
</ol>
<ul>
<li>zrank key member，<strong>从低到高</strong>计算<strong>特定 member 的排名</strong></li>
<li>zrevrank key member，<strong>从高到低</strong>计算<strong>特定 member 的排名</strong></li>
<li>zrange key start stop [WITHSCORES]，返回<strong>指定排名访问内</strong>的<strong>member</strong><ul>
<li>start 从 0 开始，同样的 stop 会被包含（如果有的话）</li>
<li>withscores 同时返回 member 对应的 score</li>
</ul>
</li>
<li><ul>
<li>zremrangebyrank key start stop，删除指定排名内的 member</li>
</ul>
</li>
</ul>
<ol start="4">
<li><strong>集合之间的操作</strong></li>
</ol>
<ul>
<li>zinterstore destination numkeys key [key …] [WEIGHTS weight] [AGGREGATE SUM|MIN|MAX]</li>
</ul>
<h4 id="应用场景-1"><a href="#应用场景-1" class="headerlink" title="应用场景"></a>应用场景</h4><p>排行榜系统</p>
<p><img src="/2021/04/01/redis-gee-study-record/image(3).png" alt="image(3)"></p>
<h3 id="Bitmap"><a href="#Bitmap" class="headerlink" title="Bitmap"></a>Bitmap</h3><p>二值状态统计，比如签到未签到，打卡未打卡，用 0和1就可以表示。</p>
<p>Bitmap 底层是使用 String 类型，String 本身会保存为二进制的字节数组，字节数组的每个 bit 位就可以用来表示一个元素的二值状态。</p>
<h4 id="命令-5"><a href="#命令-5" class="headerlink" title="命令"></a>命令</h4><ul>
<li>SETBIT key offset 1/0，设置bit 数组偏移了 offset 位上的 bit 为 0/1。注意 offset 是从 0 开始的。</li>
<li>GETGIT key offset</li>
<li>BITCOUNT key</li>
<li>BITOP，可以对多个 Bitmap 的每个 bit 位进行 『与』或者『异或』操作。</li>
</ul>
<h3 id="HyperLogLog"><a href="#HyperLogLog" class="headerlink" title="HyperLogLog"></a>HyperLogLog</h3><p>HyperLogLog 是一种用于统计基数的数据集合类型，它的优势在于不管集合元素数量如何，<strong>计算基数</strong>所需的空间总是固定的。12KB 内存可以计算约 2^64 个元素的基数。</p>
<p>HyperLogLog 的统计规则是基于概率完成的，给出的统计结果有一定误差。</p>
<h4 id="命令-6"><a href="#命令-6" class="headerlink" title="命令"></a>命令</h4><ul>
<li>PFADD page1:uv user1 user2 user3</li>
<li>PFCOUNT page1:uv</li>
</ul>
<p><a href="https://time.geekbang.org/column/article/280680">12 | 有一亿个keys要统计，应该用哪种集合？ (geekbang.org)</a></p>
<h1 id="第二部分"><a href="#第二部分" class="headerlink" title="第二部分"></a>第二部分</h1><h2 id="慢查询分析"><a href="#慢查询分析" class="headerlink" title="慢查询分析"></a>慢查询分析</h2><blockquote>
<p><a href="https://github.com/funcrayon/redis.conf-zh_cn#slow-log%E6%85%A2%E6%97%A5%E5%BF%97">redis.conf 中文翻译</a></p>
</blockquote>
<p>慢查询分析只会统计命令真正执行时的时间，注意不包括命令的排队，传输时间等等。</p>
<ul>
<li> slowlog subcommand [argument]<ul>
<li>slowlog len，获取日志长度</li>
<li>slowlog get [n]，获取 n 条记录。没有 n 则是获取全部</li>
</ul>
</li>
</ul>
<p>慢日志输出结果字段分析：</p>
<ol>
<li><ol>
<li>(integer) 2  // 唯一标识</li>
</ol>
<p>   2) (integer) 1603002171 // 时间戳</p>
</li>
</ol>
<p>    3) (integer) 22522 // 执行时间</p>
<p>    4) 1) “keys” // 执行命令</p>
<ol start="2">
<li>“*” // 参数</li>
</ol>
<p>    5) “127.0.0.1:33696”</p>
<p>    6) “”</p>
<h2 id="redis-benchmark"><a href="#redis-benchmark" class="headerlink" title="redis-benchmark"></a>redis-benchmark</h2><p>redis-benchmark 可以为 Redis 做简单的基准测试。</p>
<ul>
<li>redis-benchmark<ul>
<li>-c [num] 客户端并发数量，默认 50</li>
<li>-n [num] 客户端请求总量，默认 100000</li>
<li>-q，加上该选项则仅输出 requests per second 信息</li>
<li>-r [keyspacelen]，插入随机键进行测试， keyspacelen 只用来定义</li>
<li>-P <numreq>，每个请求 pipeline 的数据量，默认 1</numreq></li>
<li>–csv，结果按照 csv 格式输出</li>
</ul>
</li>
</ul>
<h2 id="Lua-与事务"><a href="#Lua-与事务" class="headerlink" title="Lua 与事务"></a>Lua 与事务</h2><h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><p>Redis 可以通过 multi 命令开启事务，<a href="https://redis.io/commands/exec">exec </a>命令表示事务结束或者 discard 命令取消事务。在 multi 命令和 exec 命令之间的命令会<strong>原子性并按照顺序执行</strong>。</p>
<blockquote>
<p><a href="https://redis.io/topics/transactions">https://redis.io/topics/transactions</a></p>
</blockquote>
<p>想要执行一组包含在 multi 和 exec 之间的命令时</p>
<ol>
<li>对于语法正确的命令（语法正确，但不一点能正确执行，比如想要 get 一个不存在的 key），都只会返回 QUEUED 表示排队待执行。最后使用 exec 执行事务时，</li>
<li>对于语法直接有错误的命令，直接会返回 error 相关的信息。最后使用 exec 执行事务时，会提示你 Transaction is discarded。说明整个事务内的命令都不会执行。</li>
</ol>
<p>在最后执行 exec 执行事务时，第一种情况<strong>最终会执行那些正确的命令</strong>；第二种情况最终会出现类似 Transaction is discarded 的提示信息，表示整个事务回滚了；</p>
<p>Redis 还提供一个 WATCH key 命令用于在 multi 之前来确保事务中的命令操作的 key 没有被修改过，事务才正确执行。</p>
<h3 id="Lua"><a href="#Lua" class="headerlink" title="Lua"></a>Lua</h3><p>待定</p>
<h1 id="第三部分"><a href="#第三部分" class="headerlink" title="第三部分"></a>第三部分</h1><h2 id="一，基本架构"><a href="#一，基本架构" class="headerlink" title="一，基本架构"></a>一，基本架构</h2><p>索引模块 - 数据库根据 key 定位对应的 value 位置。Redis 和 Memcached 采用哈希表作为<strong>索引的类型</strong>。</p>
<p>通常基于内存的键值数据库都会采用哈希表（随机访问的时间复杂度O(1)），这也和内存的硬件特性有关 - 提供高性能的随机访问。</p>
<p><strong>内存分配器。</strong>数据库中存储的键值对一般大小不一，不同的内存分配器面对这些情况有不同的效果。Redis 提供多种内存分配选项，之间的分配效率也不同。</p>
<p><img src="/2021/04/01/redis-gee-study-record/image(4).png" alt="image(4)"></p>
<h2 id="二、数据结构"><a href="#二、数据结构" class="headerlink" title="二、数据结构"></a>二、数据结构</h2><p>采用全局哈希表高效定位 key 对应的 value 的位置。哈希冲突影响性能后，采用渐进式 rehash 把数据拷贝，最后迁移至更大的哈希表中。</p>
<p>渐进式 rehash 过程中，服务器仍响应客户端请求，对于已拷贝的数据，如果被修改会如何处理？</p>
<p>比如索引 5 中的 entry （entry 中包含了 key，value 指针分别指向实际的键和值）链被拷贝到新的哈希表 2 中，后续的请求修改了索引 5 中的 value 值，需要重新去哈希表2中寻找并修改对应值吗？</p>
<p>答：《Redis 设计与实现》中具体说明了渐进式 rehash 的过程 - rehash 时，哈希表 1 某个索引中的数据“拷贝”至哈希表 2 后，哈希表 1 中对应的索引值会设置为 NULL。</p>
<p><strong>丰富的数据类型使用的底层数据结构种类。</strong></p>
<p><img src="/2021/04/01/redis-gee-study-record/image(5).png" alt="image(5)"></p>
<p>压缩列表类似数组，都是紧凑的数据结构，但压缩列表在表头固定有三个字段分别表示：列表长度，列表尾的偏移量和列表中的 entry 个数；表尾固定有一个 zlend 表示列表结束。（LPOP,RPUSH 等操作，只要 O(1) 的时间复杂度）</p>
<p>跳表就是添加多级索引的链表。O(logN)</p>
<h2 id="三、Redis-中的单线程指的是什么？"><a href="#三、Redis-中的单线程指的是什么？" class="headerlink" title="三、Redis 中的单线程指的是什么？"></a>三、Redis 中的单线程指的是什么？</h2><p>我们常说的 Redis 单线程，<strong>指的是 Redis 中的网络 IO 和键值对读写由单线程完成（对外提供的两个主要服务）</strong>。其他的诸如持久化，数据同步等是由其他线程完成的。</p>
<p><img src="/2021/04/01/redis-gee-study-record/image(6).png" alt="image(6)"></p>
<p>Redis 6.0，把网络 IO 相关的事情利用多线程处理了 -&gt; 客户端请求的读取和解析。</p>
<p>第 3 讲 和 第 39 讲</p>
<p><a href="https://redislabs.com/blog/diving-into-redis-6/">https://redislabs.com/blog/diving-into-redis-6/</a></p>
<h2 id="四、AOF"><a href="#四、AOF" class="headerlink" title="四、AOF"></a>四、AOF</h2><p>append only file</p>
<p>WAL - redo log，为什么传统数据库采用 wal 日志？</p>
<p>AOF <strong>采用后写</strong>，命令执行成功才写入日志，避免检查命令的开销，后写可以避免阻塞当前的命令，但是可能阻塞后续的命令，这个取决于将<strong>AOF 缓冲区同步到<strong><strong>aof</strong></strong>文件</strong>的三种策略（Always，Everysec，No）。注意 AOF 写入是在主线程进行的。</p>
<p><strong>AOF 重写</strong>，创建<strong>新的 AOF 文件</strong>，读取数据库中的所有键值对并记录。AOF 重写是由子进程 bgwriteaof 完成，避免阻塞主线程。<strong>子进程 Copy on Write</strong>，共享主线程的内存数据。子进程 fork 时是一定会阻塞主线程的。</p>
<p>关于 Copy-On-Write。Redis 调用 fork() 创建子进程，子进程会复制和父进程一样的 page table，这个过程是会阻塞的。子进程和父进程共享同一份内存实例，当父进程需要修改内存中的数据时，父进程会把新的数据或者修改后的数据写到新的物理内存地址中，并更新自己的 page table 映射。</p>
<ul>
<li><a href="https://www.geeksforgeeks.org/copy-on-write/">https://www.geeksforgeeks.org/copy-on-write/</a></li>
<li><a href="https://en.wikipedia.org/wiki/Copy-on-write">https://en.wikipedia.org/wiki/Copy-on-write</a></li>
<li><a href="https://en.wikipedia.org/wiki/Virtual_memory">https://en.wikipedia.org/wiki/Virtual_memory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fork_(system_call)">https://en.wikipedia.org/wiki/Fork_(system_call)</a></li>
<li><a href="https://linux.die.net/man/2/fork">https://linux.die.net/man/2/fork</a></li>
</ul>
<p><img src="/2021/04/01/redis-gee-study-record/image(7).png" alt="image(7)"></p>
<p><img src="/2021/04/01/redis-gee-study-record/image(8).png" alt="image(8)"></p>
<p>重放 AOF 文件时，也是单线程一个个重放恢复数据，会比较慢。</p>
<p>五、RDB</p>
<p>内存快照。RDB 记录数据库某个时刻的所有数据，不像 aof 记录的是操作，所以数据恢复速度很快，就是把 RDB 文件加载到内存中即可。</p>
<p>save，主线程中执行，会阻塞。bgsave，通过创建子进程用来生成 RDB 文件。</p>
<p>默认使用 bgsave 生成 RDB 文件，生成文件的过程中，主线程不被阻塞，仍正常处理读<strong>写</strong>请求。</p>
<p>Copy On Write，写时复制技术，子进程由主线程 fork 生成，共享主线程的所有内存数据。但主线程处理写请求时，内存中的数据在修改前会生成一个副本给子进程使用。这样在生成 RDB 文件时也不影响主线程处理写请求。<strong>写时复制，如果内存数据被修改，就要分配新的内存空间了，其余部分是共享的</strong>。</p>
<p><strong>Redis 4.0 提出混合使用 RDB 和 AOF 的方法。</strong>怎么实践，写一篇针对这个问题的实践文章。</p>
<p><img src="/2021/04/01/redis-gee-study-record/image(9).png" alt="image(9)"></p>
<p>redis 持久化方案官网 topic：</p>
<p><a href="https://redis.io/topics/persistence">https://redis.io/topics/persistence</a></p>
<p><a href="http://oldblog.antirez.com/post/redis-persistence-demystified.html">http://oldblog.antirez.com/post/redis-persistence-demystified.html</a></p>
<h2 id="六、主从同步"><a href="#六、主从同步" class="headerlink" title="六、主从同步"></a>六、主从同步</h2><p>第一次主从通信，主库生成 RDB 文件，传输给从库，从库清空本地数据后加载 RDB 文件，在此过程中，为了保存主从的数据一致，<strong>replication buffer</strong> 用来记录主库收到的写操作。最后等从库加载完 RDB 文件后把缓冲区的数据发给从库，此后维持一个长连接同步命令。</p>
<p>主从库之间的网络连接断开怎么办？增量复制 - 只记录传输断连期间主从收到的命令并同步给从库。</p>
<p>利用<strong>repl-backlog-buffer</strong>这一环形缓冲区进行。</p>
<h2 id="七、哨兵机制"><a href="#七、哨兵机制" class="headerlink" title="七、哨兵机制"></a>七、哨兵机制</h2><p>主库宕机，需要在集群中选一个新主库提供服务。在 Redis 集群中，Redis 中有一个特殊的进程用来监控、选择主库和通知。一般我们也称为哨兵机制。</p>
<h3 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h3><p><strong>哨兵会使用 PING 命令检测自己和 主、从库之间的网络连接情况，用来判断实例的状态。</strong>如果哨兵发现响应超时后，会将该实例先标记为 『主观下线』。</p>
<p>如果是从库的话影响还不大，如果是主库，哨兵判断『主观下线』后，还不能直接启动主从切换。<strong>因为哨兵也可能存在误判的情况，可能发生误判的原因可能是网络波动等原因。</strong></p>
<p>为了避免这个问题，Redis 中的哨兵也会采用多个实例组成的<strong>哨兵集群</strong>。</p>
<h3 id="选择新主库"><a href="#选择新主库" class="headerlink" title="选择新主库"></a>选择新主库</h3><p>先筛选掉一定不适合当主库的从库。例如此前网络状况不好的情况。筛选过后，按照优先级顺序进行三轮选择，如果有合适的从库那么选择新主库的工作就此结束。</p>
<ol>
<li>从库的优先级高的得分更高。可以通过 slave-priority 进行配置。如果有优先级一致的情况，按照下一步标准选择。</li>
<li>和旧主库同步程度最接近的从库得分高。使用到前面的 repl-backlog-buffer。其中从库偏移量最接近的选为新从库。如果偏移量也一致，进行下一轮。</li>
<li>从库 ID 最小的选为新主库。</li>
</ol>
<p>选出新主库后，通知从库和客户端重新连接。</p>
<h2 id="八、哨兵集群"><a href="#八、哨兵集群" class="headerlink" title="八、哨兵集群"></a>八、哨兵集群</h2><p>哨兵实例判断『主观下线』后，会给其他哨兵实例发烧 is-master-down-by-addr，其他实例就会根据自己和主库的连接情况，响应 Y 或者 N。哨兵只要获得了配置的赞成票数（包括了自己的一票）后，就会可以标记主库『客观下线』，意味着要选新主库并切换了。配置的赞成票数通过哨兵配置文件中的 quorum 配置项决定。</p>
<p>之后，哨兵集群还要选举来执行主从切换的哨兵。哨兵成功标记主库『客观下线』后，这个哨兵就可以继续给其他哨兵发送命令，表示希望由自己的执行主从切换操作，让其他哨兵进行投票。</p>
<p>任何一个想成为 Leader 的哨兵，需要满足两个条件：</p>
<ol>
<li>拿到半数以上的赞成票</li>
<li>拿到的票数 &gt;= 配置的 quorum 值</li>
</ol>
<h2 id="九、切片集群"><a href="#九、切片集群" class="headerlink" title="九、切片集群"></a>九、切片集群</h2><p>当 Redis 保存的数据量大时，使用单个实例来保存会有几个问题：</p>
<ol>
<li>持久化时间。使用 RDB 持久化的话 fork 阻塞时间随着 Redis 数据量变大。</li>
<li>Redis 发生故障后，恢复时间长，可用性也有影响。</li>
</ol>
<p>通用的解决方案：切片集群。</p>
<p>Redis 从 3.0 开始，官方提供了一个 Redis Cluster 的解决方案用来实现切片集群：按照一定的规则将大量的数据划分成多份，用多个实例分别保存数据。</p>
<p>使用切片集群有两个大问题需要解决：</p>
<ul>
<li>数据如何划分，分配给不同的实例。</li>
<li>客户端访问的时候如何找到想要访问的数据所在的实例。</li>
</ul>
<h3 id="数据和实例之间的映射"><a href="#数据和实例之间的映射" class="headerlink" title="数据和实例之间的映射"></a>数据和实例之间的映射</h3><p><strong>Redis Cluster 采用哈希槽来映射数据和实例之间的关系</strong>。一个切片集群一共有 16384 个哈希槽。</p>
<p>使用 cluster create 创建集群的时候，Redis 会把槽平均分配给实例；</p>
<p>也可以手动建立集群实例之间的关系，然后通过 cluster addslots 指定实例上的哈希槽个数。这样可以根据不同实例的情况分配资源。注意使用手动的方式分配哈希槽的时候一定要把槽都分配完，否则无法正常工作。</p>
<p>每个 key-value 会根据 key，按照 CRC16 算法计算一个 16bit的值，然后在哈希槽上取模，映射到对应的槽上。</p>
<h3 id="客户端访问时如何定位数据所在的实例？"><a href="#客户端访问时如何定位数据所在的实例？" class="headerlink" title="客户端访问时如何定位数据所在的实例？"></a>客户端访问时如何定位数据所在的实例？</h3><p>集群建立完成后，Redis 实例之间会把自己分配到的哈希槽信息发送给其他和它相连接的实例。</p>
<p>客户端和集群的实例建立连接后，实例会把哈希槽的分配信息发送给客户端，客户端吧哈希槽和实例对应的关系在本地缓存起来。<strong>客户端请求访问某个 key-value 时，先计算 key 所在的哈希槽，然后根据关系表向对应的实例发送访问请求。</strong></p>
<p>在集群中，实例的哈希槽的对应关系可能会因为新增实例等操作重新分配哈希槽。这时候客户端本地无法知道这些改变。当客户端根据缓存的信息向目标实例发送请求时，如果该实例上没有分配到被访问哈希槽，那个实例就会向客户端返回下面的命令，告诉所在数据新的访问地址。客户端连接到新的实例上发送请求即可。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GET hello:key</span><br><span class="line">(error) MOVED 13320 172.16.19.5:6379</span><br></pre></td></tr></table></figure>

<p>客户端收到 MOVED 命令同时会更新本地的哈希槽对应信息的缓存。<br>在实际场景中，客户端发送请求时，数据迁移工作可能正在进行。这种情况下，客户端就会收到 ASK 信息，表示正在迁移。之后客户端就需要先给新的实例发送 ASKING 命令，然后再发送一次操作请求。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GET hello:key</span><br><span class="line">(error) ASK 13320 172.16.19.5:6379</span><br></pre></td></tr></table></figure>

<p>客户端收到 ASK 命令不会更新本地缓存。</p>
<h2 id="Redis-实现分布式锁"><a href="#Redis-实现分布式锁" class="headerlink" title="Redis 实现分布式锁"></a>Redis 实现分布式锁</h2><p>在图中，客户端 A 和 C 同时请求加锁。因为 Redis 使用单线程处理请求，所以，即使客户端 A 和 C 同时把加锁请求发给了 Redis，Redis 也会串行处理它们的请求。</p>
<h3 id="单机实现几个重要的点："><a href="#单机实现几个重要的点：" class="headerlink" title="单机实现几个重要的点："></a><strong>单机</strong>实现几个重要的点：</h3><ul>
<li>SET NX PX。SETNX，key 不存在时才进行设置，如果 key 已存在不做操作。<ul>
<li>使用 Redis 提供的原子命令，在设置 key 的同时设置 expire 时间，避免客户端异常没有执行释放锁的操作，<strong>expire<strong><strong>的</strong></strong>时间需要评估比占用<strong><strong>锁的</strong></strong>时间长</strong>。NX 在没有这个 key 时进行设置，相当于加锁。</li>
<li>设置的 key-value，value 设置为可以识别加锁的客户端，释放锁的时候判断 key 的 value 是不是当前客户端，避免其他客户端误释放。</li>
</ul>
</li>
<li>释放锁需要 lua 脚本来保证原子性。通过 DEL 释放锁，在 DEL 前，获取 key 的 value 和传入的标识比较，相同时才进行 DEL 释放锁。</li>
</ul>
<p>不好评估，如果操作共享资源的时候过期了怎么办？</p>
<p>Redisson，看门狗，守护线程。在操作期间如果快要过期，自动设置 expire 时间进行续期。</p>
<p>单机有一个问题就实例故障的问题，且依靠集群是无法保证的。</p>
<h3 id="多个节点的实现方案-RedLock"><a href="#多个节点的实现方案-RedLock" class="headerlink" title="多个节点的实现方案 RedLock"></a>多个节点的实现方案 RedLock</h3><p>多个节点不是 Redis Cluster，而是多个独立的主库实例。官方推荐 5 个。</p>
<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ol>
<li><a href="https://mp.weixin.qq.com/s/qJK61ew0kCExvXrqb7-RSg">Redis 分布式锁的正确实现方式（ Java 版 ） (qq.com)</a></li>
<li><a href="https://xiaomi-info.github.io/2019/12/17/redis-distributed-lock/">分布式锁的实现之 redis 篇 | 小米信息部技术团队 (xiaomi-info.github.io)</a></li>
<li><a href="https://time.geekbang.org/column/article/301092">30 | 如何使用Redis实现分布式锁？ (geekbang.org)</a></li>
<li><a href="https://time.geekbang.org/column/article/5175">52 | 管理设计篇之“分布式锁” (geekbang.org)</a></li>
<li><a href="https://mp.weixin.qq.com/s/s8xjm1ZCKIoTGT3DCVA4aw">深度剖析：Redis分布式锁到底安全吗？看完这篇文章彻底懂了！ (qq.com)</a>，RedLock 使用</li>
<li><a href="https://juejin.cn/post/6854573212831842311">Redis——由分布式锁造成的重大事故 (juejin.cn)</a></li>
<li><a href="https://time.geekbang.org/column/article/350285">21 | 分布式锁：为什么基于etcd实现分布式锁比Redis锁更安全？ (geekbang.org)</a></li>
<li><a href="https://time.geekbang.org/column/article/125983">41 | 如何设计更优的分布式锁？ (geekbang.org)</a></li>
</ol>
<p><img src="/2021/04/01/redis-gee-study-record/image(10).png" alt="image(10)"></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>专栏学习</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring 事务传播机制</title>
    <url>/2021/03/09/spring-transaction-spread/</url>
    <content><![CDATA[<h1 id="事务传播机制"><a href="#事务传播机制" class="headerlink" title="事务传播机制"></a>事务传播机制</h1><p>首先我们看 <a href="https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/transaction/TransactionDefinition.html">TransactionDefinition</a> 接口的注释。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/**Interface that defines Spring-compliant transaction properties.</span><br><span class="line">* Based on the propagation behavior definitions analogous to EJB CMT attributes.</span><br><span class="line"></span><br><span class="line">大概说这个接口定义了 Spring 兼容的事务属性。基于类似于EJB CMT属性的传播行为定义。</span><br><span class="line">*/</span><br></pre></td></tr></table></figure>

<span id="more"></span>那什么是 EJB CMT 呢？参考[维基百科](https://en.wikipedia.org/wiki/Enterprise_JavaBeans)的解释，EJB 容器必须支持 Container-managed transaction（CMT）来管理 ACID 特性的事务和 Bean managed transaction（BMT）。

<p>对应到 Spring 框架就是我们常说的 Spring 事务的传播机制以及隔离级别。</p>
<p>这两个容器的目的就是不需要显式复杂的配置，事务管理可以通过简单的注解进行。</p>
<p>Spring 通过 Spring AOP 来进行事务的管理，比如我们常用的 @Transactional 注解。</p>
<p>那当我们在方法中互相调用开启了事务功能的方法时，方法之间定义的事务是怎么起作用的呢？</p>
<p>比如 A 方法开启了事务，A 方法中调用了 B 方法，B 方法也开启了事务，那么这时候 A 和 B 之间的事务是独立进行的还是一起作用的？</p>
<p>Spring 在这方面为我们提供了管理的方法，也就是今天我们讨论的 Spring 的事务传播机制。</p>
<p>相关的定义在 Spring 的 TransactionDefinition 接口文件中。Spring 定义了 6 中传播行为。</p>
<h2 id="REQUIRED"><a href="#REQUIRED" class="headerlink" title="REQUIRED"></a><strong>REQUIRED</strong></h2><p>支持当前的事务；如果当前没有，那么创建一个新的。这也是 Spring 默认的传播行为。</p>
<p>比如，方法 A 调用了方法 B，A B都开启 REQUIRED 级别的事务。那么，B 方法会发现有了当前事务 A，那么 B 就会加入到 A 的事务中。</p>
<p>如果 A 没有开启事务，B 仍旧开启自己的事务。</p>
<h2 id="SUPPORTS"><a href="#SUPPORTS" class="headerlink" title="SUPPORTS"></a><strong>SUPPORTS</strong></h2><p>支持当前事务；如果没有当前事务以非事务的方式运行。</p>
<p>也可以从字面上理解，supports 看起来就没有 required 那么强势。如果没有当前事务就非事务运行好了。</p>
<h2 id="MANDATORY（强制性）"><a href="#MANDATORY（强制性）" class="headerlink" title="MANDATORY（强制性）"></a><strong>MANDATORY（强制性）</strong></h2><p>支持当前事务；如果没有会抛出异常；</p>
<h2 id="REQUIRES-NEW"><a href="#REQUIRES-NEW" class="headerlink" title="REQUIRES_NEW"></a><strong>REQUIRES_NEW</strong></h2><p>始终创建一个新的事务；如果有当前事务，会把当前事务挂起；</p>
<h2 id="NOT-SUPPORTED"><a href="#NOT-SUPPORTED" class="headerlink" title="NOT_SUPPORTED"></a><strong>NOT_SUPPORTED</strong></h2><p>不支持当前事务；总是以非事务方式执行。</p>
<h2 id="NEVER"><a href="#NEVER" class="headerlink" title="NEVER"></a>NEVER</h2><p>不支持当前事务；如果有当前事务会抛出异常。</p>
<h2 id="NESTED（嵌套）"><a href="#NESTED（嵌套）" class="headerlink" title="NESTED（嵌套）"></a>NESTED（嵌套）</h2><p>如果当前事务存在，就以嵌套的方式运行。行为和 REQUIRED 很像。</p>
<h2 id="自调用的问题"><a href="#自调用的问题" class="headerlink" title="自调用的问题"></a>自调用的问题</h2><p>Stackover Flow 上有一个相关的<a href="https://stackoverflow.com/questions/37217075/spring-nested-transactions">问答</a>，大意是在方法中调用了同一个 service 的方法，比如方法 A （<strong>REQUIRED</strong>）调用了方法 B，即使 B 声明了 <strong>REQUIRS_NEW</strong>的传播行为（始终创建一个新事务执行），但在 A 方法抛出异常的时候，B 方法还是会回滚。</p>
<p>这个问题需要从 Spring AOP 中动态代理的角度来分析。</p>
<p>Spring AOP 使用 JDK / Cglib 进行动态代理，它会通过代理，织入增强代码。比如调用带 @Transactional 注解的方法，会通过调用<strong>代理对象（增强后）</strong>的该方法进行。</p>
<p>但当进行自调用的时候，方法内部调用的方法还是使用原对象进行。也就是说，Spring AOP 的代理对象只会在不同的 bean 之间相互调用的时候使用。</p>
<p>一般遇到自调用的问题时，一个解决办法是新建一个帮助类，然后去调用它。</p>
<p>大家也可以看看 <a href="https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/data-access.html#transaction-declarative-annotations">Spring doc</a> 中的 1.4.6 节，有关自调用方面的说明。</p>
<p>Spring 官方还建议把 @Transactional 注解使用在具体的类上，不建议用在 接口 上。</p>
<p>原因是，当你使用了基于 类 的代理方式时，使用在 接口 上的注解就失效了。</p>
<h1 id="事务隔离级别"><a href="#事务隔离级别" class="headerlink" title="事务隔离级别"></a>事务隔离级别</h1><p>Spring 的事务隔离级别和数据库的没有什么区别。</p>
<ul>
<li>读未提交</li>
<li>读提交</li>
<li>可重复读</li>
<li>串行化</li>
</ul>
<p>Spring 有一个 ISOLATION_DEFAULT 级别，作用是采用和 数据库 一致的隔离级别。</p>
<h1 id="异常回滚"><a href="#异常回滚" class="headerlink" title="异常回滚"></a>异常回滚</h1><p>@Transactional 还有一个 rollbackFor() 属性。</p>
<p>接口上的注释大意是，这个属性定义了发生什么类型的异常会触发事务的回滚。</p>
<p>By default，事务会因为发生非受检异常回滚，受检异常发生了不会回滚。</p>
<p>你可以这么用，来定义自己的异常回滚策略。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@Transactional(propagation = Propagation.REQUIRED,rollbackFor = Exception.class)</span><br></pre></td></tr></table></figure>

<p>最后做一个总结，Spring 利用 Spring AOP 提供了方便的事务管理功能。在此基础上，通过配置事务的传播机制，进一步解决实际上使用事务时可能存在的问题。</p>
<p>因为 Spring AOP 动态代理的特性，在自调用的时候会出现事务失效的问题。除了刚才提到的使用帮助类来调用，还有一种方法是使用 AspectJ ，这是另一种 AOP 方案，其原理是通过在编译期在字节码层面织入代码实现增强功能。</p>
<p>最后就是事务的隔离级别。隔离级别这方面需要了解比如脏读，幻读等问题。</p>
<p>笔者也是一位初学者，如若文章有错误和疑惑的地方，也希望您可以指出，我们共同讨论和进步.</p>
<p>‍(=・ω・=)</p>
]]></content>
      <categories>
        <category>Spring Framework</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP 学习记录</title>
    <url>/2021/02/01/tcp-related/</url>
    <content><![CDATA[<p>首先了解 TCP 在 OSI 的七层模型中的第四层 - 传输层（来自《图解 TCP/IP》）<span id="more"></span></p>
<p><img src="/2021/02/01/tcp-related/image(1).png" alt="image(1)"></p>
<p>再简单的看一下 客户端 和 服务端之间的数据传输</p>
<p><img src="/2021/02/01/tcp-related/image(2).png" alt="image(2)"></p>
<p>数据发送时经过每一层都会加上对应的协议头，接收端则会一层层解析头，交给高层的协议处理。</p>
<h1 id="TCP-首部"><a href="#TCP-首部" class="headerlink" title="TCP 首部"></a>TCP 首部</h1><p><img src="/2021/02/01/tcp-related/image(3).png" alt="image(3)"></p>
<p>其中我觉得比较重要的几个内容是</p>
<ul>
<li><strong>序号</strong>。为包编上号，解决包的乱序问题。</li>
<li><strong>确认序号，<strong><strong>A</strong></strong>cknowledgement Number。</strong>也就是我们常说的确认应答号 ACK ，可以解决丢包的问题。</li>
<li><strong>Window</strong>。也就是我们常说的<strong>滑动窗口。</strong></li>
<li><strong>TCP Flags，状态位。</strong></li>
<li><strong>Port，</strong>应用监听的端口号。</li>
</ul>
<h1 id="TCP-的状态机"><a href="#TCP-的状态机" class="headerlink" title="TCP 的状态机"></a>TCP 的状态机</h1><p>TCP 就是靠改变，维持通讯双方的<strong>状态</strong>来保证他们之间的“连接”的。</p>
<p><img src="/2021/02/01/tcp-related/image(4).png" alt="image(4)"></p>
<p>上图中，客户端发送 SYN ，就是期待发起连接，客户端就切换为 SYN-SENT 状态。服务端被动监听端口，处于 LISTEN 状态。Server 接收到 SYN 包后，也向 client 发送 SYN，以及对接收到的 SYN 的 ACK，此时 server 处于 SYN-RECEIVED 状态。 client 收到 server 的 SYN 以及对于自己之前 SYN 的 ACK 后，也要针对 server 的 SYN 发送 ACK，就变成 ESTABLISHED 状态。若此刻 server 成功收到最后这个 ACK ，也进入 ESTABLISHED 状态。</p>
<p>刚才描述的过程就是我们常说的<strong>建立连接时的三次握手。</strong></p>
<p><img src="/2021/02/01/tcp-related/image(5).png" alt="image(5)"></p>
<p>对于三次握手，重要的点在于：</p>
<ul>
<li><strong>Synchronize Sequence Numbers，SYN。</strong>SYN seq = x seq = y，主要就是双方去确定 <strong>Sequence Numbers</strong>的值。****这个号就是以后通信要用到的包的序号。</li>
</ul>
<p><img src="/2021/02/01/tcp-related/image(6)-2560545.png" alt="image(6)"></p>
<p>对于<strong>四次挥手断开连接</strong>，因为TCP连接是全双工（两方可以互相同时传输数据）的，所以当任何一方想要断开连接时，都不能那么任性。你可以保证自己没有数据要发送了，但是你不知道对方还有没有数据要继续发送。所以我理解为什么是四次，因为双方都需要像对方提出断开连接并收一下 ACK。</p>
<p>在建立连接和断开连接时会有各种复杂情况，以下说明一些常见的</p>
<ul>
<li>建立连接时，客户端发送 SYN 后，直接掉线。server 就收不到 client 对于自己 SYN 的 ACK。当然 server 会一直尝试发送 SYN-ACK。 在Linux下，默认重试次数为5次，重试的间隔时间从1s开始每次都翻售，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s都知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 2^6 -1 = 63s，TCP才会把断开这个连接。</li>
<li>断开连接时的 time-wait 到 close 状态中间等待的一段 2MSL 时间，（Maximum Segment Lifetime）。因为双方在最后收到对方的 FIN 报文时，要给对方一个 ACK，让对方知道自己知道你要也要断开连接了。而不是发了 ACK 直接跑路，这样 B 就一直收不到自己 FIN 的 ACK。</li>
<li>这个 MSL ，<strong>报文最大生存时间</strong>。可以理解成报文在网络中可以存活的最长时间，超过这个时间还没到达目的地，就会被丢弃。所有 A B 等待的 2MSL 时间还有一个原因就是，避免下一个占用了此端口的应用收到上次与自己无关连接的包。等那么久还没有收到包也就被丢弃了。</li>
</ul>
<h1 id="TCP-重传机制"><a href="#TCP-重传机制" class="headerlink" title="TCP 重传机制"></a>TCP 重传机制</h1><p>TCP 保证可靠，稳定的传输，保证包全部顺利到达对方。但是网络世界很复杂，各种意外情况如何去保证呢？</p>
<p>其中一种就是前面提到的 ACK 机制。比如接收端收到 4000 的包，ACK 回去要 4001 之后的包，发送端就知道 4000 包成功到达了。</p>
<p>其中的意外情况有，1）接收端没收到 4000 的包，就一直 ACK 3999，发送端就知道要重发 4000 的包了。2）发送端没收到 ACK，就以为接收端没收到（实际上收到了），也重发 4000 的包。</p>
<h2 id="累计应答"><a href="#累计应答" class="headerlink" title="累计应答"></a>累计应答</h2><p>相比于一个一个包的发送，确认。实际上，接收端只会给发送端 ACK 收到的连续包的最后一个序号。比如发送端发送了 1-5 个包，接收端 ACK 一个 3 （x + 1,3这个包还没收到）给发送端。说明收到了 1,2 两个包。也就是<strong>累计应答</strong>。</p>
<p>需要注意的是，seq 和 ACK 是以字节数来计算的，故不能跳着 ack。<strong>只能确认最大的连续收到的包。</strong></p>
<h2 id="引入窗口"><a href="#引入窗口" class="headerlink" title="引入窗口"></a>引入窗口</h2><p>简单的重传机制就是发一个等一个，效率低。</p>
<p>引入窗口的目的就是减少等待，在没有收到部分包 ACK 的情况下，允许发送最大的段。比如窗口被定义为 5，就允许最多连续发送 5 个段，而不是一个个等待。</p>
<h2 id="重发控制"><a href="#重发控制" class="headerlink" title="重发控制"></a>重发控制</h2><p><img src="/2021/02/01/tcp-related/image(7).png" alt="image(7)"></p>
<p><img src="/2021/02/01/tcp-related/image(8).png" alt="image(8)"></p>
<p>还有一种需要重发的情况是，发送端一直接收不到 ACK。TCP 就会等待一段时间，如果超过就重发。这个等待时间不宜超过 RTT（数据包往返的时间），否则可能进行不必要的重传。</p>
<p>重发的时候就涉及到一个问题，当发送端一直接收不到 3001 的 ACK 时（接收端确实没有收到 3000 的数据），而接收端收到了到 5001 的数据（放在缓冲区）。那么发送端到底是选择重发 3000 的数据，还是把 3001 ，4001, 5001 全部重发呢？</p>
<h2 id="SACK"><a href="#SACK" class="headerlink" title="SACK"></a>SACK</h2><p>**Selective Acknowledgment (SACK)**（参看<a href="http://tools.ietf.org/html/rfc2018">RFC 2018</a>），这种方式需要在TCP头里加一个SACK的东西，ACK还是Fast Retransmit的ACK，SACK则是汇报收到的数据碎版。参看下图：</p>
<p><img src="/2021/02/01/tcp-related/image(9).png" alt="image(9)"></p>
<p>接收端不仅发送 ACK，还发送一个 SACK 向发送端说明自己缓冲区已经收到了 5000 的数据（只是还无法想你发送 5001 的 ACK ，因为在前面断了一截儿）。</p>
<p>但是，发送端不能把 SACK 作为真正意义上的 ACK 看待，因为接收端对于 SACK 的数据是可能放弃掉的。后续发送端如果检测到 ACK 没有实际性的增长，仍然需要重发该部分的数据。</p>
<h1 id="滑动窗口，窗口控制"><a href="#滑动窗口，窗口控制" class="headerlink" title="滑动窗口，窗口控制"></a>滑动窗口，窗口控制</h1><p>TCP 头中有一个 window 字段，又叫 Advertised-Window，这个字段的作用是接收端会告诉发送端自己能接受处理的最大数据，发送端会根据这个值调整发送的数据多少，避免接收端压力太大。</p>
<p>且这个 window 是有可能到 0 的。也就是说，发送端不再发送数据。</p>
<p>当 window 变成 0 的时候，发送方会定时发送窗口探测数据包，看看有没有增加 window 值的可能。一般这个值会设置成3次，第次大约30-60秒（不同的实现可能会不一样）。如果3次过后还是0的话，有的TCP实现就会发RST把链接断了。</p>
<h1 id="TCP-拥塞控制"><a href="#TCP-拥塞控制" class="headerlink" title="TCP 拥塞控制"></a>TCP 拥塞控制</h1><p>前面讨论的窗口，关注的点是接收端的处理能力。这里的拥塞控制也有一个窗口的概念，但是关注的点是对于整个网络的影响。</p>
<p>我们知道，TCP 有超时重发机制，如果每个 TCP 不顾及整个网络的情况，不断的重发数据，网络状况因此可能更差，形成恶性循环。</p>
<h2 id="慢启动"><a href="#慢启动" class="headerlink" title="慢启动"></a><strong>慢启动</strong></h2><p>Congestion Window，cwnd。MSS（maximum segment size最大分段长度）</p>
<p>算法如下</p>
<ol>
<li>连接建立，初始化 cwnd 为 1，说明可以传输一个 MSS 大小</li>
<li>每收到一个 ACK，cwnd++，线性增长</li>
<li>每过一个 RTT，cwnd = cwnd * 2，指数增长</li>
<li>ssthresh（slow start threshold），当 cwnd &gt;= ssthresh 时，进入“拥塞避免算法”</li>
</ol>
<p>从算法过程可以判断，当网络状况良好的时候，ACK 的快，RTT 也快，这个慢启动也不算非常慢。网络状况差的话，那就是缓慢增长，到定义的阈值。</p>
<h2 id="拥塞避免算法-–-Congestion-Avoidance"><a href="#拥塞避免算法-–-Congestion-Avoidance" class="headerlink" title="拥塞避免算法 – Congestion Avoidance"></a>拥塞避免算法 – Congestion Avoidance</h2><p>前面说过，还有一个ssthresh（slow start threshold），是一个上限，当cwnd &gt;= ssthresh时，就会进入“拥塞避免算法”。一般来说ssthresh的值是65535，单位是字节，当cwnd达到这个值时后，算法如下：</p>
<p>1）收到一个ACK时，cwnd = cwnd + 1/cwnd</p>
<p>2）当每过一个RTT时，cwnd = cwnd + 1</p>
<p>这样就可以避免增长过快导致网络拥塞，慢慢的增加调整到网络的最佳值。很明显，是一个线性上升的算法。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://coolshell.cn/articles/11564.html">TCP 的那些事儿</a> </li>
<li><a href="https://time.geekbang.org/column/intro/85">趣谈网络协议</a></li>
</ul>
]]></content>
      <categories>
        <category>网络协议</category>
      </categories>
      <tags>
        <tag>tcp</tag>
      </tags>
  </entry>
  <entry>
    <title>redis.conf-zh_cn 中文翻译</title>
    <url>/2021/06/02/redis-conf-zh-cn/</url>
    <content><![CDATA[<h1 id="Redis-redis-conf-中文翻译"><a href="#Redis-redis-conf-中文翻译" class="headerlink" title="Redis - redis.conf 中文翻译"></a>Redis - redis.conf 中文翻译</h1><blockquote>
<p>Redis 5.0.8 默认配置文件的翻译。个人英语水平有限，应以原文档为标准。</p>
</blockquote>
<span id="more"></span>

<p><strong>完结撒花~…</strong></p>
<p>Redis 配置文件范例。</p>
<p>需要注意的是为了能顺利读取配置文件，Redis 启动时要将配置文件路径作为第一个参数：</p>
<p>./redis-server /path/to/redis.conf</p>
<h2 id="INCLUDES-（包含）"><a href="#INCLUDES-（包含）" class="headerlink" title="INCLUDES （包含）"></a>INCLUDES （包含）</h2><p>在这配置包含一个或多个配置文件。这个配置项适用于那些对大部分 Redis 实例有标准的配置模板，但对小部分 Redis 实例有定制化需求的场景。 包括文件可以包含其他文件，所以请明智使用。</p>
<p>请注意 “include” 配置不会被 admin 或者 Redis 哨兵 “CONFIG REWRITE” 命令重写。由于 Redis 总是使用最后处理的行作为配置值，所以最好将 includes 配置放在该文件的最开始以此避免配置在运行的时候被重写。</p>
<p>相反的你想要用 includes 配置来重写配置项，那 include 应该放在最后一行会更好。</p>
<h3 id="include-path-to-local-conf"><a href="#include-path-to-local-conf" class="headerlink" title="#include /path/to/local.conf"></a><strong>#include /path/to/local.conf</strong></h3><h3 id="include-path-to-other-conf"><a href="#include-path-to-other-conf" class="headerlink" title="#include /path/to/other.conf"></a><strong>#include /path/to/other.conf</strong></h3><h2 id="MODULES（模块）"><a href="#MODULES（模块）" class="headerlink" title="MODULES（模块）"></a>MODULES（模块）</h2><p>启动时（at startup）加载模块。如果 server 加载模块失败服务器会终止（abort）。</p>
<h3 id="loadmodule-path-to-my-module-so"><a href="#loadmodule-path-to-my-module-so" class="headerlink" title="#loadmodule /path/to/my_module.so"></a><strong>#loadmodule /path/to/my_module.so</strong></h3><h3 id="loadmodule-path-to-other-moudle-so"><a href="#loadmodule-path-to-other-moudle-so" class="headerlink" title="#loadmodule /path/to/other_moudle.so"></a><strong>#loadmodule /path/to/other_moudle.so</strong></h3><h2 id="NETWORK（网络）"><a href="#NETWORK（网络）" class="headerlink" title="NETWORK（网络）"></a>NETWORK（网络）</h2><p>如果没有使用 bind 进行配置，Redis 则默认监听所有 Server 上可以访问的网络接口的连接。如果配置了 bind 指向具体的值，Redis 则只监听配置的那些连接的网络接口。可以是一个 IP 或者紧接着多个 IP 地址。</p>
<p>示例：</p>
<p><strong>#bind 192.167.2.34 10.0.0.1</strong></p>
<p><strong>#bind 127.0.0.1 ::1</strong></p>
<p>警告：如果跑 Redis 的机器直接暴露在网络中，binding（指定，绑定）所有的网络接口有潜在的危险，且会让实例暴露给网络上的所有人。因此，我们取消注释了下面的 bind 指令，这会让 Redis 只监听 IPv4 的环回地址（意味着 Redis 只接受跑在和 Redis 实例一台机器上的客户端连接）。</p>
<p><strong>如果你确认你的 Redis 实例可以接受来自所有地址的请求，把下面的指令注释掉即可。</strong></p>
<h3 id="bind-127-0-0-1"><a href="#bind-127-0-0-1" class="headerlink" title="bind 127.0.0.1"></a><strong>bind 127.0.0.1</strong></h3><p>保护模式是安全防护的其中一层，保护模式的存在是为了避免暴露在网络中的 Redis 实例被不当的连接滥用（Redis instances left open on the internet are accessed and exploited）。</p>
<p>当保护模式打开且：</p>
<p>1）Redis 服务没有使用 “bind” 去绑定明确的 ip 地址集合。</p>
<p>2）没有配置密码。</p>
<p>那么，Redis 服务只接受来自 IPv4 和 IPv6 的环回地址 127.0.0.1 和 ::1并且是来自 Unix 域的套接字。</p>
<p>保护模式默认开启。除非你确定你的 Redis 实例在没有配置连接认证或者使用 bind 命令限制特定的 ip 连接的情况下还可以被连接。不然最好保持该模式开启。</p>
<h3 id="protected-mode-yes"><a href="#protected-mode-yes" class="headerlink" title="protected-mode yes"></a><strong>protected-mode yes</strong></h3><p>通过特定端口进行连接，默认端口是 6379（IANA #815344）。如果端口配置成 0，Redis 就不会监听 TCP 套接字。</p>
<h3 id="port-6379"><a href="#port-6379" class="headerlink" title="port 6379"></a><strong>port 6379</strong></h3><p>TCP listen() 积压（backlog）。</p>
<p>在高频请求场景下的 Redis，为了避免慢的客户端连接，你需要配置较高的 backlog。提醒事项：Linux 内核会默默的将其截断成 /proc/sys/net/core/somaxconn 的值，所以保证同时提高 somaxconn 和 tcp_max_syn_backlog 的值以求预期的效果。</p>
<h3 id="tcp-backlog-511"><a href="#tcp-backlog-511" class="headerlink" title="tcp-backlog 511"></a><strong>tcp-backlog 511</strong></h3><p><strong>Unix 套接字</strong></p>
<p>自己指定特定的 Unix 套接字路径来监听可能来的连接。Redis 没有为此配置默认值，如果你也没有手动去配置指定的话，那 Redis 不会监听一个 unix 套接字。</p>
<p><strong>#unixsocket /tmp/redis.sock</strong></p>
<p><strong>#unixsocketperm 700</strong></p>
<p><strong>N 秒后</strong>（0 表示此配置无效），客户端和服务端之间是空闲的，则断开连接。</p>
<h3 id="timeout-0"><a href="#timeout-0" class="headerlink" title="timeout 0"></a><strong>timeout 0</strong></h3><p><strong>TCP keepalive</strong></p>
<p>如果配置了非零的值，使用 SO_KEEPALIVE 发送 TCP 的 ACKs 给那些可能断连的客户端。这很管用，原因有：</p>
<p>1）检测死掉的同伴链接（Detect dead peers）。</p>
<p>2）从中间网络设备的视角来看，连接持续保存。</p>
<p>在 Linux，配置特定的值（单位为 秒）为周期来发送 ACKs。注意事项：需要两倍的该时间来关闭连接。不同的内核中该周期取决于内核的配置。</p>
<p>300 秒是一个比较合理的选择，这也是 Redis 从 3.2.1 版本开始配置的默认值。</p>
<h3 id="tcp-keepalive-300"><a href="#tcp-keepalive-300" class="headerlink" title="tcp-keepalive 300"></a>tcp-keepalive 300</h3><h2 id="GENERAL"><a href="#GENERAL" class="headerlink" title="GENERAL"></a>GENERAL</h2><p>Redis 运行默认不是守护进程。需要的话将该项配置成 yes。</p>
<p>注意事项：该配置开启后，Redis 会默认在 /var/run/redis.pid 文件中写相关信息。</p>
<h3 id="daemonize-no"><a href="#daemonize-no" class="headerlink" title="daemonize no"></a>daemonize no</h3><p>如果你是以 upstart 或者 systemd 方式跑 Redis，Redis 可以与你的监督数（supervision tree）交互。具体的选项：</p>
<ul>
<li>supervised no    - 不进行监督树的交互。</li>
<li>supervised upstart    - 通过将 Redis 置为 SIGSTOP 模式进行 upstart 信号通知。</li>
<li>supervised systemd    - 通过将 READY=1 写入 $NOTIFY_SOCKET 进行 systemd 的信号通知。</li>
<li>supervised auto    - 基于 UPSTART_JOB 或者 NOTIFY_SOCKET 环境变量来检测是 upstart 还是 systemd 方式。</li>
</ul>
<p>注意：以上的 supervision 方法只通知 “处理准备就绪” 的信号。他们不会持续的响应你配置的 supervisor。</p>
<h3 id="supervised-no"><a href="#supervised-no" class="headerlink" title="supervised no"></a>supervised no</h3><p>如果配置指定了 pid 文件，Redis 就用该配置的 pid 文件写入，退出的时候移除对应的 pid 文件。</p>
<p>如果 Redis 是以非守护进程模式的运行，又没有配置指定的 pid 文件，那么不会创建 pid 文件。如果 Redis 是守护进程的模式，即使没有配置指定的 pid 文件，会默认使用 “/var/run/redis.pid”文件。</p>
<p>最好创建一个 pid 文件（Creating a pid file is best effort）：没有创建 pid 文件不会有任何影响，Server 还是会正常运行。</p>
<h3 id="pidfile-var-run-redis-6379-pid"><a href="#pidfile-var-run-redis-6379-pid" class="headerlink" title="pidfile /var/run/redis_6379.pid"></a>pidfile /var/run/redis_6379.pid</h3><p>指定 Server 的日志级别（Specify the server<strong>verbosity</strong>level）。</p>
<p>有以下四种级别：</p>
<ul>
<li>debug（包含许多具体信息，开发/测试 环境下很方便）</li>
<li>verbose（包含许多不常用的信息，但没有 debug 级别那么混乱）</li>
<li>notice（moderately verbose，不多不少，很适合生产环境）</li>
<li>warning（只记录重要或者非常的信息）</li>
</ul>
<h3 id="loglevel-notice"><a href="#loglevel-notice" class="headerlink" title="loglevel notice"></a>loglevel notice</h3><p>指定 log 文件名。配置成空串的话可以强制 Redis 在标准输出记录日志。注意事项：如果你使用标准输出进行日志记录且是以 守护进程 的模式运行，日志会在 /dev/null 中。</p>
<h3 id="logfile-“”"><a href="#logfile-“”" class="headerlink" title="logfile “”"></a>logfile “”</h3><p>想让日志记录到系统日志，设置 ‘syslog-enabled’ 成 yes，使用 syslog 带有的其他配置选项来满足你的需求。</p>
<h3 id="syslog-enabled-no"><a href="#syslog-enabled-no" class="headerlink" title="#syslog-enabled no"></a>#syslog-enabled no</h3><p>指定 syslog 的身份。</p>
<h3 id="syslog-ident-redis"><a href="#syslog-ident-redis" class="headerlink" title="#syslog-ident redis"></a>#syslog-ident redis</h3><p>指定 syslog 工具（facility）。一定要是 USER 或者在 LOCAL0-LOCAL7 之间。</p>
<h3 id="syslog-facility-local0"><a href="#syslog-facility-local0" class="headerlink" title="#syslog-facility local0"></a>#syslog-facility local0</h3><p>设置数据库的号码。默认的数据库号是 DB 0，你在每个连接中，通过 SELECT <dbid>，选择一个 0~databases-1 的数来配置特定的数据库号。</dbid></p>
<h3 id="databases-16"><a href="#databases-16" class="headerlink" title="databases 16"></a>databases 16</h3><p>Redis 会在启动的时候，如果标准输出日志是 TTY，则会在开始记录标准输出日志的时候展示一个 ASCII 字符组成的 Redis logo。也就是说，通常只在交互的会话中会展示该 logo。</p>
<h3 id="always-show-logo-yes"><a href="#always-show-logo-yes" class="headerlink" title="always-show-logo yes"></a>always-show-logo yes</h3><h2 id="SNAPSHOTTING（快照）"><a href="#SNAPSHOTTING（快照）" class="headerlink" title="SNAPSHOTTING（快照）"></a>SNAPSHOTTING（快照）</h2><p>在硬盘保存数据库：</p>
<p>#save <seconds> <changes>，如果 seconds 和 写操作都配置了，那么一旦达到了配置条件 Redis 会将 DB 保存到硬盘。</changes></seconds></p>
<p>以本配置文件的默认配置举例，达到了以下条件会触发写磁盘：</p>
<p>900 秒内（15 分钟）且数据库中至少有一个 key 被改变。</p>
<p>300 秒内（5 分钟）且数据库中至少有10 个 key 被改变。</p>
<p>60 秒内 且数据库中只有一个 10000 个 key 被改变。</p>
<p>提醒：你可以通过注释以下所有的 save 配置行以取消该功能。</p>
<p>也可以通过添加一个带空串的 save 指令来让配置的 save 选择失效。比如：</p>
<p>save “”</p>
<p>save 900 1</p>
<p>save 300 10</p>
<h3 id="save-60-10000"><a href="#save-60-10000" class="headerlink" title="save 60 10000"></a>save 60 10000</h3><p>在开启了 RDB 快照后，如果最近的一次 RDB 快照在后台生成失败的话，Redis 默认会拒绝所有的写请求。这么做的目的是为了让用户注意到后台持久化可能出现了问题。否则用户可能一直无法注意到问题，进而可能导致灾难级别的事情发生。</p>
<p>如果后台存储（bgsave）能继续顺利工作，Redis 会自动的继续处理写请求。</p>
<p>但是，如果你已经为你的 Redis 实例和持久化配置了合适的监控手段，且希望 Redis 在非理想情况下（比如硬盘问题，权限问题等等）仍继续提供服务，可以将此项配置为 no。</p>
<h3 id="stop-writes-on-bgsave-error-yes"><a href="#stop-writes-on-bgsave-error-yes" class="headerlink" title="stop-writes-on-bgsave-error yes"></a>stop-writes-on-bgsave-error yes</h3><p>想要在生成 rdb 文件的时候使用 LZF 压缩 String 对象？</p>
<p>将该配置保持默认为 ‘yes’ 几乎不会出现意外状况。（it’s almost alwats a win）</p>
<p>可以将该配置设置为 “no” 来节省 CPU 开销。但是那些原本可以被压缩的 key 和 value 会让数据集更大。</p>
<h3 id="rdbcompression-yes"><a href="#rdbcompression-yes" class="headerlink" title="rdbcompression yes"></a>rdbcompression yes</h3><p>从 5.0 版本开始 RDB 文件的末尾会默认放置一个 CRC64 的校验码。</p>
<p>这会让文件的格式更加容易检验验证，代价是生成和加载 RDB 文件的性能会损失 10% 左右。你可以把该配置关闭以求更佳的性能。</p>
<p>没有开启校验码配置的 RDB 文件会将校验码设置为 0，加载该文件的程序就会跳过校验过程。</p>
<h3 id="rdbchecksum-yes"><a href="#rdbchecksum-yes" class="headerlink" title="rdbchecksum yes"></a>rdbchecksum yes</h3><p>配置 rdb 文件的名称。</p>
<h3 id="dbfilename-dump-rdb"><a href="#dbfilename-dump-rdb" class="headerlink" title="dbfilename dump.rdb"></a>dbfilename dump.rdb</h3><p>存储 rdb 文件的目录。</p>
<p>数据库会使用该配置放置 rdb 文件，文件的名字使用上面的 ‘dbfilename’ 指定的文件名。</p>
<p>AOF 文件的存储位置也会使用这个配置项。</p>
<p>注意：配置一个目录而不是文件名。</p>
<h3 id="dir"><a href="#dir" class="headerlink" title="dir ./"></a>dir ./</h3><h2 id="REPLICATION（复制）"><a href="#REPLICATION（复制）" class="headerlink" title="REPLICATION（复制）"></a>REPLICATION（复制）</h2><p>主从复制。使用 replicaof 来让一个 Redis 实例复制另一个 Redis 实例。接来下是关于 Redis 复制需要了解的一些事情。</p>
<p><img src="/2021/06/02/redis-conf-zh-cn/image-20210925113724247.png" alt="image-20210925113724247"></p>
<p>1）Redis 复制时异步进行的，但是可以通过配置让 Redis 主节点拒绝写请求：配置会给定一个值，主节点至少需要和大于该值的从节点个数成功连接。</p>
<p>2）如果 Redis 从节点和主节点意外断连了很少的一段时间，从节点可以向主节点进行<strong>增量复制</strong>。你可以根据你的需要配置复制的备份日志文件大小（在下一部分可以看到相关的配置）</p>
<p>3）复制会自动进行且不需要人为介入（intervention）。在网络划分后复制会自动与主节点重连且同步数据。</p>
<h3 id="replicaof"><a href="#replicaof" class="headerlink" title="#replicaof  "></a>#replicaof <masterip> <masterport></masterport></masterip></h3><p>如果主节点配置了密码（使用了 “requirepass” 配置项），从节点需要进行密码认证才能进行复制同步的过程，否则主节点会直接拒绝从节点的复制请求。</p>
<h3 id="masterauth"><a href="#masterauth" class="headerlink" title="#masterauth "></a>#masterauth <master-password></master-password></h3><p>当复制过程与主节点失去连接，或者当复制正在进行时，复制可以有两种行为模式：</p>
<p>1）如果 replica-serve-stale-data 设置为 ‘yes’（默认设置），从节点仍可以处理客户端请求，但该从节点的数据很可能和主节点不同步，从节点的数据也可能是空数据集，如果这是与主节点进行的第一次同步。</p>
<p>2）如果 replica-serve-stale-data 设置成 ‘no’，从节点会对除了 INFO，replicaOF，AUTH，PING，SHUTDOWN，REPLCONF，ROLE，CONFIG，SUBSCRIBE，UNSUBSCRIBE，PSUBSCRIBE，PUNSUBSCRIBE，PUBLISH，PUBSUB，COMMAND， POST，HOST： and LATENCY 这些命令之外的请求均返回 “SYNC with master in process”。</p>
<h3 id="replica-serve-stale-data-yes"><a href="#replica-serve-stale-data-yes" class="headerlink" title="replica-serve-stale-data yes"></a>replica-serve-stale-data yes</h3><p>可以配置从节点是否可以处理写请求。针对从节点开启写权限来存储时效低的（ephemeral）数据可能是一种有效的方式（因为写入到从节点的数据很可能随着重新同步而被删除），但是开启该配置也会导致一些问题。</p>
<p>从 Redis 2.6 开始从节点默认是仅可读的。</p>
<p>提示：可读的从节点一般不会暴露给网络中不信任的客户端。这仅是针对不正确使用实例的一层保护。从节点默认仍会响应管理层级的命令，比如 CONFIG，DEBUG 等等。在一定程度上可以使用 ‘rename-command’ 避免那些 管理/危险 的命令，提高安全性（To a limited extent you can improve security of read only replicas using ‘rename-command’ to shadow all the administrative / dangerous commands）。</p>
<h3 id="replica-read-only-yes"><a href="#replica-read-only-yes" class="headerlink" title="replica-read-only yes"></a>replica-read-only yes</h3><p>同步复制策略：硬盘或者套接字。</p>
<hr>
<p>警告：不使用硬盘的复制策略目前还在实验阶段</p>
<hr>
<p>新建立连接和重连的副本不会根据数据情况进行恢复传输，只会进行全量复制。主节点会传输在从节点之间传输 RDB 文件。传输行为有两种方式：</p>
<p>1）硬盘备份：Redis 主节点创建一个子进程来向硬盘写 RDB 文件。之后由父进程持续的文件传给副本。</p>
<p>2）不使用硬盘：Redis 主节点建立一个进程直接向副本的网络套接字写 RDB 文件，不涉及到硬盘。</p>
<p>对于方式 1，在生成 RDB 文件时，多个副本会进行入队并在当前子进程完成 RDB 文件时立即为副本进行 RDB 传输。</p>
<p>对于方式 2，一旦传输开始，新来的副本传输请求会入队且只在当前的传输断开后才建立新的传输连接。</p>
<p>如果使用方式 2，主节点会等待一段时间，根据具体的配置，等待是为了可以在开始传输前可以有期望的副本同步请求到达，这样可以使用并行传输提高效率。</p>
<p>对于配置是比较慢的硬盘，而网络很快（带宽大）的情况下，使用方式 2 进行副本同步会更适合。</p>
<h3 id="repl-diskless-sync-no"><a href="#repl-diskless-sync-no" class="headerlink" title="repl-diskless-sync no"></a>repl-diskless-sync no</h3><p>如果 diskless sync 是开启的话，就需要配置一个延迟的秒数，这样可以服务更多通过 socket 传输 RDB 文件的副本。</p>
<p>这个配置很主要，因为一旦传输开始，就不能为新来的副本传输服务，只能入队等待下一次 RDB 传输，所以该配置一个延迟的值就是为了让更多的副本请求到达。</p>
<p>延迟配置的单位是秒，默认是 5 秒。不想要该延迟的话可以配置为 0 秒，传输就会立即开始。</p>
<h3 id="repl-diskless-sync-delay-5"><a href="#repl-diskless-sync-delay-5" class="headerlink" title="repl-diskless-sync-delay 5"></a>repl-diskless-sync-delay 5</h3><p>副本会根据配置好的时间间隔（interval）想主节点发送 PING 命令。可以通过 repl_ping_replica_period 配置修改时间间隔。默认为 10 秒。</p>
<h3 id="repl-ping-replica-period-10"><a href="#repl-ping-replica-period-10" class="headerlink" title="#repl-ping-replica-period 10"></a>#repl-ping-replica-period 10</h3><p>下面的配置会将副本进行超时处理，为了：</p>
<p>1）在副本的角度，在同步过程中批量进行 I/O 传输。</p>
<p>2）从副本s的角度，主节点超时了。</p>
<p>3）从主节点的角度，副本超时了。</p>
<p>需要重视的一点是确保该选项的配置比 repl-ping-replica-period 配置的值更高，否则每次主从之间的网络比较拥挤时就容易被判定为超时。</p>
<h3 id="repl-timeout-60"><a href="#repl-timeout-60" class="headerlink" title="#repl-timeout 60"></a>#repl-timeout 60</h3><p>同步过后在副本套接字上关闭 TCP_NODELAY？</p>
<p>如果你选择了 ‘yes’ ，Redis 会使用很小的 TCP 包，占用很低的带宽来想副本发送数据。但是这么做到达副本的数据会有一些延迟，使用默认的配置值且是 Linux 内核该延迟最多可能 40 毫秒。</p>
<p>如果你选择 ‘no’，副本的数据延迟会更低但是占用的带宽会更多一些。</p>
<p>我们默认会为了低延迟进行优化，但是在比较拥挤网络情况下或者是主节点和副本之间的网络情况比较复杂，比如中间有很多路由跳转的情况下，把选项设置为 ‘yes’ 应该会比较适合。</p>
<h3 id="repl-disable-tcp-nodelay-no"><a href="#repl-disable-tcp-nodelay-no" class="headerlink" title="repl-disable-tcp-nodelay no"></a>repl-disable-tcp-nodelay no</h3><p>配置副本的缓冲区（backlog）大小。该缓冲区用来在副本断开连接后暂存副本数据。这样做的因为但副本重新连接后，不一定要重新进行全量复制，很多时候增量复制同步（仅同步断连期间副本可能丢失的数据）完全足够了。</p>
<p>配置的缓冲区越大，副本可以承受的断连时间可以更长。</p>
<p>至少有一个副本连接时缓冲区才会进行分配。</p>
<h3 id="repl-backlog-size-1mb"><a href="#repl-backlog-size-1mb" class="headerlink" title="#repl-backlog-size 1mb"></a>#repl-backlog-size 1mb</h3><p>主节点如果一段时间没有副本连接，上面提到的缓冲区会被释放。你可以通过配置一个指定的时间来释放缓冲区，如果主节点在这个时间内还没有与新的副本建立连接。</p>
<p>需要注意的是副本不会因为超时释放缓冲区，因为副本可能会被晋升（promot）为主节点，需要保持对其他副本进行增量复制的能力：因此他们总是积累缓冲区。</p>
<p>配置为 0 意味着不释放缓冲区。</p>
<h3 id="repl-backlog-ttl-3600"><a href="#repl-backlog-ttl-3600" class="headerlink" title="#repl-backlog-ttl 3600"></a>#repl-backlog-ttl 3600</h3><p>副本的优先级是一个整型树字，可以由 Redis 的 INFO 命令显示。优先级的作用在于当主节点无法提供服务后，Redis 哨兵会使用到优先级进行选举副本，晋升为主节点。</p>
<p>值越低，代表该副本晋升成为主节点的优先级越高，比如说有三个副本，优先级的值分别为 10，100，25，Redis 哨兵会选择最低的那个，即优先级配置为10的那个。</p>
<p>但是，一个特殊的配置值 ‘0’，意味着该副本不可能充当主节点的角色，故优先级配置为 0 的副本永远不会被 Redis 哨兵选择晋升。</p>
<p>默认的优先级配置时 100.</p>
<h3 id="replica-priority-100"><a href="#replica-priority-100" class="headerlink" title="replica-priority 100"></a>replica-priority 100</h3><p>主节点可以根据目前连接的延迟小于 M 秒的副本数量，选择是否拒绝写请求。</p>
<p>数量 N 的副本需要是 “online” 的状态。</p>
<p>延迟的秒数（The lag（落后） in seconds） M ，计算方式是根据上一次副本发送 ping 命令到主节点的时间计算。通常每秒都会发送 ping 命令。</p>
<p>这个选项不保证 N 个副本会接受写请求，但是如果没有足够的副本可用，则会限制那些丢失写请求的暴露窗口至特定的秒数（This option does not GUARANTEE that N replicas will accept the write, but will limit the window of exposure for lost writes in case not enough replicas are available, to the specified number of seconds.）</p>
<p>比如要求至少有三个延迟小等于 10 秒的副本，你可以这么配置：</p>
<h3 id="min-replicas-to-write-3"><a href="#min-replicas-to-write-3" class="headerlink" title="#min-replicas-to-write 3"></a>#min-replicas-to-write 3</h3><h3 id="min-replicas-max-lag-10"><a href="#min-replicas-max-lag-10" class="headerlink" title="#min-replicas-max-lag 10"></a>#min-replicas-max-lag 10</h3><p>配置设置为 0 会关闭该功能。</p>
<p>默认的 min-replicas-to-write 被设置为 0（功能关闭），min-replicas-max-lag 设置为 10.</p>
<p>主节点应该有多种方式来列举出依附与它的副本的信息（ip 和 port）。比如 “INFO replication” 就可以提供这些信息，它也会被其他的功能使用，比如 Redis 哨兵就会使用该命令列举副本实例。还有一种方式是在主节点运行 “ROLE” 命令来获取这些信息。</p>
<p>副本获取监听的 IP 和 地址分别通过以下的方式：</p>
<ul>
<li>IP：IP 地址在副本和主节点建立的 socket 连接中自动被检测到。</li>
<li>Port：端口信息会在副本进行复制的 TCP 握手中交流传递，端口也是副本用来监听连接的一部分。</li>
</ul>
<p>然而，如果使用了端口转发或者 NAT（Network Address Translation），实际连接到副本很可能通过的是不同的 IP 和 端口对。下面的两个配置选项用来让副本上报特定的 IP 和 端口 集合给它连接的主节点，之后主节点使用 “INFO” 或者 “ROLE” 命令都可以输出这些上报的值。</p>
<p>如果你只想上报 ip 或 端口其中一个，就没有必要两个都使用。</p>
<h3 id="replica-announce-ip-5-5-5-5"><a href="#replica-announce-ip-5-5-5-5" class="headerlink" title="#replica-announce-ip 5.5.5.5"></a>#replica-announce-ip 5.5.5.5</h3><h3 id="replica-announce-port-1234"><a href="#replica-announce-port-1234" class="headerlink" title="#replica-announce-port 1234"></a>#replica-announce-port 1234</h3><h2 id="SECURITY（安全）"><a href="#SECURITY（安全）" class="headerlink" title="SECURITY（安全）"></a>SECURITY（安全）</h2><p>要求客户端先使用命令 AUTH <PASSWORD> 进行认证，才能处理其他命令。 在一个可不信的环境，也就是说你不想所有知道该主机的客户端都可以与之建立连接的情况下很有用。</PASSWORD></p>
<p>该配置为了向后的兼容器应该保持被注释不使用，因为大多数的使用者不需要认证（e.g. 他们只是在自己的机器上跑实例）</p>
<p>警告：因为 Redis 的响应速率很快，所以恶意攻击者可能在每秒中发送 150k 数据量的密码尝试解密。这意味着你设置的密码强度要足够大，否则很容易被破解。</p>
<h3 id="requirepass-foobared"><a href="#requirepass-foobared" class="headerlink" title="#requirepass foobared"></a>#requirepass foobared</h3><p>命名的重命名。</p>
<p>可以在共享的环境中重命名那些比较危险的命令。比如把 CONFIG 命令重命名成一个不好猜的名字，这样内部的功能还可以使用，且可以避免大部分的客户端使用。</p>
<p>例如：rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52</p>
<p>甚至可以将命名重命名成一个空串，使其失效。</p>
<h3 id="rename-command-CONFIG-“”"><a href="#rename-command-CONFIG-“”" class="headerlink" title="#rename-command CONFIG “”"></a>#rename-command CONFIG “”</h3><p>请注意修改命令名称的行为会记录在 AOF 文件中或传输到副本可能会导致意外情况。</p>
<h2 id><a href="#" class="headerlink" title></a></h2><h2 id="CLIENTS（客户端）"><a href="#CLIENTS（客户端）" class="headerlink" title="CLIENTS（客户端）"></a>CLIENTS（客户端）</h2><p>设置可以同时连接客户端的最大数量。默认该项设置为 10000 个客户端，但是如果 Redis server 不能配置过程文件来限制最大的同时连接数，那么实际的最大连接数会变成当前文件配置的数组再减去 32（因为 Redis 内部需要维护一部分文件描述符）</p>
<p>一旦达到该限制数 Redis 会拒绝所有的新连接并返回错误信息 ‘max number of clients reached’。</p>
<h3 id="maxclients-10000"><a href="#maxclients-10000" class="headerlink" title="#maxclients 10000"></a>#maxclients 10000</h3><h2 id="MEMEORY-MANAGEMENT（内存管理）"><a href="#MEMEORY-MANAGEMENT（内存管理）" class="headerlink" title="MEMEORY MANAGEMENT（内存管理）"></a>MEMEORY MANAGEMENT（内存管理）</h2><p>设置限定的最大内存使用。</p>
<p>但内存使用达到限制 Redis 会根据配置的淘汰策略（见 maxmemory-policy）移除键值对。</p>
<p>如果根据淘汰策略，Redis 不能移除键值对，Redis 会拒绝那些申请更大内存的命令，比如 SET，LPUSH 等等，但是仍可以处理读请求，比如 GET 等。</p>
<p>该选项对那些使用 Redis 进行 LRU，LFU 缓存系统或者硬性限制内存很友好（使用 ‘noeviction’ 策略）。</p>
<p>警告：如果你为实例配置了 maxmemory，且该实例配置了子节点，那么已使用内存的大小就需要加上为副本配置的输出缓冲区的大小。这样因为 网络问题/重新同步 不会一直触发键的淘汰行为。相反的，副本缓冲区中充满了对键的删除或淘汰的情况可能触发更多 key 被淘汰，以此类推直到库完全被清空。</p>
<blockquote>
<p>WARNING: If you have replicas attached to an instance with maxmemory on, the size of the output buffers needed to feed the replicas are subtracted from the used memory count, so that network problems / resyncs will not trigger a loop where keys are evicted, and in turn the output buffer of replicas is full with DELs of keys evicted triggering the deletion of more keys, and so forth until the database is completely emptied.</p>
</blockquote>
<p>简单说就是，如果你为实例配置了副本，那么建议你设置一个较低的 maxmemory 值，这样系统中就有更多的内存空间留给 副本缓冲区（如果淘汰策略是 ‘noeviction’ 那上面说的就没有必要）。</p>
<h3 id="maxmemory"><a href="#maxmemory" class="headerlink" title="#maxmemory  "></a>#maxmemory  <bytes></bytes></h3><p>MAXMEMORY POLICY：在内存使用达到 maxmemory 后，Redis 如何选择 键值对 进行淘汰。有以下几种：</p>
<ul>
<li>volatile-lru，使用 LRU 算法，在设置了过期时间的 key 中选择。</li>
<li>allkeys-lru，使用 LRU 算法，在所有的 key 中选择。</li>
<li>volatile-lfu，使用 LFU 算法，在设置了过期时间 key 中选择。</li>
<li>allkeys-lfu，使用 LFU 算法，在所有的 key 中选择。</li>
<li>volatile-random，在设置了过期时间的 key 中随机选择。</li>
<li>allkeys-random，在所有 key 中随机选择。</li>
<li>volatile-ttl，在设置了过期时间的 key 中，选择过期时间最近的 key。</li>
<li>noeviction，不淘汰 key ，对任何写操作（使用额外内存）返回错误。</li>
</ul>
<p>LRU 代表最近最少未使用。</p>
<p>LFU 代码最近最不常使用。</p>
<p>LRU，LFU 和 volatile-ttl 均由近似的随机算法实现。</p>
<p>提示：不管采用了以上的哪种策略，对于新的写请求，如果没有合适的 key 可以淘汰，Redis 均会响应一个 error。</p>
<p>比如如下的写命令：</p>
<p>set setnx setex append incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd</p>
<p>sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby getset mset msetnx exec sort。</p>
<p>默认策略是：</p>
<h3 id="maxmemory-policy-noeviction"><a href="#maxmemory-policy-noeviction" class="headerlink" title="#maxmemory-policy noeviction"></a>#maxmemory-policy noeviction</h3><p>LRU，LFU 以及最小 TTL 的实现都不是精确的而是比较粗略的近似算法（为了节省内存），为了速度或者精确度，你可以进行相应的配置。默认 Redis 会检查 5 个 key，在其中选择最近最少使用的，你也可以直接在下面的配置项中配置 Redis 选择的样本数量。</p>
<p>默认配置的值是 5，已经可以有一个很完美的结果。10 的话可能会让选择策略更像真正意义上的 LRU 算法，但是需要更多 CPU 资源。3 的话会更快，但是不够精确。</p>
<h3 id="maxmemory-samples-5"><a href="#maxmemory-samples-5" class="headerlink" title="#maxmemory-samples 5"></a>#maxmemory-samples 5</h3><p>从 Redis 5.0 之后，副本默认会忽略为其配置的 maxmemory 选项（除非因为故障转移（failover）或者选择将其晋升为主节点）。也就是说 key 的淘汰只会由主节点执行，副本对应的是主节点发送对应的删除命令给副本作为 key 的淘汰方式。</p>
<p>这个行为模式保证了主副节点的一致性（这通常也是你需要的），但是如果你的副本是可写的或者你想要你的副本有不同的内存配置，而且你也很确认到达副本的写操作能保证幂等性（idempotenet），那你可以修改这个默认值（但是最好保证你理解了这么做的原因）。</p>
<p>提示：因为副本默认没有 maxmemory 和淘汰策略，副本实际的内存占用可能比 maxmemeory 配置的值大（可能因为副本缓冲区，或者某些数据结构占用了额外的内存等等原因）。所以确保对副本有合适的监控手段，保证在主节点达到配置的 maxmemory 设置之前，副本有足够的内存保证不会出现真正的 out-of-memory 条件。</p>
<h3 id="replica-ignore-maxmemory-yes"><a href="#replica-ignore-maxmemory-yes" class="headerlink" title="#replica-ignore-maxmemory yes"></a>#replica-ignore-maxmemory yes</h3><h2 id="LAZY-FREEING（懒释放）"><a href="#LAZY-FREEING（懒释放）" class="headerlink" title="LAZY FREEING（懒释放）"></a>LAZY FREEING（懒释放）</h2><p>Redis 有两个可以删除 key 的原语（primitive）。其中一种是调用 DEL ，阻塞地删除对象。也就是说 Redis Server 需要通过同步的方式确认回收了所有和刚才删除的 key 相关的内存后，才能处理接下来的命令。如果要删除的 key 很小，执行 DEL 命令的时间也很短，和其他时间复杂度为 O(1) 或 O(log_N) 的命令差不多。但是，如果要删除的 key 涉及到一个存储着百万级别元素的集合，Redis server 就可能因此阻塞一段时间（甚至到秒的级别）。</p>
<p>由于同步的处理方式可能带来的问题，Redis 提供了非阻塞的删除原语比如 UNLINK 以及异步的选项比如 FLUSHALL 和 FLUSHDB 命名，为的就是在后台回收内存。这些命名会在固定时间执行（in constant time）。另外的线程会在后台以尽可能快的速度释放这些对象。</p>
<p>DEL，UNLINK 和带有 ASYNC 选项的 FLUSHALL 和 FLUSHDB 命名都可以由用户控制。这取决于应用层面是否理解且合适的使用相应的命令来达到目的。但是还是有一些情况要注意，Redis 有时会因为其他操作的副作用导致触发 key 的删除或者刷新整个数据库。特别是在用户调用了对象删除的以下场景：</p>
<ol>
<li>在淘汰策略下，因为配置了 maxmemory 和 maxmemory policy，为了在不超过配置的内存限制下腾出空间给新来的数据。</li>
<li>因为过期时间的配置：当一个 key 配置了 expire 时间且时间到了，那它必须从内存中移除。</li>
<li>命名在已经存在的 key 上进行数据的存储操作的副作用。比如 RENAME 命名在替换的时候需要删除原本的 key 的内容。类似的带有 STORE 选项的 SUNIONSTORE 或者 SORT 命名可能会删除已存在的 key。SET 命令本身为了用新的值替换，会将要操作的 key 的旧值先删除掉。</li>
<li>在 REPLICATION 期间，当副本执行了全量同步复制，副本的整个数据库会被清空，然后加载传输来的 RDB 文件。</li>
</ol>
<p>上面的场景在默认情况下都是以阻塞的方式删除对象，比如调用 DEL 的时候。你在本配置项中为每个场景进行配置，这样就可以像 UNLINK 被调用时以非阻塞的方式释放内存。</p>
<h3 id="lazyfree-lazy-eviction-no"><a href="#lazyfree-lazy-eviction-no" class="headerlink" title="lazyfree-lazy-eviction no"></a>lazyfree-lazy-eviction no</h3><h3 id="lazyfree-lazy-expire-no"><a href="#lazyfree-lazy-expire-no" class="headerlink" title="lazyfree-lazy-expire no"></a>lazyfree-lazy-expire no</h3><h3 id="lazyfree-lazy-server-del-no"><a href="#lazyfree-lazy-server-del-no" class="headerlink" title="lazyfree-lazy-server-del no"></a>lazyfree-lazy-server-del no</h3><h3 id="lazyfree-lazy-flush-no"><a href="#lazyfree-lazy-flush-no" class="headerlink" title="lazyfree-lazy-flush no"></a>lazyfree-lazy-flush no</h3><h2 id="APPEND-ONLY-MODE（附加模式）"><a href="#APPEND-ONLY-MODE（附加模式）" class="headerlink" title="APPEND ONLY MODE（附加模式）"></a>APPEND ONLY MODE（附加模式）</h2><p>Redis 默认使用异步方式转储文件到硬盘。这种模式在很多应用场景下都很适用，但是在 Redis 处理出现问题或者设备断电的意外期间可能丢失相应的写操作（取决于 save 配置的时间点）。</p>
<p>AOF 文件是 Redis 提供的另外一种提供更好的持久性的持久化模式。例如如果使用默认的数据传输策略（根据之后提供的配置）Redis 在发生意外情况下比如设备断电，或者 Redis 本身的进程出现了一些问题的情况下（操作系统正常运行），Redis 可以仅仅丢失 1 秒钟的写操作。</p>
<p>AOF 和 RDB 的持久化策略可以同时启用。如果打开了 AOF，Redis 启动时会加载 AOF，因为 AOF 的持久化表现更好。</p>
<p>点击<a href="http://redis.io/topics/persistence">http://redis.io/topics/persistence</a>获取更多相关的信息。</p>
<h3 id="appendonly-on"><a href="#appendonly-on" class="headerlink" title="appendonly on"></a>appendonly on</h3><p>AOF 的文件名（默认：”appendonly.aof”）</p>
<h3 id="appendfilename-“appendonly-aof”"><a href="#appendfilename-“appendonly-aof”" class="headerlink" title="appendfilename “appendonly.aof”"></a>appendfilename “appendonly.aof”</h3><p>函数 fsync() 会告诉操作系统立即把数据写到磁盘上而不是等输出缓冲区有更多的数据时才进行。有些 OS 会马上把数据刷到硬盘，有些 OS 只保证尽快进行刷盘操作。</p>
<p>Redis 支持三种模式：</p>
<p>no：不 fsync，让操作系统来决定什么时候进行刷盘。最不会影响 Server 响应。</p>
<p>always：每写入 aof 文件就进行 fsync。影响 Server 响应，但是数据更安全。</p>
<p>everysec：每秒进行 fsync。最稳健的形式。</p>
<p>默认的模式是 everysec，在响应速度和数据安全方面最稳妥的选择。以上三种模式的选择都取决你对应用的理解，选择 no ，让 OS 选择写入时机，这样有更好的性能表现（但是如果你的业务可以忍受一些数据的丢失，其实你可以考虑使用默认的持久化策略 - RDB）。又或者使用 always，可以会让响应变慢一些但是数据的安全性会更高。</p>
<p>更多的相关知识戳下面的文章链接：</p>
<p><a href="http://antirez.com/post/redis-persistence-demystified.html">http://antirez.com/post/redis-persistence-demystified.html</a></p>
<p>如果你不确定选哪种的话，那就用 “everysec” 吧。</p>
<h3 id="appendfsync-always"><a href="#appendfsync-always" class="headerlink" title="#appendfsync always"></a>#appendfsync always</h3><h3 id="appendfsync-everysec"><a href="#appendfsync-everysec" class="headerlink" title="appendfsync everysec"></a>appendfsync everysec</h3><h3 id="appendfsync-no"><a href="#appendfsync-no" class="headerlink" title="#appendfsync no"></a>#appendfsync no</h3><p>当 AOF fsync 策略是 always 或者  everysec，会启动一个后台进程（后台进行保存或者 AOF 文件的后台重写），该进程会在磁盘上频繁的 I/O，在一些 Linux 配置下 Redis 的 fsync() 调用可能会阻塞太久。需要注意的是目前还没有相应的优化策略，极端情况下在不同线程进行的  fsync 可能阻塞同步的 write(2) 调用。</p>
<p>为了减缓上面提到的问题，可以在主线程调用 BGSAVE 或者 BGREWRITEAOF 命名避免 fsync() 在主线程上调用。</p>
<p>这意味着但其他的子节点在保存的时候，Redis 的持久化就和 “appendfsync none” 策略一样。这意味着在实际中的最糟糕的场景下（在默认的 Linux 配置下）有可能丢失超过 30s 时间粒度的 log。</p>
<p>如果你的应用不能忍受延迟问题，将下面的选项配置为 “yes”。否则保持为 “no”，这样才持久化的角度上是最安全的选择。</p>
<h3 id="no-appendfsync-on-rewrite-no"><a href="#no-appendfsync-on-rewrite-no" class="headerlink" title="no-appendfsync-on-rewrite no"></a>no-appendfsync-on-rewrite no</h3><p>自动重写 aof 文件。</p>
<p>Redis 支持调用 BGREWRITEAOF 命名，并在 AOF 文件达到特定的百分比的时候自动重写 AOF 文件。</p>
<p>一般是这么工作的：Redis 会记录最近一次重写后的 AOF 文件大小（如果启动后没有重写过，则记录启动时的 AOF 文件大小）。</p>
<p>基础的文件大小和当前的文件大小进行比较。如果当前的大小比配置的百分比大，则触发重写操作。同时也应该配置一个触发重写的最小文件大小，这么做可以避免当 AOF 文件达到了配置的百分比，但是 AOF 文件还是很小的情况触发重写操作。</p>
<p>配置百分比为 0 意味着关闭自动重写 AOF 的特性。</p>
<h3 id="auto-aof-rewtire-percentage-100"><a href="#auto-aof-rewtire-percentage-100" class="headerlink" title="auto-aof-rewtire-percentage 100"></a>auto-aof-rewtire-percentage 100</h3><h3 id="auto-aof-rewrite-min-size-64mb"><a href="#auto-aof-rewrite-min-size-64mb" class="headerlink" title="auto-aof-rewrite-min-size 64mb"></a>auto-aof-rewrite-min-size 64mb</h3><p>当 AOF 文件的数据加载到内存的时候，AOF 文件可能在 Redis 启动的时候在末尾被截断。这可能在跑 Redis 进程的系统崩溃的情况下出现，特别是当一个 ext4 文件系统挂载的时候没有使用 data=ordered 选项（但是，在 Redis 进程自己崩溃或者中止，但是操作系统还正常运行时，这种情况就不会发生）。</p>
<p>当 Redis 发现 AOF 在末尾被截断的时候，Redis 可以主动退出进程或者尽可能的加载更多的数据（目前的默认行为）并正常启动。下面的配置可以控制这一行为。</p>
<p>如果 aof-load-truncated 设置成 yes，Redis 加载被截断的 AOF 文件，启动，并将相关的信息写到 log 中通知用户有这一现象发生。如果设置成 no，Redis 错误充电并拒绝启动。当该配置设置为 no 的时候，就要求用户在重启服务前使用 “redis-check-aof” 来修复 AOF 文件。</p>
<p>注意：如果 AOF 文件的中间位置出现了问题，Redis 仍会错误退出。这个配置选项只在 Redis 想从 AOF 文件中读取更多数据但是实在没有新的可以读取的情况下才有作用。</p>
<h3 id="aof-load-truncated-yes"><a href="#aof-load-truncated-yes" class="headerlink" title="aof-load-truncated yes"></a>aof-load-truncated yes</h3><p>当重写 AOF 文件的时候，Redis 也可以在 AOF 文件中 preamble 应用 RDB 文件来更快的重写和恢复。当该配置选项开启，AOF 文件的重写组成由这两部分组成：</p>
<p>[RDB file][AOF tail]</p>
<p>Redis 加载 AOF 文件的时候发现 AOF 文件里由 “REDIS” 字符串打头，Redis 就会加载预先的 RDB 文件，接着在尾部加载 AOF 文件。</p>
<h3 id="aof-use-rdb-preamble-yes"><a href="#aof-use-rdb-preamble-yes" class="headerlink" title="aof-use-rdb-preamble yes"></a>aof-use-rdb-preamble yes</h3><h2 id="LUA-SCRIPTING（LUA-脚本）"><a href="#LUA-SCRIPTING（LUA-脚本）" class="headerlink" title="LUA SCRIPTING（LUA 脚本）"></a>LUA SCRIPTING（LUA 脚本）</h2><p>Lua 脚本的最大限制执行时间（单位：毫秒）</p>
<p>如果 Lua 执行时间达到了最大时间限制，Redis 会记录该脚本的执行时间达到了限制且还未结束，并会对那些查询响应错误。</p>
<p>当一个脚本运行了太久触及了配置的最大执行时间，那么只有 SCRIPT KILL 和 SHUTDOWN NOSVAE 命名可以使用。第一个命令可以用来停止还没有调用写命名的脚本。而当你的脚本已经运行了写命令但是你又不想要等待脚本自己主动断开连接，那么第二个命令就是你唯一可以用来停止服务的命令。</p>
<p>将该配置设置为 0 或者负值，则无最长执行时间的限制且没有相关的报警。</p>
<h3 id="lua-time-limit-5000"><a href="#lua-time-limit-5000" class="headerlink" title="lua-time-limit 5000"></a>lua-time-limit 5000</h3><h2 id="REDIS-CLUSTER（Redis-集群）"><a href="#REDIS-CLUSTER（Redis-集群）" class="headerlink" title="REDIS CLUSTER（Redis 集群）"></a>REDIS CLUSTER（Redis 集群）</h2><p>一般的 Redis 实例不能成为 Redis 集群的一部分；只有作为集群启动的节点才可以。如果想要将 Redis 实例用作集群节点只需要把下面的配置取消掉注释即可：</p>
<h3 id="cluster-enable-yes"><a href="#cluster-enable-yes" class="headerlink" title="#cluster-enable yes"></a>#cluster-enable yes</h3><p>每个集群节点都有一个集群配置文件。这个文件不倾向于去手动编辑。它由 Redis 节点创建和更新。每个 Redis 集群节点要求有不同的集群配置文件。需要确保跑在同一个系统的实例没有重叠的集群配置文件名。</p>
<h3 id="cluster-config-file-nodes-6379-conf"><a href="#cluster-config-file-nodes-6379-conf" class="headerlink" title="#cluster-config-file nodes-6379.conf"></a>#cluster-config-file nodes-6379.conf</h3><p>集群节点的超时时间配置（单位：毫秒）应该不超过被视为连接失败的时间。</p>
<p>大部分的内部时间限制配置一般是集群节点超时时间的倍数。</p>
<h3 id="cluster-node-time-15000"><a href="#cluster-node-time-15000" class="headerlink" title="#cluster-node-time 15000"></a>#cluster-node-time 15000</h3><p>如果主节点故障，如果副本的数据太旧，应该避免使用该副本进行故障转移。</p>
<p>对于副本的 “数据新旧” 并没有一个简单的衡量方式，但是至少应该具备以下的两个特点：</p>
<ol>
<li>如果有多个副本可以进行故障转移，它们之间会互相交换信息，然后给那些从主节点复制更多数据的副本更高的优先级。副本之间通过复制的程度进行排序，然后根据它们的排名，以一定比较的时延开始故障转移（and apply to the start of the failover a delay proportional to their rank）。</li>
<li>每个副本都会计算自己最近一次和主节点进行通信的时间。这个时间可以由最近的一次 ping 或者接受到命令的时间（如果主节点还处于 “connected” 状态），又或者是自从上一次和主节点断开连接的时间（如果复制的连接已经断开）。如果上一次的通信时间太早了，那该副本完全没有进行故障转移的资格。</li>
</ol>
<p>第 2 点可以由用户来调整。但是还有一个条件就是，如果副本自从上次和主节点通信以来，超过了下面这个公式的时候后，这个副本无论如何都不能被选来进行故障转移：</p>
<p>(node-timeout * replica-validity-factor) + repl-ping-replica-period</p>
<p>比如，node-timeout 为 30s，replica-validity-factor 为 10s，假设 repl - ping - replica - period 为默认值 10s，那么副本如果超过 310s 还没有和主节点通上信，那么该副本不会被选择为故障转移的对象。</p>
<p>replica-validity-factor 值比较大的话，副本的数据延迟就会比较高。如果太小的话，cluster 就可以无法选举合适的进行故障转移。</p>
<p>为了更好的可用性，可以把  replica - validity - factor 的值设置为 0，也就是说，不管副本上次和主节点进行通信的时间过了多久，副本都有机会尝试进行故障转移。（但是他们总会尝试按照偏移量的排名应用延迟）（However they’ll always try to apply a delay proportional to their offset rank）</p>
<p>Zero is the only value able to guarantee that when all the partitions heal the cluster will always be able to continue.</p>
<h3 id="cluster-replica-validity-factor-10"><a href="#cluster-replica-validity-factor-10" class="headerlink" title="#cluster-replica-validity - factor 10"></a>#cluster-replica-validity - factor 10</h3><p>副本集群可以向孤独的主节点转移，孤独的意思就是该主节点没有依附的副本可用。这样可以提升集群抵抗风险的能力，毕竟如果孤独主节点异常后可能没有可用的副本可选。</p>
<p>副本集群向孤独主节点进行迁移是有条件的，这个条件是主节点至少还有给定数量的副本仍为其服务。这个数量值一般称为 “migration barrier”。比如该值配置为 1，说明副本迁移的条件是该主节点至少还有 1 个副本为其工作，以此类推。这一般也反映了你想要为主节点配置的集群的副本数量。</p>
<p>该配置项默认值是 1（副本迁移只在目标主节点至少还有一个副本为其工作的条件下才会进行）。想要禁止迁移的话只要把该项的值设置的大一点即可。也可以设置为 0 值，但是最好是在测试环境下使用，生产环境下是危险的配置。</p>
<h3 id="cluster-migration-barrier-1"><a href="#cluster-migration-barrier-1" class="headerlink" title="#cluster-migration-barrier 1"></a>#cluster-migration-barrier 1</h3><p>默认情况下，如果 Redis 集群节点检测到至少有一个哈希槽没有覆盖到（没有可用的节点来服务它），集群节点会停止接受查询。这样子的话，如果集群部分瘫痪（比如一个范围内的哈希槽没有被覆盖），最终整个集群都会停止服务。当所有的槽都被覆盖后，集群会自动恢复服务。</p>
<p>但有时候你又想在集群部分瘫痪的情况下，让那些还在工作且正常进行覆盖的节点继续接受查询。那么只要把配置选项设置为 no 即可。</p>
<h3 id="cluster-require-full-coverage-yes"><a href="#cluster-require-full-coverage-yes" class="headerlink" title="#cluster-require-full-coverage yes"></a>#cluster-require-full-coverage yes</h3><p>把该配置设置为 yes 的话，主节点发生故障期间副本无法进行自动转移。但主节点仍然可以进行手动故障转移。</p>
<p>这个配置项在多场景中可以发挥作用，特别…</p>
<h3 id="cluster-replica-no-failover-no"><a href="#cluster-replica-no-failover-no" class="headerlink" title="#cluster-replica-no-failover no"></a>#cluster-replica-no-failover no</h3><p>通过阅读官方的<a href="http://redis.io/">在线文档</a>来确保正确地配置你的 cluster 吧。</p>
<h2 id="CLUSTER-DOCKER-NAT-support"><a href="#CLUSTER-DOCKER-NAT-support" class="headerlink" title="CLUSTER DOCKER/NAT support"></a>CLUSTER DOCKER/NAT support</h2><p>在某些部署情况中，Redis 集群节点可能会出现地址发现失败，原因是地址是 NAT-ted 或者端口转发（一个典型的场景就是 Docker 或者其他容器）。</p>
<p>为了让 Redis 集群在这种环境下正常工作，就需要个静态的配置文件来让集群节点知晓他们的公共地址。下面两个选项就有这个作用：</p>
<ul>
<li>cluster-announce-ip</li>
<li>cluster-announce-port</li>
<li>cluster-announce-bus-port</li>
</ul>
<h2 id="SLOW-LOG（慢日志）"><a href="#SLOW-LOG（慢日志）" class="headerlink" title="SLOW LOG（慢日志）"></a>SLOW LOG（慢日志）</h2><p>Redis 的慢日志用来记录那些执行了超过特定时间的查询行为。这里的执行时间不包括 I/O 操作，比如和客户端的通信，发送回复的时间等等。而应该只是执行了这个命令本身需要的时间（就是说执行这个命令期间，线程会阻塞且不会同时响应其他的请求）。</p>
<p>慢日志有两个属性可以配置：一个用来告诉 Redis 执行时间的定义，什么样的执行时间才要被记录。另一个用来配置慢日志的长度。记录一个新的命令，队列中的最旧的命令会被移除。</p>
<p>下面配置的时间单位是<strong>微秒</strong>，所以 1000000 相当于 1 秒。注意如果配置的是负值，慢日志则不起作用。如果是 0 的话，慢日志则会记录每个命令。</p>
<h3 id="slowlog-log-slower-than-10000"><a href="#slowlog-log-slower-than-10000" class="headerlink" title="slowlog-log-slower-than 10000"></a>slowlog-log-slower-than 10000</h3><p>长度的配置没有任何限制。但是主要内存的消耗。你可以使用慢日志的 SLOWLOG RESET 来回收内存。</p>
<h3 id="slowlog-max-len-128"><a href="#slowlog-max-len-128" class="headerlink" title="slowlog-max-len 128"></a>slowlog-max-len 128</h3><h2 id="LATENCY-MONITOR（延迟监控）"><a href="#LATENCY-MONITOR（延迟监控）" class="headerlink" title="LATENCY MONITOR（延迟监控）"></a>LATENCY MONITOR（延迟监控）</h2><p>Redis 的延迟监控系统会在 Redis 运行期间以不同的操作对象为样本，收集和 Redis 实例相关的延迟行为。</p>
<p>用户可以通过 LETENCY 命令，打印相关的图形信息和获取相关的报告。</p>
<p>延迟监控系统只会收集那些执行时间超过了我们通过 latency-monitor-threshold 配置的值的操作。当 latency-monitor-threshold 的值设置为 0 的时候，延迟监控系统就会关闭。</p>
<p>默认情况下延迟监控是关闭的，因为大多数情况下你可能没有延迟相关的问题，而且收集数据对性能表现是有影响的，虽然影响很小，但是在系统高负载运行情况下还是不能忽视的。延迟监控系统可以在运行期间使用 “CONFIG SET latency-monitor-threshold <milliseconds>“ 开启。</milliseconds></p>
<h3 id="latency-monitor-threshold-0"><a href="#latency-monitor-threshold-0" class="headerlink" title="#latency-monitor-threshold 0"></a>#latency-monitor-threshold 0</h3><h2 id="EVENT-NOTIFICATION（事件通知）"><a href="#EVENT-NOTIFICATION（事件通知）" class="headerlink" title="EVENT NOTIFICATION（事件通知）"></a>EVENT NOTIFICATION（事件通知）</h2><p>Redis 可以将键空间中的事件通知到 发布/订阅 客户端。这一特性在<a href="http://redis.io/topics/notifications">http://redis.io/topics/notifications</a>有详细的文档记录。</p>
<p>如果实例上的键空间时间通知开启的话，这时候客户端对存储在 Database 0 的 “foo” 键执行 DEL 操作，那么会有两条信息通过 发布/订阅 被公布：</p>
<ul>
<li>PUBLISH __keyspace@0__：foo del</li>
<li>PUBLISH __keyevent@0__：del foo</li>
</ul>
<p>也可以在一组 classes 中选择 Redis 会通知的事件。每个 class 通过一个字符定义：</p>
<ul>
<li>K     Keyspace events, published with <strong>keyspace@<db></db></strong> prefix.</li>
<li>E     Keyevent events, published with <strong>keyevent@<db></db></strong> prefix.</li>
<li>g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, …</li>
<li>$     String commands</li>
<li>l     List commands</li>
<li>s     Set commands</li>
<li>h     Hash commands</li>
<li>z     Sorted set commands</li>
<li>x     Expired events (events generated every time a key expires)</li>
<li>e     Evicted events (events generated when a key is evicted for maxmemory)</li>
<li>A     Alias for g$lshzxe, so that the “AKE” string means all the events.</li>
</ul>
<p>“notify-keyspace-events” 的参数采用一个由 0 个或者多个字符的字符串。空串意味着关闭通知事件。</p>
<p>比如：开启 list 和 generic 事件，从事件名称的角度，可以使用：notify-keyspace-events Elg</p>
<p>比如：为了获得订阅了 <strong>keyevnet@0</strong>:expired 的过期键的流，使用：notify-keyspace-evnets Ex</p>
<p>默认所有的通知事件都是关闭的因为大多数的用户不需要这个功能且这个功能需要额外的开销（has some overhead）。注意：如果你没有配置至少一个 K 或者 E，没有事件会被传递。</p>
<p>notify-keyspace-events “”</p>
<h2 id="ADVANCED-CONFIG（高级配置）"><a href="#ADVANCED-CONFIG（高级配置）" class="headerlink" title="ADVANCED CONFIG（高级配置）"></a>ADVANCED CONFIG（高级配置）</h2><p>哈希（数据类型）如果保存的 entry 很少的话，其底层的数据结构会采用更加节省内存的方式存储。最大的 entry 不应该超过给定的阈值。可以通过下面的配置项配置阈值。</p>
<h3 id="hash-max-ziplist-entries-512"><a href="#hash-max-ziplist-entries-512" class="headerlink" title="hash-max-ziplist-entries 512"></a>hash-max-ziplist-entries 512</h3><h3 id="hash-max-ziplist-value-64"><a href="#hash-max-ziplist-value-64" class="headerlink" title="hash-max-ziplist-value 64"></a>hash-max-ziplist-value 64</h3><p>Lists（数据类型）底层也采用特殊的编码来节省空间。</p>
<p>每个 list 节点内部的 entry 数目可以通过固定的最大大小和最大元素数量来指定。</p>
<p>比如一个固定的最大大小，使用 -5 到 -1，说明：</p>
<ul>
<li>-5：最大大小：64kb，对正常的工作量来说不推荐</li>
<li>-4：最大大小：32kb，不推荐</li>
<li>-3：最大大小：16kb，可能不太推荐</li>
<li>-2：最大大小：8kb，推荐</li>
<li>-1：最大大小：4kb，推荐</li>
</ul>
<p>正数值代表每个 list 节点可以存储的元素数量。</p>
<p>各方面表现最好的选择一般是 -2（8kb 大小）或者 -1（4kb 大小），当然如果你的应用场景比较特殊的话，你可以自己进行调整。</p>
<h3 id="list-max-ziplist-size-2"><a href="#list-max-ziplist-size-2" class="headerlink" title="list-max-ziplist-size -2"></a>list-max-ziplist-size -2</h3><p>Lists 也可以压缩。</p>
<p>压缩程度的值是指从 ziplist 节点的一侧到 list 的另一侧之间进行压缩。为了保持 list 的 push/pop 命令可以快速的执行，list 的头结点和尾节点总是不会被压缩。具体的设置如下：</p>
<ul>
<li>0：不进行任何的压缩操作</li>
<li>1：depth 1 指的是排除了头尾的一个节点长度，其余的进行压缩。比如 [head]-&gt;node1-&gt;[tail]，除了头尾节点，node1 会被压缩。</li>
<li>2： [head]-&gt;node1-&gt;node2-&gt;node3-&gt;node4-&gt;[tail]，2 意味着 head + node1，tail + node4 不会被压缩。之间的节点会被压缩。</li>
<li>以此类推…</li>
</ul>
<h3 id="list-compress-depth-0"><a href="#list-compress-depth-0" class="headerlink" title="list-compress-depth 0"></a>list-compress-depth 0</h3><p>Sets 只在一种情况下会进行特殊编码：当该 set 仅仅由 strings 组成，且恰好是在基数为 10 的 64 位有符号整数范围内的整数。</p>
<p>此项配置限制了 sets 进行特殊编码策略的最大 set 大小。</p>
<h3 id="set-max-intset-entries-512"><a href="#set-max-intset-entries-512" class="headerlink" title="set-max-intset-entries 512"></a>set-max-intset-entries 512</h3><p>和 hashes，lists 类似，sorted set 也有特殊的节省空间的编码策略。这个编码策略只在 sorted set 的长度和元素低于下面的限制才会生效：</p>
<h3 id="zset-max-ziplist-entries-128"><a href="#zset-max-ziplist-entries-128" class="headerlink" title="zset-max-ziplist-entries 128"></a>zset-max-ziplist-entries 128</h3><h3 id="zset-max-ziplist-value-64"><a href="#zset-max-ziplist-value-64" class="headerlink" title="zset-max-ziplist-value 64"></a>zset-max-ziplist-value 64</h3><p>HyperLogLog 稀疏代表字节的限制配置。该限制包括了 16 个字节的首部。如果 HyperLogLog 使用稀疏代表的字节超过了该配置的限制，就会转换成密集的表示形式。</p>
<p>该值超过了 16000 就起不到作用了。因为到达了该限制时使用密集的表示形式在内存上会更高效。</p>
<p>建议配置的值大约在 3000 左右，这个值在使用高效的空间编码同时，还不会让在稀疏编码情况下时间复杂度为 O(N) 的 PFADD 命令性能下降的太厉害。如果你的 CPU 完全够用，比较关心空间的话，且数据集合大部分是由基数在 0 ~ 15000 范围内组成的 HyperLogLog 组成，该配置值可以提高至约 10000。</p>
<h3 id="hll-sparse-max-bytes-3000"><a href="#hll-sparse-max-bytes-3000" class="headerlink" title="hll-sparse-max-bytes 3000"></a>hll-sparse-max-bytes 3000</h3><p>Streams 集节点的最大 大小 / 个数。 stream 这一数据结构大概是一个带有多个节点，节点中包含了多个项的一棵树。这个配置可以决定每个节点最大的大小，以及当增加了新的 stream 条目，在旧节点向新节点转换之前可以包含的最大的项数量。其中的任何一项设置成 0 就可以取消对应的限制。所以如果你只想要其中的一项就把另一个项设置为 0 即可。</p>
<h3 id="stream-node-max-bytes-4096"><a href="#stream-node-max-bytes-4096" class="headerlink" title="stream-node-max-bytes 4096"></a>stream-node-max-bytes 4096</h3><h3 id="stream-node-max-entries-100"><a href="#stream-node-max-entries-100" class="headerlink" title="stream-node-max-entries 100"></a>stream-node-max-entries 100</h3><p>Active rehash 会使用 CPU 时间 100 毫秒中的 1 个毫秒来 rehash Redis 的主哈希表（该哈希表是用 key 来定位 value 的位置）。Redis 的这个哈希表实现使用了 lazy-rehash：对该哈希表的操作越多，哈希表的 rehash 步骤进行的越多。如果你的 Redis 实例很空闲，rehash 就不会完成且哈希表可能占用更多的内存空间。</p>
<p>默认的话 active rehash 会使用 1 秒中的 10 毫秒来 rehash 哈希表，且在可以的时候释放内存空间。</p>
<p>如果你不确定该不该用的话（可以进行如下参考）：</p>
<p>对于延迟的要求很高，比如 Redis 对查询的延迟有 2 毫秒的延迟都无法忍受的话，使用 “no” 选项。</p>
<p>对延迟的要求不高，在希望在可以的时候尽快(assp，as soon as possible)释放内存空间，使用 “yes”。</p>
<h3 id="activerehashing-yes"><a href="#activerehashing-yes" class="headerlink" title="activerehashing yes"></a>activerehashing yes</h3><p>客户端输出缓冲区限制可以在客户端因为某些原因无法及时从服务端读取数据时（一个常见的原因是一个 发布/订阅 的客户端的消费速度匹配不上发布端的生产速度），用来强制客户端断开链接。</p>
<p>因为存在三种不同类型的客户端，这个限制也有三种：</p>
<ul>
<li>normal，正常的客户端包括了 MONITOR 客户端。</li>
<li>replica，副本客户端。</li>
<li>pubsub，那些至少订阅了 pubsub 频道或者模式的客户端。</li>
</ul>
<p>client-output-buffer-limit 的语法如下：</p>
<p>client-output-buffer-limit <class> <hard limit> <soft limit> <soft seconds></soft></soft></hard></class></p>
<p>客户端输出缓冲区一达到 hard limit 或者达到了 soft limit 且持续了 soft seconds ，客户端会立即断开连接。</p>
<p>比如说一个实例配置的 hard limit 是 32 megebytes，soft limit 是 16 megabytes / 10 seconds，客户端会因为输出缓冲区到达了 32 megebytes 或者超过了 16 megabytes 且持续 10 秒 时被断连。</p>
<p>默认的 normal 客户端没有这种限制因为他们没有进行请求的话一般不会收到数据，如果这种客户端发送了一个请求，其实也只有异步客户端可能会出现发出请求的待接收数据超出了客户端的接收能力。</p>
<p>pubsub 和 replica 客户端是有默认限制的，因为订阅端和副本端接收数据通过另一方推送决定的。</p>
<p>hard 和 soft limit 都可以通过设置为 0 来取消。</p>
<h3 id="client-outputbuffer-limit-normal-0-0-0"><a href="#client-outputbuffer-limit-normal-0-0-0" class="headerlink" title="client-outputbuffer-limit normal 0 0 0"></a>client-outputbuffer-limit normal 0 0 0</h3><h3 id="client-outputbuffer-limit-replica-256mb-64mb-60"><a href="#client-outputbuffer-limit-replica-256mb-64mb-60" class="headerlink" title="client-outputbuffer-limit replica 256mb 64mb 60"></a>client-outputbuffer-limit replica 256mb 64mb 60</h3><h3 id="client-outputbuffer-limit-norma-32mb-8mb-60"><a href="#client-outputbuffer-limit-norma-32mb-8mb-60" class="headerlink" title="client-outputbuffer-limit norma 32mb 8mb 60"></a>client-outputbuffer-limit norma 32mb 8mb 60</h3><p>客户端用来累计新命令的查询缓冲区（Client query buffers accumulate new commands）。他们默认被限制成一个固定的值来避免比如不进行同步的协议（很可能是客户端的 bug）导致在查询缓冲区未绑定的内存占用。如果你有比如巨大的 multi/exec 请求这种特殊的需求，你也可以关系这项配置。</p>
<h3 id="client-query-buffer-limit-1gb"><a href="#client-query-buffer-limit-1gb" class="headerlink" title="client-query-buffer-limit 1gb"></a>client-query-buffer-limit 1gb</h3><p>在 Redis 协议中，块请求，即单个请求的元素，通常限制在 512 mb。你也可以在这里改变这个配置。</p>
<h3 id="proto-max-bulk-len-512-mb"><a href="#proto-max-bulk-len-512-mb" class="headerlink" title="proto-max-bulk-len 512 mb"></a>proto-max-bulk-len 512 mb</h3><p>Redis 的内部调用用来执行很多后台任务，比如关闭超时的客户端连接，清除（purging）一直没有被访问的过期键值对，等等等等。</p>
<p>每个任务调用不一定都是在一个频率，Redis 会通过配置的 “hz” 值来检测需要执行的任务。</p>
<p>默认的 “hz” 设置为 10。提高这个值的话 Redis 在<strong>空闲时</strong>会占用更多 CPU，但是同时也会让 Redis 对于处理上面提到的那些任务更加快速和精确。</p>
<p>“hz” 可以配置的范围在 1 到 500。但是超过 100 就已经不是一个好选择了。大部分的用户应该用默认值就足够了，如果严格要求低延迟的话可以把这个值提到 100。</p>
<h3 id="hz-10"><a href="#hz-10" class="headerlink" title="hz 10"></a>hz 10</h3><p>通常来说，对于数量会改变的客户端连接来说，HZ 值可以根据这个进行成比例的改变是很有效的。例如，这有助于避免每次后台的任务调用处理过多客户端连接，这样可以避免延迟飙升。</p>
<p>由于 Redis 提供的默认值设定为 10，比较保守。为此 Redis 也默认开启了可以暂时提高 HZ 的值以应对过多客户端连接的情况。</p>
<p>默认 HZ 动态配置是开启的，该动态值以配置的静态值为基准，在客户端连接数多的时候，HZ 值可以上升到基准值的数倍。这样的好处是空闲的实例占用更少的 CPU 同时繁忙的实例响应速度会更好。</p>
<h3 id="dynamic-hz-yes"><a href="#dynamic-hz-yes" class="headerlink" title="dynamic-hz yes"></a>dynamic-hz yes</h3><p>当子节点重写 AOF 文件时，同时这个配置开启的话，AOF 文件每生成 32 MB 就会进行一次同步。这样做的好处是文件可以分步写到磁盘且避免了阻塞导致的高延迟。</p>
<h3 id="aof-rewrite-incremental-fsync-yes"><a href="#aof-rewrite-incremental-fsync-yes" class="headerlink" title="aof-rewrite-incremental-fsync yes"></a>aof-rewrite-incremental-fsync yes</h3><p>Redis 存储 RDB 文件时，同时这个配置开启的话，RDB 文件每生成 32 MB 就会进行一次同步。这样做的好处是文件可以分步写到磁盘且避免了阻塞导致的高延迟。</p>
<h3 id="rdb-save-incremental-fsync-yes"><a href="#rdb-save-incremental-fsync-yes" class="headerlink" title="rdb-save-incremental-fsync yes"></a>rdb-save-incremental-fsync yes</h3><p>Redis 的 LFU 淘汰策略（看 maxmemroy setting 那一部分）可以进行调整。但是最好的情况还是保持默认的配置。最好对这些配置的影响有深刻的理解，且明白 LFU 对 key 的影响（可以通过 OBJECT FREQ 命令了解），再进行 LFU 策略的调整。</p>
<p>Redis 的 LFU 实现有两个小配置可以调整：the counter logarithm factor and the counter decay time。在该这两个配置前一定要有充分的理解。</p>
<p>LFU 计数器每个 key 最少 8 个比特，最大可以到 255 比特。Redis 使用对数的形式进行概率性的增长。对一个旧的计数器值，当这个 key 被访问后，计数器增长方式如下：</p>
<ol>
<li>先给一个 0 到 1 的随机值 R。</li>
<li>在通过 1/(old_value*lfu_log_factor+1) 算出一个概率值 P。</li>
<li>如果 R &lt; P，计数器的值才会进行增长。</li>
</ol>
<p>lfu_log_factor 的默认值为 10。下面这个表展示了不同的 lfu_log_factor 值以及 key 访问频率对应的计数器变化的频率：</p>
<p><img src="/2021/06/02/redis-conf-zh-cn/image-20210925114039445.png" alt="image-20210925114039445"></p>
<p>注意 1：上面的表可以通过以下的命令获取：</p>
<p>redis-benchmark -n 1000000 incr foo</p>
<p>redis-cli object freq foo</p>
<p>注意 2：为了给新的 key 计算命中数的机会，计数器的值会初始化为 5 。</p>
<p>计数器的衰减时间（单位：分钟），必须足够让 key counter 变为一半（值小等 10 的话，则递减）。</p>
<p>默认的 lfu-decay-time 值是 1。配置为 0 意味着每次扫描到的话都会衰减 计数器。</p>
<h3 id="lfu-log-factor-10"><a href="#lfu-log-factor-10" class="headerlink" title="#lfu-log-factor 10"></a>#lfu-log-factor 10</h3><h3 id="lfu-decay-time-1"><a href="#lfu-decay-time-1" class="headerlink" title="#lfu-decay-time 1"></a>#lfu-decay-time 1</h3><h2 id="ACTIVE-DEFRAGMENTATION（碎片整理）"><a href="#ACTIVE-DEFRAGMENTATION（碎片整理）" class="headerlink" title="ACTIVE DEFRAGMENTATION（碎片整理）"></a>ACTIVE DEFRAGMENTATION（碎片整理）</h2><p><strong>警告：以下的特性都是实验性的。</strong>但这些配置在生产环境中由多名工程师进行过多次的压力测试。</p>
<p><strong>什么是碎片整理？</strong></p>
<p>活动碎片整理可以让 Redis 在分配和回收内存后，整理聚合随之产生的内存碎片，以此来进行内存回收。</p>
<p>每个分配器（幸运的是用  Jemalloc 会产生的更少）工作时或多或少都会产生碎片。通常 Server 需要通过重启减少碎片，或者至少要通过冲刷所有数据并重新生成来减少碎片。我们得感谢 Oran Agra 从 Redis 4.0 开始实现的可以在 Server 运行时进行上面描述的操作来减少碎片。</p>
<p>当产生的碎片超过了某个程度后（可以看下面的配置项了解），Redis 就会利用 Jemalloc 提供的特性开始在一个连接的内存区域创建值的副本，同时会释放有了副本的数据。对所有的 key 重复的进行这样的处理会让碎片化程度回到正常的范围。</p>
<p>一定要理解的几点：</p>
<ol>
<li>这个特性默认关闭，且只当你使用 Jemalloc 来重新编译 Redis 的源码才会生效。Linux 下默认是这么做的。</li>
<li>如果没有碎片化的问题，这个特性最好永远不要打开。</li>
<li>一旦你遇到了碎片化的问题，你可以在需要的时候通过命令 “CONFIG SET activedefrag yes” 开启该特性。</li>
</ol>
<p>该配置还有很多参数就是用来配置上述提到的有关碎片整理的功能特性的。如果你不确定他们的意思的话那最好还是保持默认的配置选择。</p>
<p>开启碎片整理。</p>
<h3 id="activedefrag-yes"><a href="#activedefrag-yes" class="headerlink" title="#activedefrag yes"></a>#activedefrag yes</h3><p>开始碎片整理的最低碎片浪费空间大小。</p>
<h3 id="active-defrag-ignore-bytes-100mb"><a href="#active-defrag-ignore-bytes-100mb" class="headerlink" title="#active-defrag-ignore-bytes 100mb"></a>#active-defrag-ignore-bytes 100mb</h3><p>开始碎片整理的最低碎片空间占用百分比。</p>
<h3 id="active-defrag-threshold-lower-10"><a href="#active-defrag-threshold-lower-10" class="headerlink" title="#active-defrag-threshold-lower 10"></a>#active-defrag-threshold-lower 10</h3><p>我们最大程度进行整理的最大碎片程度（Maximum percentage of fragmentation at which we use maximum effort）。</p>
<h3 id="active-defrag-threshold-upper-100"><a href="#active-defrag-threshold-upper-100" class="headerlink" title="#active-defrag-threshold-upper 100"></a>#active-defrag-threshold-upper 100</h3><p>碎片整理的最小的 CPU 占用百分比。</p>
<h3 id="active-defrag-cycle-min-5"><a href="#active-defrag-cycle-min-5" class="headerlink" title="#active-defrag-cycle-min 5"></a>#active-defrag-cycle-min 5</h3><p>碎片整理的最大的 CPU 占用比。</p>
<h3 id="active-defrag-cycle-max-75"><a href="#active-defrag-cycle-max-75" class="headerlink" title="#active-defrag-cycle-max 75"></a>#active-defrag-cycle-max 75</h3><p>在主哈希表扫描中，最多进行处理的 set/hash/zset/list 域的数量。</p>
<h3 id="active-defrag-max-scan-fields-1000"><a href="#active-defrag-max-scan-fields-1000" class="headerlink" title="#active-defrag-max-scan-fields 1000"></a>#active-defrag-max-scan-fields 1000</h3>]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>翻译</tag>
        <tag>redis</tag>
      </tags>
  </entry>
</search>
